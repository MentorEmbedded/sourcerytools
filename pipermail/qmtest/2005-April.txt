From ghost at cs.msu.su  Mon Apr  4 07:41:22 2005
From: ghost at cs.msu.su (Vladimir Prus)
Date: Mon, 4 Apr 2005 11:41:22 +0400
Subject: The state of GUI
Message-ID: <200504041141.22921.ghost@cs.msu.su>


Hello,
over time, I'm getting more and more dissatisfied with the QMTest GUI. I'll 
start with listing some of the complaints. I have version 2.2.

1. When I start QMTest I see a list of directories. How do I add a new 
directory? There's only "new suite" menu item, but no "new directory".

2. Say I click on directory "backward" and then click "New test". The test 
name entry field is empty. If I type "foo", I get test "foo" created at the 
top-level. The entry field should either be prefilled with "backward.", or 
the "backward." should be prepended to the test name at creation time. I was 
hit by this several times already.

3. The drop-down box with test type lists class names. How does some developer 
who did not wrote the test know what test names like "nm_model.Backward2" 
mean? The description is only shown *after* selecting test name *and* 
clicking "Next".

4. The screen shown after "Run" command is non-ideal. It shows only 10 first 
test results, so in case that I have 100 tests I need to click always "next".
The summary at the top shown pass/fail counts, but if some tests fail, I don't 
know which one failed -- I need to look at next 10 results, and then on next 
10 results and so on until I see the failing one.

If I rerun a single test, the result is added last, and I'm shown the first 10 
results. Again, I need several clicks just to see the result I'm interested 
in.

5. I can't add annotations to tests, directories or expected failures.

There are some issues caused my HTML + local web server design.

1. The expectations must be manually saved after modification.

2. The GUI feels a bit slow


Most of the above are glitches, not very terrible bugs. But, given that they 
exist for quite some time, I start to wonder how QMTest GUI is supposed to be 
used. Maybe, the idea is that most users will create custom databases and use 
them, directly creating tests in the filesystem/DB/whatever? And GUI is just 
for running tests?

- Volodya



From seefeld at sympatico.ca  Mon Apr  4 10:38:11 2005
From: seefeld at sympatico.ca (Stefan Seefeld)
Date: Mon, 04 Apr 2005 06:38:11 -0400
Subject: [qmtest] The state of GUI
In-Reply-To: <200504041141.22921.ghost@cs.msu.su>
References: <200504041141.22921.ghost@cs.msu.su>
Message-ID: <42511913.9050104@sympatico.ca>

Hi Vladimir,

Vladimir Prus wrote:

> Most of the above are glitches, not very terrible bugs. But, given that they 
> exist for quite some time, I start to wonder how QMTest GUI is supposed to be 
> used. Maybe, the idea is that most users will create custom databases and use 
> them, directly creating tests in the filesystem/DB/whatever? And GUI is just 
> for running tests?

I don't know about the original intend of the various access points, but as
far as I am concerned as a user, I have indeed mostly been working with
custom databases that are non-modifiable. I'v only recently started to use
the GUI.

I'd be interested into whether many QMTest users use the interactive GUI mode to
modify test databases, too, as that would give us an indication about what features
to work on. I agree that the GUI can be enhanced a lot.

I'm currently looking into a 'qmtest report' command that reads multiple
result files (potentially from runs on different platforms) and generates an xml
report. In this context I'm interested to know what information should be available
in the report to make it usable. Your suggested expectation annotations seem to
be a good candidate here.

I'd be happy to hear about other suggestions, too.

Thanks,
		Stefan




From mark at codesourcery.com  Mon Apr  4 17:03:38 2005
From: mark at codesourcery.com (Mark Mitchell)
Date: Mon, 04 Apr 2005 10:03:38 -0700
Subject: [qmtest] The state of GUI
In-Reply-To: <200504041141.22921.ghost@cs.msu.su>
References: <200504041141.22921.ghost@cs.msu.su>
Message-ID: <4251736A.5060807@codesourcery.com>

Vladimir Prus wrote:

> Most of the above are glitches, not very terrible bugs. But, given that they 
> exist for quite some time, I start to wonder how QMTest GUI is supposed to be 
> used. Maybe, the idea is that most users will create custom databases and use 
> them, directly creating tests in the filesystem/DB/whatever? And GUI is just 
> for running tests?

I agree with every single one of your criticisms.

I think that the root cause is that nobody has invested the energy in 
trying to fix the GUI.  If you were to submit patches to fix the things 
that you listen, I can assure you that Stefan and I would happily review 
them.

-- 
Mark Mitchell
CodeSourcery, LLC
mark at codesourcery.com
(916) 791-8304


From eichin at metacarta.com  Mon Apr  4 18:58:35 2005
From: eichin at metacarta.com (eichin at metacarta.com)
Date: 04 Apr 2005 14:58:35 -0400
Subject: [qmtest] The state of GUI
In-Reply-To: <42511913.9050104@sympatico.ca>
References: <200504041141.22921.ghost@cs.msu.su>
	<42511913.9050104@sympatico.ca>
Message-ID: <7gwtrifjxg.fsf@pikespeak.metacarta.com>


> far as I am concerned as a user, I have indeed mostly been working with
> custom databases that are non-modifiable. I'v only recently started to use
> the GUI.

When I first started with QMtest, the gui was a usefully narrow way to
poke at something and expect that whatever I poked at would do
*something* reasonable.  Eventually I wrapped another layer of
homebrew automation[*] around it all, though, and I don't really see any
way *any* GUI would help there, though I suspect that if I were using
QMtest for something narrower, or had a better vocabulary of tests
that a non-programmer[**] could come up with new combinations for, it
would get more use.
			_Mark_ <eichin at metacarta.com>



[*] That's mostly because my testing problem is more at a systems
level and I stretched QMtest a *lot* to solve it; looking back, I need
abstractions that would get in the way in more conventional software
test situations.  (It's a credit to qmtest and python that I was
*able* to stretch it that far :-) and nothing else was, or is,
anywhere close.)

[**] Not that I believe in non-programmer testers anymore.
"exploratory" testing (ie. banging on things) is something you do in
order to come up with new automation.  Microsoft's SDE/T model makes a
lot of sense to me now...


From ghost at cs.msu.su  Tue Apr  5 06:35:10 2005
From: ghost at cs.msu.su (Vladimir Prus)
Date: Tue, 5 Apr 2005 10:35:10 +0400
Subject: [qmtest] The state of GUI
In-Reply-To: <42511913.9050104@sympatico.ca>
References: <200504041141.22921.ghost@cs.msu.su> <42511913.9050104@sympatico.ca>
Message-ID: <200504051035.10809.ghost@cs.msu.su>

Hi Stefan,

> Vladimir Prus wrote:
> > Most of the above are glitches, not very terrible bugs. But, given that
> > they exist for quite some time, I start to wonder how QMTest GUI is
> > supposed to be used. Maybe, the idea is that most users will create
> > custom databases and use them, directly creating tests in the
> > filesystem/DB/whatever? And GUI is just for running tests?
>
> I don't know about the original intend of the various access points, but as
> far as I am concerned as a user, I have indeed mostly been working with
> custom databases that are non-modifiable. I'v only recently started to use
> the GUI.
>
> I'd be interested into whether many QMTest users use the interactive GUI
> mode to modify test databases, too, as that would give us an indication
> about what features to work on. 

Fair enough. I'm too not entirely sure if it's best to improve the existing 
GUI, use custom databases, or just my own GUI with PyQt. 

I tend to like the GUI for running tests, at least when developing (nightly 
build surely uses "run + summarize"). However, creation of tests is less 
convenient. Say, I'd like to list a set of source files and automatically 
compute expected results, and create a test. Whether it's better to use 
custom database and a shell script, or standard XML database with some Python 
script that uses QMTest API is the question, though.

> I agree that the GUI can be enhanced a lot. 
>
> I'm currently looking into a 'qmtest report' command that reads multiple
> result files (potentially from runs on different platforms) and generates
> an xml report. In this context I'm interested to know what information
> should be available in the report to make it usable. Your suggested
> expectation annotations seem to be a good candidate here.

Did you look at boost results format?  See:

    http://www.meta-comm.com/engineering/boost-regression/cvs-head/developer/program_options.html

Essentially, there are several test results which are merged in a big table. 
Each failure can have "notes" -- that's my proposed failure annotations. As 
I've mentioned somewhere in the tracker, the key point is that there's one 
expectation file for all toolsets.

As a related note -- GUI might make perfect sense for such expectation file. 
From time to time somebody commits ill-formed XML expectation file to Boost, 
and all test reporting break ;-)

- Volodya


From stefan at codesourcery.com  Tue Apr  5 10:42:20 2005
From: stefan at codesourcery.com (Stefan Seefeld)
Date: Tue, 05 Apr 2005 06:42:20 -0400
Subject: [qmtest] The state of GUI
In-Reply-To: <200504051035.10809.ghost@cs.msu.su>
References: <200504041141.22921.ghost@cs.msu.su> <42511913.9050104@sympatico.ca> <200504051035.10809.ghost@cs.msu.su>
Message-ID: <42526B8C.9000309@codesourcery.com>

Vladimir Prus wrote:

> Fair enough. I'm too not entirely sure if it's best to improve the existing 
> GUI, use custom databases, or just my own GUI with PyQt. 

I find the GUI quite convenient to introspect a test database interactively,
and so I'm very interested into improving the existing one (in particular,
since it is much more portable than any GUI toolkit - based will ever be).

> I tend to like the GUI for running tests, at least when developing (nightly 
> build surely uses "run + summarize"). However, creation of tests is less 
> convenient. Say, I'd like to list a set of source files and automatically 
> compute expected results, and create a test. Whether it's better to use 
> custom database and a shell script, or standard XML database with some Python 
> script that uses QMTest API is the question, though.

Right. I'm not quite sure how important / popular this use case of modifying
a test database via the GUI really is.

> Did you look at boost results format?  See:
> 
>     http://www.meta-comm.com/engineering/boost-regression/cvs-head/developer/program_options.html

Thanks for the pointer. I have casually looked at the boost regression report,
and I definitely intend to have a close look to see what I could add to our
own reporting facility to support this.

> Essentially, there are several test results which are merged in a big table. 
> Each failure can have "notes" -- that's my proposed failure annotations. As 
> I've mentioned somewhere in the tracker, the key point is that there's one 
> expectation file for all toolsets.

Oh, I missed that. Since in QMTest expectations are simply results of prior
test executions, my initial design proposal is to provide the means to
annotate result files. (Nothing is stopping you from using the same result
file as expectation in multiple toolsets.)

Does this make sense ?

Regards,
		Stefan


From nmarais at gmail.com  Tue Apr  5 16:23:38 2005
From: nmarais at gmail.com (Neilen Marais)
Date: Tue, 05 Apr 2005 18:23:38 +0200
Subject: Fortran unit tests with QMTest, anyone doing them?
Message-ID: <1112718218.20280.20.camel@wanbalans>

Hi!

I'm interested in using QMTest to do Fortran 90 unit tests. I am
thinking of writing a QMTest class to do this. That would be nice, since
then the f90 code could be evaluated directly, instead of having to do
an exec test to call a driver etc.

I'm wondering if anyone else is/has/is considering doing the same? If
you have already written a QMTest class, or something similar, I'd be
quite happy to share in the fruits of your labour :)

Thanks
Neilen



From stefan at codesourcery.com  Tue Apr  5 12:58:44 2005
From: stefan at codesourcery.com (Stefan Seefeld)
Date: Tue, 05 Apr 2005 08:58:44 -0400
Subject: [qmtest] Fortran unit tests with QMTest, anyone doing them?
In-Reply-To: <1112718218.20280.20.camel@wanbalans>
References: <1112718218.20280.20.camel@wanbalans>
Message-ID: <42528B84.8060200@codesourcery.com>

Neilen Marais wrote:
> Hi!
> 
> I'm interested in using QMTest to do Fortran 90 unit tests. I am
> thinking of writing a QMTest class to do this. That would be nice, since
> then the f90 code could be evaluated directly, instead of having to do
> an exec test to call a driver etc.

I'm not sure I understand what you mean by 'directly'. Are you saying you
intend to load your compiled fortran code as a python extension module and
run it in-process ? Why would you do that ?

Typically QMTest would execute a compiler to generate the binary and then
execute it with an 'exec'-like call, allowing the user to validate return
value, as well as output on stdout/stderr.
One of the advantages of such an 'indirect' test is robustness, as the
qmtest process itself would not be compromized by crashes in any of the
test binaries it executes.

We have been thinking about making the compilation more convenient, for
example by letting users specify variables such as CC/CFLAGS/CLIBS/... or,
in your case, F90/F90FlAGS/F90LIBS/.... Would that be helpful ?

Regards,
		Stefan


From nmarais at snowisp.com  Tue Apr  5 18:05:56 2005
From: nmarais at snowisp.com (Neilen Marais)
Date: Tue, 05 Apr 2005 20:05:56 +0200
Subject: [qmtest] Fortran unit tests with QMTest, anyone doing them?
In-Reply-To: <42528B84.8060200@codesourcery.com>
References: <1112718218.20280.20.camel@wanbalans> <42528B84.8060200@codesourcery.com>
Message-ID: <1112724356.20280.27.camel@wanbalans>

Hi Stefan

On Tue, 2005-04-05 at 08:58 -0400, Stefan Seefeld wrote:Typically QMTest
would execute a compiler to generate the binary and then
> execute it with an 'exec'-like call, allowing the user to validate return
> value, as well as output on stdout/stderr.
> One of the advantages of such an 'indirect' test is robustness, as the
> qmtest process itself would not be compromized by crashes in any of the
> test binaries it executes.
> 
> We have been thinking about making the compilation more convenient, for
> example by letting users specify variables such as CC/CFLAGS/CLIBS/... or,
> in your case, F90/F90FlAGS/F90LIBS/.... Would that be helpful ?

Yes, that is what I have in mind. What I'd like is something that has a
similar interface to the curent python tests. IOW, the f90 code is
included in the test, and then QMTest automatically compiles/links it as
neccesary. 

Cheers
Neilen

> 
> Regards,
> 		Stefan



From mark at codesourcery.com  Tue Apr  5 18:08:47 2005
From: mark at codesourcery.com (Mark Mitchell)
Date: Tue, 05 Apr 2005 11:08:47 -0700
Subject: [qmtest] Fortran unit tests with QMTest, anyone doing them?
In-Reply-To: <1112724356.20280.27.camel@wanbalans>
References: <1112718218.20280.20.camel@wanbalans> <42528B84.8060200@codesourcery.com> <1112724356.20280.27.camel@wanbalans>
Message-ID: <4252D42F.5050300@codesourcery.com>

Neilen Marais wrote:
> Hi Stefan
> 
> On Tue, 2005-04-05 at 08:58 -0400, Stefan Seefeld wrote:Typically QMTest
> would execute a compiler to generate the binary and then
> 
>>execute it with an 'exec'-like call, allowing the user to validate return
>>value, as well as output on stdout/stderr.
>>One of the advantages of such an 'indirect' test is robustness, as the
>>qmtest process itself would not be compromized by crashes in any of the
>>test binaries it executes.
>>
>>We have been thinking about making the compilation more convenient, for
>>example by letting users specify variables such as CC/CFLAGS/CLIBS/... or,
>>in your case, F90/F90FlAGS/F90LIBS/.... Would that be helpful ?
> 
> 
> Yes, that is what I have in mind. What I'd like is something that has a
> similar interface to the curent python tests. IOW, the f90 code is
> included in the test, and then QMTest automatically compiles/links it as
> neccesary. 

Some of the DejaGNU emulation code in QMTest might be useful in this 
respect.  That would allow you to actually store your tests as 
individual Fortran files; QMTest then will automatically 
compile/link/run each test.  Adding a new test is then as simple as 
dropping in a new source file.

-- 
Mark Mitchell
CodeSourcery, LLC
mark at codesourcery.com
(916) 791-8304


From nmarais at snowisp.com  Tue Apr  5 19:56:57 2005
From: nmarais at snowisp.com (Neilen Marais)
Date: Tue, 05 Apr 2005 21:56:57 +0200
Subject: qmtest documentation
Message-ID: <1112731017.20280.33.camel@wanbalans>

Hi!

I've only recently (yesterday) started playing with QMTest. So far I've
been quite impressed by what seems to be possible, but have not had much
luck getting QMTest to do what I want. It seems to be a lack of
education on my part. However, I can't seem to find any detailed
documentation to help me out!

Is the manual linked from http://www.codesourcery.com/qmtest/ the only
available user documentation? If this does not answer my questions,
where do I look? Can I ask my silly n00b questions here?

Thanks
Neilen





From mark at codesourcery.com  Tue Apr  5 21:51:49 2005
From: mark at codesourcery.com (Mark Mitchell)
Date: Tue, 05 Apr 2005 14:51:49 -0700
Subject: [qmtest] qmtest documentation
In-Reply-To: <1112731017.20280.33.camel@wanbalans>
References: <1112731017.20280.33.camel@wanbalans>
Message-ID: <42530875.6070601@codesourcery.com>

Neilen Marais wrote:

> Is the manual linked from http://www.codesourcery.com/qmtest/ the only
> available user documentation? If this does not answer my questions,

Yes, I'm afraid so.

> where do I look? Can I ask my silly n00b questions here?

Sure!

-- 
Mark Mitchell
CodeSourcery, LLC
mark at codesourcery.com
(916) 791-8304


From ghost at cs.msu.su  Wed Apr  6 07:05:46 2005
From: ghost at cs.msu.su (Vladimir Prus)
Date: Wed, 6 Apr 2005 11:05:46 +0400
Subject: [qmtest] The state of GUI
In-Reply-To: <42526B8C.9000309@codesourcery.com>
References: <200504041141.22921.ghost@cs.msu.su> <200504051035.10809.ghost@cs.msu.su> <42526B8C.9000309@codesourcery.com>
Message-ID: <200504061105.46779.ghost@cs.msu.su>

On Tuesday 05 April 2005 14:42, Stefan Seefeld wrote:

> > Fair enough. I'm too not entirely sure if it's best to improve the
> > existing GUI, use custom databases, or just my own GUI with PyQt.
>
> I find the GUI quite convenient to introspect a test database
> interactively, and so I'm very interested into improving the existing one
> (in particular, since it is much more portable than any GUI toolkit - based
> will ever be).

I realize this point. OTOH, HTML approach incurs some sloweness and autoload 
tricks. Maybe some smart JavaScript can sure this, but I don't now much of 
JavaScript to comment.

> > Essentially, there are several test results which are merged in a big
> > table. Each failure can have "notes" -- that's my proposed failure
> > annotations. As I've mentioned somewhere in the tracker, the key point is
> > that there's one expectation file for all toolsets.
>
> Oh, I missed that. Since in QMTest expectations are simply results of prior
> test executions, my initial design proposal is to provide the means to
> annotate result files. (Nothing is stopping you from using the same result
> file as expectation in multiple toolsets.)
>
> Does this make sense ?

You mean "this test results are for configuration 1"? That would make sense. 
However
- last time I checked, expectations loading was hardcoded logic to test either 
for xml file or for pickled data. Maybe this changed, but if not, this is not 
particularly flexible. What if I have my own way to describe expectations.
- If xml/pickle format is OK for now, there should be some way to edit it with 
a GUI. Loading results for each configuration, editing them and saving would 
be very inconvenient. Say I want to mark a test as failing for gcc-2.95, 2.96 
and 2.95-stlport.

This suggests we probably need a format for expectation file that can contain 
several sets of results. So, QMTest will load the file and ask for results 
for a specific tested configuration. 

- Volodya


From nmarais at snowisp.com  Wed Apr  6 08:31:03 2005
From: nmarais at snowisp.com (Neilen Marais)
Date: Wed, 06 Apr 2005 10:31:03 +0200
Subject: [qmtest] qmtest documentation
In-Reply-To: <42530875.6070601@codesourcery.com>
References: <1112731017.20280.33.camel@wanbalans> <42530875.6070601@codesourcery.com>
Message-ID: <1112776263.4161.9.camel@wanbalans>

Hi Mark,

On Tue, 2005-04-05 at 14:51 -0700, Mark Mitchell wrote:
> Neilen Marais wrote:
> 
> > Is the manual linked from http://www.codesourcery.com/qmtest/ the only
> > available user documentation? If this does not answer my questions,
> 
> Yes, I'm afraid so.

> > where do I look? Can I ask my silly n00b questions here?
> Sure!

Cool, thanks, expect the questions to come along. I have a suggestion
though. Perhaps you should set up a wiki, or similar for QMTest. I'd be
more than happy to chronicle some of my (projected) successes on such a
site. This would certainly go some way to addressing the lack of
hand-holding documentation :)

Thanks
Neilen





From nmarais at snowisp.com  Wed Apr  6 09:51:26 2005
From: nmarais at snowisp.com (Neilen Marais)
Date: Wed, 06 Apr 2005 11:51:26 +0200
Subject: Python unit tests
Message-ID: <1112781086.4161.28.camel@wanbalans>

Hi!

I'm trying to do unit testing as I develop some python apps that support
the fortran code I mentioned before. The way I assumed it should work,
is to use one of the python test classes, then to import my .py file,
and do some tests etc.  But I can't seem to figure out where to put my
python source, I always get: 

qmtest.exception  exceptions.ImportError: No module named outfilecomp

I have a test database in a directory ...../tests. I put my .py file
there, which seems to be the CWD when QMTest is running python tests.
Then I use (in this instance) a python.ExecTest. My test code looks
something like this:

import outfilecomp

testvar = outfilecomp.blah(5)

where blah is just a short circuit testing function that returns its
argument, and the python expression is:

testvar == 5

Am I missing something obvious? Where should I put my .py file, or where
can I specify the load path? It seems like a bad idea to hardcode it
into my tests. To me the obvious solution seems to be to specify this in
a context variable. But how do I get to access the context variable
short of A) writing my own test class or B) accessing it as environment
variables inside the test code.

A) is unattractive, since I've never written a QMTest class, and it
seems like something that should have been solved already ;)
B) is unattractive, since that would clutter every test case with code
to read environment variables. Not a lot of code, but still unnecesary
repetition.

If I'm approaching this wrong, please hit me with a clue-stick. BTW, I'm
using version 2.2 from Debian Sarge, which seems to be the newest
version.

Thanks
Neilen



From slowrey at nextone.com  Wed Apr  6 12:10:52 2005
From: slowrey at nextone.com (Scott Lowrey)
Date: Wed, 06 Apr 2005 08:10:52 -0400
Subject: [qmtest] qmtest documentation
In-Reply-To: <1112776263.4161.9.camel@wanbalans>
References: <1112731017.20280.33.camel@wanbalans> <42530875.6070601@codesourcery.com> <1112776263.4161.9.camel@wanbalans>
Message-ID: <4253D1CC.5070302@nextone.com>

Neilen Marais wrote:

> Cool, thanks, expect the questions to come along. I have a suggestion
>
>though. Perhaps you should set up a wiki, or similar for QMTest. 
>  
>
That's a good idea.  My company has been using QMTest for about a year 
now and I'd be happy to write up some of our experiences for the good of 
the community. 

-Scott


From seefeld at sympatico.ca  Wed Apr  6 08:51:30 2005
From: seefeld at sympatico.ca (Stefan Seefeld)
Date: Wed, 06 Apr 2005 04:51:30 -0400
Subject: [qmtest] The state of GUI
In-Reply-To: <200504061105.46779.ghost@cs.msu.su>
References: <200504041141.22921.ghost@cs.msu.su> <200504051035.10809.ghost@cs.msu.su> <42526B8C.9000309@codesourcery.com> <200504061105.46779.ghost@cs.msu.su>
Message-ID: <4253A312.9030605@sympatico.ca>

Vladimir Prus wrote:

> You mean "this test results are for configuration 1"? That would make sense. 
> However
> - last time I checked, expectations loading was hardcoded logic to test either 
> for xml file or for pickled data. Maybe this changed, but if not, this is not 
> particularly flexible. What if I have my own way to describe expectations.

I think the idea is that expectations (at least in the context of regression tests)
are the result of an initial (or former) test run, and thus QMTest uses its own
result files as expectation input.

> - If xml/pickle format is OK for now, there should be some way to edit it with 
> a GUI. Loading results for each configuration, editing them and saving would 
> be very inconvenient. Say I want to mark a test as failing for gcc-2.95, 2.96 
> and 2.95-stlport.

I can see that, yes.

> This suggests we probably need a format for expectation file that can contain 
> several sets of results. So, QMTest will load the file and ask for results 
> for a specific tested configuration. 

I'm not sure whether the best solution is indeed a new file format. May be it
would be simpler to expand on the idea of a report as a view on results from
multiple configurations and then provide some convenience interface (GUI ?)
to set annotations to multiple result files at once ? (you still need to
provide a list of all those configurations to which you do want the change
to propagate...)

Regards,
		Stefan


From mark at codesourcery.com  Wed Apr  6 16:20:58 2005
From: mark at codesourcery.com (Mark Mitchell)
Date: Wed, 06 Apr 2005 09:20:58 -0700
Subject: [qmtest] The state of GUI
In-Reply-To: <200504061105.46779.ghost@cs.msu.su>
References: <200504041141.22921.ghost@cs.msu.su> <200504051035.10809.ghost@cs.msu.su> <42526B8C.9000309@codesourcery.com> <200504061105.46779.ghost@cs.msu.su>
Message-ID: <42540C6A.7090407@codesourcery.com>

Vladimir Prus wrote:

> You mean "this test results are for configuration 1"? 

I'm not convinced that extending the results file format to contain 
multiple runs is that good of an idea.  The way I would expect the 2.95 
vs. 2.95+stlport vs. ... expectations would be handled is by simply 
running the testsuite is those configurations and saving the results 
files.  The GUI allows you to edit expectations, but the recommended way 
of getting expectations set up is just running the testsuite.

Is that inconvenient?

-- 
Mark Mitchell
CodeSourcery, LLC
mark at codesourcery.com
(916) 791-8304


From dwcraig at qualcomm.com  Wed Apr  6 16:40:39 2005
From: dwcraig at qualcomm.com (Craig, Dave)
Date: Wed, 6 Apr 2005 09:40:39 -0700
Subject: [qmtest] The state of GUI
Message-ID: <0320111483D8B84AAAB437215BBDA52601494926@NAEX01.na.qualcomm.com>

Multiple results would be useful to get an idea of the reliability of a
test case.  A result may fail 100% of the time, 63% of the time, etc.  A
failure percentage seems to be a more reliable result when there are
poorly designed test cases.

Expectation based on a previous run will report changes for
intermittently passing test cases.  If the expectation were a little
richer, FAIL 100%, PASS 100%, and intermittent pass/fail, the reports
would reflect more significant changes.

Thanks,
     Dave

-----Original Message-----
From: Mark Mitchell [mailto:mark at codesourcery.com] 
Sent: Wednesday, April 06, 2005 9:21 AM
To: Vladimir Prus
Cc: qmtest at codesourcery.com
Subject: Re: [qmtest] The state of GUI

Vladimir Prus wrote:

> You mean "this test results are for configuration 1"? 

I'm not convinced that extending the results file format to contain 
multiple runs is that good of an idea.  The way I would expect the 2.95 
vs. 2.95+stlport vs. ... expectations would be handled is by simply 
running the testsuite is those configurations and saving the results 
files.  The GUI allows you to edit expectations, but the recommended way

of getting expectations set up is just running the testsuite.

Is that inconvenient?

-- 
Mark Mitchell
CodeSourcery, LLC
mark at codesourcery.com
(916) 791-8304


From slowrey at nextone.com  Wed Apr  6 17:40:14 2005
From: slowrey at nextone.com (Scott Lowrey)
Date: Wed, 06 Apr 2005 13:40:14 -0400
Subject: [qmtest] Python unit tests
In-Reply-To: <1112781086.4161.28.camel@wanbalans>
References: <1112781086.4161.28.camel@wanbalans>
Message-ID: <42541EFE.5050601@nextone.com>


Neilen Marais wrote:

>Hi!
>
>I'm trying to do unit testing as I develop some python apps that support
>the fortran code I mentioned before. The way I assumed it should work,
>is to use one of the python test classes, then to import my .py file,
>and do some tests etc.  But I can't seem to figure out where to put my
>python source, I always get: 
>
>qmtest.exception  exceptions.ImportError: No module named outfilecomp
>
>  
>
(Sorry, my other responses were not formatted correctly for the qmtest 
list server.)

You should put custom Python modules in lib/python2.x/site-packages.  If 
you don't you'll have to define the environment variable PYTHONPATH to 
point to the directory where your stuff is.

It seems that, in this case, you haven't actually written a QMTest 
extension.  That's different.  You have an API to conform to and a 
registration process to complete before QMTest will use the extension.  
See the manual for details.

-- 
*Scott Lowrey*
Test Engineering Manager
NexTone Communications <http://nextone.com>
Gaithersburg, Maryland USA

/1.240.912.1369/



From mark at codesourcery.com  Wed Apr  6 18:35:14 2005
From: mark at codesourcery.com (Mark Mitchell)
Date: Wed, 06 Apr 2005 11:35:14 -0700
Subject: [qmtest] Python unit tests
In-Reply-To: <1112781086.4161.28.camel@wanbalans>
References: <1112781086.4161.28.camel@wanbalans>
Message-ID: <42542BE2.7080304@codesourcery.com>

Neilen Marais wrote:

> B) is unattractive, since that would clutter every test case with code
> to read environment variables. Not a lot of code, but still unnecesary
> repetition.

I think that's the best solution at the moment.  Because these are 
ExecTests, they're designed to run arbitrary programs; the fact that 
they happen to be written in Python, like QMTest, is just coincidence. 
So, there's nothing for QMTest to do to set up the load path.

Now, you could of course build a test class designed especially for 
running Python files.  You could derive from the ExecTest class, and 
override MakeEnvironment to set up PYTHONPATH.

-- 
Mark Mitchell
CodeSourcery, LLC
mark at codesourcery.com
(916) 791-8304


From mark at codesourcery.com  Wed Apr  6 22:32:51 2005
From: mark at codesourcery.com (Mark Mitchell)
Date: Wed, 06 Apr 2005 15:32:51 -0700
Subject: [qmtest] qmtest documentation
In-Reply-To: <1112776263.4161.9.camel@wanbalans>
References: <1112731017.20280.33.camel@wanbalans> <42530875.6070601@codesourcery.com> <1112776263.4161.9.camel@wanbalans>
Message-ID: <42546393.4020608@codesourcery.com>

Neilen Marais wrote:

> Cool, thanks, expect the questions to come along. I have a suggestion
> though. Perhaps you should set up a wiki, or similar for QMTest. I'd be
> more than happy to chronicle some of my (projected) successes on such a
> site. This would certainly go some way to addressing the lack of
> hand-holding documentation :)

We've decided to set up a FAQ for QMTest.

While not quite as flexible as a Wiki, in that people can't add stuff 
directly, we'll be able to organize things a bit better, and avoid legal 
issues that arise with Wikis.  We're also going to create a category in 
the QMTest issue-tracker just for questions; that way when you put a 
question there, we'll try to put an answer in the FAQ.

Thanks for the suggestion,

-- 
Mark Mitchell
CodeSourcery, LLC
mark at codesourcery.com
(916) 791-8304


From ghost at cs.msu.su  Thu Apr  7 06:14:44 2005
From: ghost at cs.msu.su (Vladimir Prus)
Date: Thu, 7 Apr 2005 10:14:44 +0400
Subject: [qmtest] The state of GUI
In-Reply-To: <42540C6A.7090407@codesourcery.com>
References: <200504041141.22921.ghost@cs.msu.su> <200504061105.46779.ghost@cs.msu.su> <42540C6A.7090407@codesourcery.com>
Message-ID: <200504071014.45699.ghost@cs.msu.su>

On Wednesday 06 April 2005 20:20, Mark Mitchell wrote:
> Vladimir Prus wrote:
> > You mean "this test results are for configuration 1"?
>
> I'm not convinced that extending the results file format to contain
> multiple runs is that good of an idea.  The way I would expect the 2.95
> vs. 2.95+stlport vs. ... expectations would be handled is by simply
> running the testsuite is those configurations and saving the results
> files.  The GUI allows you to edit expectations, but the recommended way
> of getting expectations set up is just running the testsuite.
>
> Is that inconvenient?

Kind of. 

<aside>
I'm not saying that QMTest should take Boost requirements as the only target, 
and in fact I'm not sure many Boosters will be interested in using QMTest, 
and there are some deeper issues than just annotation format (Stefan should 
remember), but Boost is the good example for multi-platform project.
</aside>

Yes, it's easy to store results for each configuration and then use them as 
expectations. But I think we use different meaning of "expected" failure.
You take in as the failure which present in some previous run. I take it as a 
failure which was present in some previous run, and for which the developer 
has explicitly decided that this failure should not be fixed, and so for all 
subsequence run the failures should not specially announced.

For Boost, with 10 different compilers some of which are rather broken, this 
approach is a lot better then looking at all failures and trying to recall 
which must be fixed.

So:
1. Even for initial tests run, this is not good. One must be able to look at 
failures and decide which are expected.

2. If new tests are added, expectations must be again set manually. If you 
just rerun tests and use the results, then everybody must reevaluate all 
tests failures.

3. Each test result is for specific compiler version. I really don't want to 
explicitly edit test results for each version of some broken compiler -- I 
want to specify that some compiler is broken.

The above means that editing expectation is not so uncommon expectation. I 
also manually edit them for my project at work, for the same reason. And I 
suppose some people would be more comfortable editing text file, as opposed 
to using GUI, so allowing to specify expectations in a text file would be 
nice.

The point 3) means that it might be good to specify expectations independently 
from specific test results.

OTOH, it might be reasonable to start with simpler model of just a collection 
of expected result files.

- Volodya







From nmarais at snowisp.com  Thu Apr  7 09:02:55 2005
From: nmarais at snowisp.com (Neilen Marais)
Date: Thu, 07 Apr 2005 11:02:55 +0200
Subject: [qmtest] Python unit tests
In-Reply-To: <42541EFE.5050601@nextone.com>
References: <1112781086.4161.28.camel@wanbalans> <42541EFE.5050601@nextone.com>
Message-ID: <1112864575.4161.38.camel@wanbalans>

Hi Scott!

On Wed, 2005-04-06 at 13:40 -0400, Scott Lowrey wrote:

> >python source, I always get: 
> >
> >qmtest.exception  exceptions.ImportError: No module named outfilecomp
> >
> >  
> >
> (Sorry, my other responses were not formatted correctly for the qmtest 
> list server.)
> 
> You should put custom Python modules in lib/python2.x/site-packages.  If 
> you don't you'll have to define the environment variable PYTHONPATH to 
> point to the directory where your stuff is.

Ah, yes, thanks! Setting PYTHONPATH is the way to go about it I think.
Putting things in /lib/... would require root priviliges, and be a
little messy. Setting PYTHONPATH gets te desired effect.

Thanks
Neilen



From seefeld at sympatico.ca  Thu Apr  7 09:13:52 2005
From: seefeld at sympatico.ca (Stefan Seefeld)
Date: Thu, 07 Apr 2005 05:13:52 -0400
Subject: [qmtest] The state of GUI
In-Reply-To: <200504071014.45699.ghost@cs.msu.su>
References: <200504041141.22921.ghost@cs.msu.su> <200504061105.46779.ghost@cs.msu.su> <42540C6A.7090407@codesourcery.com> <200504071014.45699.ghost@cs.msu.su>
Message-ID: <4254F9D0.7090805@sympatico.ca>

Vladimir Prus wrote:

> 1. Even for initial tests run, this is not good. One must be able to look at 
> failures and decide which are expected.

Would the ability to generate an expectation without actually running a test
do what you want, especially if we add the possibility to annotate it ?

> 2. If new tests are added, expectations must be again set manually. If you 
> just rerun tests and use the results, then everybody must reevaluate all 
> tests failures.

Not if you distribute not only the tests but also the expectation files,
which at least one developer needs to create by running the test or otherwise.

I'm not sure I understand your concern here, as the first thing I do when writing
a new test is somehow expressing what result I expect. It's just that QMTest
until now requires you to run the test at least once to do that.

> 3. Each test result is for specific compiler version. I really don't want to 
> explicitly edit test results for each version of some broken compiler -- I 
> want to specify that some compiler is broken.

At what granularity do you want to indicate the brokenness ? If it is not
per test, how else ? (Why running the test suite on that compiler if 'broken'
is not per-test ?)

> The above means that editing expectation is not so uncommon expectation. I 
> also manually edit them for my project at work, for the same reason. And I 
> suppose some people would be more comfortable editing text file, as opposed 
> to using GUI, so allowing to specify expectations in a text file would be 
> nice.

I believe there is a xml-based result format that QMTest understands.

Regards,
		Stefan


From ghost at cs.msu.su  Thu Apr  7 13:44:31 2005
From: ghost at cs.msu.su (Vladimir Prus)
Date: Thu, 7 Apr 2005 17:44:31 +0400
Subject: [qmtest] The state of GUI
In-Reply-To: <4254F9D0.7090805@sympatico.ca>
References: <200504041141.22921.ghost@cs.msu.su> <200504071014.45699.ghost@cs.msu.su> <4254F9D0.7090805@sympatico.ca>
Message-ID: <200504071744.32031.ghost@cs.msu.su>

On Thursday 07 April 2005 13:13, Stefan Seefeld wrote:
> Vladimir Prus wrote:
> > 1. Even for initial tests run, this is not good. One must be able to look
> > at failures and decide which are expected.
>
> Would the ability to generate an expectation without actually running a
> test do what you want, 

Well, I think I can do it with a GUI already :-/ Not sure though.

> > 2. If new tests are added, expectations must be again set manually. If
> > you just rerun tests and use the results, then everybody must reevaluate
> > all tests failures.
>
> Not if you distribute not only the tests but also the expectation files,
> which at least one developer needs to create by running the test or
> otherwise.
>
> I'm not sure I understand your concern here, as the first thing I do when
> writing a new test is somehow expressing what result I expect. It's just
> that QMTest until now requires you to run the test at least once to do
> that.

I'm just arguing with the statement that one can just use results of a prior 
test run as expected result, without any manual editing. Imagine that I have 
just one configuration and an expected results file -- that contains manually 
changed expectation. If I add new test, rerun tests, and replace the 
expectations file, I loose the manually set expectations already stored 
there.

> > 3. Each test result is for specific compiler version. I really don't want
> > to explicitly edit test results for each version of some broken compiler
> > -- I want to specify that some compiler is broken.
>
> At what granularity do you want to indicate the brokenness ? If it is not
> per test, how else ? 

test/compiler_vendor combination. Say, "this tests fails on all versions of 
yfc", as opposed to explicitly listing 15 versions of "yfc" where the test 
fails.

> (Why running the test suite on that compiler if 
> 'broken' is not per-test ?)
>
> > The above means that editing expectation is not so uncommon expectation.
> > I also manually edit them for my project at work, for the same reason.
> > And I suppose some people would be more comfortable editing text file, as
> > opposed to using GUI, so allowing to specify expectations in a text file
> > would be nice.
>
> I believe there is a xml-based result format that QMTest understands.

Yea, but if people decide that editing 15 files for 15 versions of "yfc" is 
not too nice (even with GUI), they should be able to do something. Like using 
custom file format.

- Volodya


From seefeld at sympatico.ca  Thu Apr  7 09:57:11 2005
From: seefeld at sympatico.ca (Stefan Seefeld)
Date: Thu, 07 Apr 2005 05:57:11 -0400
Subject: [qmtest] The state of GUI
In-Reply-To: <200504071744.32031.ghost@cs.msu.su>
References: <200504041141.22921.ghost@cs.msu.su> <200504071014.45699.ghost@cs.msu.su> <4254F9D0.7090805@sympatico.ca> <200504071744.32031.ghost@cs.msu.su>
Message-ID: <425503F7.3070700@sympatico.ca>

Vladimir Prus wrote:

>>I'm not sure I understand your concern here, as the first thing I do when
>>writing a new test is somehow expressing what result I expect. It's just
>>that QMTest until now requires you to run the test at least once to do
>>that.
> 
> 
> I'm just arguing with the statement that one can just use results of a prior 
> test run as expected result, without any manual editing. Imagine that I have 
> just one configuration and an expected results file -- that contains manually 
> changed expectation. If I add new test, rerun tests, and replace the 
> expectations file, I loose the manually set expectations already stored 
> there.

Ok, I see your point now.

>>At what granularity do you want to indicate the brokenness ? If it is not
>>per test, how else ? 
> 
> 
> test/compiler_vendor combination. Say, "this tests fails on all versions of 
> yfc", as opposed to explicitly listing 15 versions of "yfc" where the test 
> fails.

I don't think it's QMTest's job to keep track of a toolchain taxonomy.

> Yea, but if people decide that editing 15 files for 15 versions of "yfc" is 
> not too nice (even with GUI), they should be able to do something. Like using 
> custom file format.

I don't think a custom file format is a good idea, as I don't believe QMTest
itself should care about these things. However, once we have an API to manipulate
expectation files it should be easy for you to define some aliasing mechanism
that lets you make modifications that get propagated to all instances that are
in your alias list. But again, that would be on top of such a 'QMTest API'.

Regards,
		Stefan


From mark at codesourcery.com  Thu Apr  7 15:51:54 2005
From: mark at codesourcery.com (Mark Mitchell)
Date: Thu, 07 Apr 2005 08:51:54 -0700
Subject: [qmtest] The state of GUI
In-Reply-To: <200504071744.32031.ghost@cs.msu.su>
References: <200504041141.22921.ghost@cs.msu.su> <200504071014.45699.ghost@cs.msu.su> <4254F9D0.7090805@sympatico.ca> <200504071744.32031.ghost@cs.msu.su>
Message-ID: <4255571A.3020308@codesourcery.com>

Vladimir Prus wrote:

> Yea, but if people decide that editing 15 files for 15 versions of "yfc" is 
> not too nice (even with GUI), they should be able to do something. Like using 
> custom file format.

When Stefan and I talked about this yesterday, we decided that the most 
consistent thing would be to add command-line functionality for 
adding/removing/updating/constructing results in a results file.  Then, 
you would just use our friend the shell:

   for x in *.qmr; do
     qmtest <some command-line here> $x
   done

For example, one primitive might be "merge".  The idea is that you would 
provide N results files, and the output would be a results files 
containing all of the results in the N files, with results in later 
files overridding earlier ones.  So, you would run the test once, to get 
a failure, or create a results file with a failure in some other way 
(say, via the GUI), and then do:

   for x in *.qmr; do
     qmtest merge $x fail.qmr
   done

Would that work for you?

-- 
Mark Mitchell
CodeSourcery, LLC
mark at codesourcery.com
(916) 791-8304


From nmarais at snowisp.com  Mon Apr 11 17:18:57 2005
From: nmarais at snowisp.com (Neilen Marais)
Date: Mon, 11 Apr 2005 19:18:57 +0200
Subject: QMTest with Ubuntu
Message-ID: <1113239937.17755.11.camel@wanbalans>

Hi!

Are there known issues using QMTest with Ubuntu?  I was using it before
with Debian Sarge, and am now trying it with Ubuntu Hoary. I'm using the
ubuntu universe package of QMTest. The only real difference I can see
between my Debian and Ubuntu setups are that Sarge defaults to python
2.3, and ubuntu 2.4. Can QMTest work with python 2.4? 

I'm still feeling my way around with the GUI mode. The symptom is that,
whenever I try to view or edit any of my tests, I get the following:

You Have Found a Bug in QMTest!
This page indicates that QMTest has encountered an internal error.

You can return to the main page.


________________________________________________________________________

Exception type: exceptions.UnboundLocalError

Exception value: local variable 'item' referenced before assignment

Stack trace:

  File "/usr/lib/qm/web.py", line 476, in __HandleScriptRequest
    script_output = self.server.ProcessScript(request)

  File "/usr/lib/qm/web.py", line 821, in ProcessScript
    return self.__scripts[request.GetUrl()](request)

  File "/usr/lib/qm/test/web/web.py", line 2066, in HandleShowItem
    return ShowItemPage(self, item, edit, create, type)(request)

or similar.

Thanks
Neilen



From stefan at codesourcery.com  Mon Apr 11 13:50:28 2005
From: stefan at codesourcery.com (Stefan Seefeld)
Date: Mon, 11 Apr 2005 09:50:28 -0400
Subject: [qmtest] QMTest with Ubuntu
In-Reply-To: <1113239937.17755.11.camel@wanbalans>
References: <1113239937.17755.11.camel@wanbalans>
Message-ID: <425A80A4.7040407@codesourcery.com>

Hi Neilen,

thanks for the bug report !

Neilen Marais wrote:
> Can QMTest work with python 2.4? 

The bug is known and has been fixed in the repository.
We are considering spinning a 2.2.1 release in the not-so-distant
future to support Python 2.4.

Regards,
		Stefan


From nmarais at snowisp.com  Tue Apr 12 15:51:23 2005
From: nmarais at snowisp.com (Neilen Marais)
Date: Tue, 12 Apr 2005 17:51:23 +0200
Subject: [qmtest] qmtest documentation
In-Reply-To: <42546393.4020608@codesourcery.com>
References: <1112731017.20280.33.camel@wanbalans> <42530875.6070601@codesourcery.com> <1112776263.4161.9.camel@wanbalans> <42546393.4020608@codesourcery.com>
Message-ID: <1113321083.23313.21.camel@wanbalans>

Hi Mark

On Wed, 2005-04-06 at 15:32 -0700, Mark Mitchell wrote:
> Neilen Marais wrote:

> We've decided to set up a FAQ for QMTest.

Where would one find this?

> While not quite as flexible as a Wiki, in that people can't add stuff 
> directly, we'll be able to organize things a bit better, and avoid legal 
> issues that arise with Wikis.  

What kind of legal issues? Many other FLOSS projects seem to get by with
wikis!

> We're also going to create a category in 
> the QMTest issue-tracker just for questions; that way when you put a 
> question there, we'll try to put an answer in the FAQ.

OK, so I ask in QMTest, and when you have time you answer in the FAQ. Is
this up yet?

BTW, perhaps someone should put up an "unofficial" wiki. It certainly
has different strengths as compared the FAQ approach. Actually, I can
easily see the need for both...
> 
> Thanks for the suggestion,
> 
No worries :)

Cheers
Neilen

Cheers
Neilen



From sram at qualcomm.com  Tue Apr 12 16:54:21 2005
From: sram at qualcomm.com (Siddharth Ram)
Date: Tue, 12 Apr 2005 09:54:21 -0700
Subject: qmtest resource related error
Message-ID: <425BFD3D.8050703@qualcomm.com>

I have a question/problem related to resources.

I have defined the following resource. It is supposed to spawn a process 
(named Server) on the local machine with which my test interacts

<?xml version="1.0" ?>
<!DOCTYPE extension
  PUBLIC '-//QM/2.2 - or-changes/Extension//EN'
  'http://www.codesourcery.com/qm/dtds/2.2 - or-changes/-//qm/2.2 - 
or-changes/extension//en.dtd'>
<extension class="process_resource.ProcessResource" 
kind="resource"><argument 
name="delay"><integer>0</integer></argument><argument 
name="path"><text>/test/qmtest-db/orbtest.qms/pytest.qms/servant/Server</text></argument><argument 
name="args"><set/></argument><argument 
name="resources"><set/></argument></extension>

When I run the test, the server seems to start up fine, receives 
requests from the client, but threequarters of the way through the test, 
it crashes with a stack trace as below:
Any help would be appreciated.
Also, documentation on resources seems to be pretty thin.. could someone 
point me to where it might be documented ?

thanks
Siddharth
-------------------------------------
Traceback (most recent call last):
  File "/opt/or-buildtools/bin/qmtest", line 190, in ?
    exit_code = main()
  File "/opt/or-buildtools/bin/qmtest", line 168, in main
    exit_code = command.Execute()
  File "/opt/or-buildtools/lib/qm/test/cmdline.py", line 661, in Execute
    return method()
  File "/opt/or-buildtools/lib/qm/test/cmdline.py", line 1374, in 
__ExecuteRun
    if engine.Run():
  File "/opt/or-buildtools/lib/qm/test/execution_engine.py", line 288, 
in Run
    target.Stop()
  File "/opt/or-buildtools/lib/qm/test/classes/process_target.py", line 
179, in Stop
    self.__ReadResults(self.__busy_children[0][1].fileno())
  File "/opt/or-buildtools/lib/qm/test/classes/process_target.py", line 
230, in __ReadResults
    results = cPickle.load(child[1])
cPickle.UnpicklingError: could not find MARK
[root at sram qmtest-db]# (27842|4143949504) ready to 
Compute(27842|4143949504|SCHED_FIFO|2): DONE with Compute Area servant

Traceback (most recent call last):
  File "/opt/or-buildtools/bin/qmtest", line 190, in ?
    exit_code = main()
  File "/opt/or-buildtools/bin/qmtest", line 168, in main
    exit_code = command.Execute()
  File "/opt/or-buildtools/lib/qm/test/cmdline.py", line 661, in Execute
    return method()
  File "/opt/or-buildtools/lib/qm/test/cmdline.py", line 1292, in 
__ExecuteRemote
    sys.stdout.flush()
IOError: [Errno 32] Broken pipe





From mark at codesourcery.com  Thu Apr 14 23:07:58 2005
From: mark at codesourcery.com (Mark Mitchell)
Date: Thu, 14 Apr 2005 16:07:58 -0700
Subject: [qmtest] qmtest documentation
In-Reply-To: <1113321083.23313.21.camel@wanbalans>
References: <1112731017.20280.33.camel@wanbalans> <42530875.6070601@codesourcery.com> <1112776263.4161.9.camel@wanbalans> <42546393.4020608@codesourcery.com> <1113321083.23313.21.camel@wanbalans>
Message-ID: <425EF7CE.109@codesourcery.com>

Neilen Marais wrote:
> Hi Mark
> 
> On Wed, 2005-04-06 at 15:32 -0700, Mark Mitchell wrote:
> 
>>Neilen Marais wrote:
> 
> 
>>We've decided to set up a FAQ for QMTest.
> 
> 
> Where would one find this?

It's not there yet.  It will be on our web site, next to the other 
QMTest things, when has been populated.

>>We're also going to create a category in 
>>the QMTest issue-tracker just for questions; that way when you put a 
>>question there, we'll try to put an answer in the FAQ.
> 
> 
> OK, so I ask in QMTest, and when you have time you answer in the FAQ. Is
> this up yet?

Stefan, have you had a chance to set up the questions category in 
Roundup, yet?

-- 
Mark Mitchell
CodeSourcery, LLC
mark at codesourcery.com
(916) 791-8304


From stefan at codesourcery.com  Fri Apr 15 15:55:18 2005
From: stefan at codesourcery.com (Stefan Seefeld)
Date: Fri, 15 Apr 2005 11:55:18 -0400
Subject: [qmtest] qmtest documentation
In-Reply-To: <425EF7CE.109@codesourcery.com>
References: <1112731017.20280.33.camel@wanbalans> <42530875.6070601@codesourcery.com> <1112776263.4161.9.camel@wanbalans> <42546393.4020608@codesourcery.com> <1113321083.23313.21.camel@wanbalans> <425EF7CE.109@codesourcery.com>
Message-ID: <425FE3E6.3030408@codesourcery.com>

Mark Mitchell wrote:
> Neilen Marais wrote:

>> OK, so I ask in QMTest, and when you have time you answer in the FAQ. Is
>> this up yet?
> 
> 
> Stefan, have you had a chance to set up the questions category in 
> Roundup, yet?

Please file any documentation-related issues at
https://support.codesourcery.com/QMTest/ as a new submission, using
the component 'documentation'.

Thanks,
		Stefan




From ghost at cs.msu.su  Mon Apr 18 06:42:38 2005
From: ghost at cs.msu.su (Vladimir Prus)
Date: Mon, 18 Apr 2005 10:42:38 +0400
Subject: [qmtest] The state of GUI
In-Reply-To: <425503F7.3070700@sympatico.ca>
References: <200504041141.22921.ghost@cs.msu.su> <200504071744.32031.ghost@cs.msu.su> <425503F7.3070700@sympatico.ca>
Message-ID: <200504181042.38832.ghost@cs.msu.su>

On Thursday 07 April 2005 13:57, Stefan Seefeld wrote:

> > test/compiler_vendor combination. Say, "this tests fails on all versions
> > of yfc", as opposed to explicitly listing 15 versions of "yfc" where the
> > test fails.
>
> I don't think it's QMTest's job to keep track of a toolchain taxonomy.

No, but ideally there should be a an extension mechanism I can use to define 
expectation in any way I like.

>
> > Yea, but if people decide that editing 15 files for 15 versions of "yfc"
> > is not too nice (even with GUI), they should be able to do something.
> > Like using custom file format.
>
> I don't think a custom file format is a good idea, as I don't believe
> QMTest itself should care about these things. However, once we have an API
> to manipulate expectation files it should be easy for you to define some
> aliasing mechanism that lets you make modifications that get propagated to
> all instances that are in your alias list. But again, that would be on top
> of such a 'QMTest API'.

If fact, if there's "qmtest report" that takes N test results and N expected 
result and produces a report, then I can write a script that takes N toolset 
names and a expectation file in Boost format and produces N separate expected 
result files, which can be used by "qmtest report".

Seems workable.

- Volodya


From ghost at cs.msu.su  Mon Apr 18 06:46:05 2005
From: ghost at cs.msu.su (Vladimir Prus)
Date: Mon, 18 Apr 2005 10:46:05 +0400
Subject: [qmtest] The state of GUI
In-Reply-To: <4255571A.3020308@codesourcery.com>
References: <200504041141.22921.ghost@cs.msu.su> <200504071744.32031.ghost@cs.msu.su> <4255571A.3020308@codesourcery.com>
Message-ID: <200504181046.05601.ghost@cs.msu.su>

On Thursday 07 April 2005 19:51, Mark Mitchell wrote:
> Vladimir Prus wrote:
> > Yea, but if people decide that editing 15 files for 15 versions of "yfc"
> > is not too nice (even with GUI), they should be able to do something.
> > Like using custom file format.
>
> When Stefan and I talked about this yesterday, we decided that the most
> consistent thing would be to add command-line functionality for
> adding/removing/updating/constructing results in a results file.  Then,
> you would just use our friend the shell:
>
>    for x in *.qmr; do
>      qmtest <some command-line here> $x
>    done
>
> For example, one primitive might be "merge".  The idea is that you would
> provide N results files, and the output would be a results files
> containing all of the results in the N files, with results in later
> files overridding earlier ones.  So, you would run the test once, to get
> a failure, or create a results file with a failure in some other way
> (say, via the GUI), and then do:
>
>    for x in *.qmr; do
>      qmtest merge $x fail.qmr
>    done
>
> Would that work for you?

I though about it and think that if I have a file describing expectation for 
all possible toolset, and QMTest requires N expectation files, each for a 
specific toolset, then nothing stops me from writing a script that will 
produce those N expectation files. I'm not even sure that command like 
interface for adding expecation is needed; for me, Python API is file.

- Volodya







From seefeld at sympatico.ca  Mon Apr 18 13:19:43 2005
From: seefeld at sympatico.ca (Stefan Seefeld)
Date: Mon, 18 Apr 2005 09:19:43 -0400
Subject: [qmtest] The state of GUI
In-Reply-To: <200504181042.38832.ghost@cs.msu.su>
References: <200504041141.22921.ghost@cs.msu.su> <200504071744.32031.ghost@cs.msu.su> <425503F7.3070700@sympatico.ca> <200504181042.38832.ghost@cs.msu.su>
Message-ID: <4263B3EF.5090308@sympatico.ca>

Vladimir Prus wrote:
> On Thursday 07 April 2005 13:57, Stefan Seefeld wrote:

>>I don't think it's QMTest's job to keep track of a toolchain taxonomy.
> 
> 
> No, but ideally there should be a an extension mechanism I can use to define 
> expectation in any way I like.

The simplest approach would provide the python API to manipulate results
(result files), and users would write their own python scripts using
qmtest's python modules, instead of using the 'qmtest' CLI.
In any way, that's the first step to any solution.

>>>Yea, but if people decide that editing 15 files for 15 versions of "yfc"
>>>is not too nice (even with GUI), they should be able to do something.
>>>Like using custom file format.
>>
>>I don't think a custom file format is a good idea, as I don't believe
>>QMTest itself should care about these things. However, once we have an API
>>to manipulate expectation files it should be easy for you to define some
>>aliasing mechanism that lets you make modifications that get propagated to
>>all instances that are in your alias list. But again, that would be on top
>>of such a 'QMTest API'.
> 
> 
> If fact, if there's "qmtest report" that takes N test results and N expected 
> result and produces a report, then I can write a script that takes N toolset 
> names and a expectation file in Boost format and produces N separate expected 
> result files, which can be used by "qmtest report".
> 
> Seems workable.

I think qmtest should store the expectation within the result file as annotation,
so 'qmtest report' would only have to load N result files.
However, that doesn't appear to be what qmtest does, i.e. while the summary
section indicates the unexpected passes/failures, individual results don't
contain information about whether they were expected or not.
Mark, is that accidental ?

Regards,
		Stefan



From ghost at cs.msu.su  Mon Apr 18 14:27:01 2005
From: ghost at cs.msu.su (Vladimir Prus)
Date: Mon, 18 Apr 2005 18:27:01 +0400
Subject: [qmtest] The state of GUI
In-Reply-To: <4263B3EF.5090308@sympatico.ca>
References: <200504041141.22921.ghost@cs.msu.su> <200504181042.38832.ghost@cs.msu.su> <4263B3EF.5090308@sympatico.ca>
Message-ID: <200504181827.01738.ghost@cs.msu.su>

On Monday 18 April 2005 17:19, Stefan Seefeld wrote:
> Vladimir Prus wrote:
> > On Thursday 07 April 2005 13:57, Stefan Seefeld wrote:
> >>I don't think it's QMTest's job to keep track of a toolchain taxonomy.
> >
> > No, but ideally there should be a an extension mechanism I can use to
> > define expectation in any way I like.
>
> The simplest approach would provide the python API to manipulate results
> (result files), and users would write their own python scripts using
> qmtest's python modules, instead of using the 'qmtest' CLI.
> In any way, that's the first step to any solution.

Sure.

> >>>Yea, but if people decide that editing 15 files for 15 versions of "yfc"
> >>>is not too nice (even with GUI), they should be able to do something.
> >>>Like using custom file format.
> >>
> >>I don't think a custom file format is a good idea, as I don't believe
> >>QMTest itself should care about these things. However, once we have an
> >> API to manipulate expectation files it should be easy for you to define
> >> some aliasing mechanism that lets you make modifications that get
> >> propagated to all instances that are in your alias list. But again, that
> >> would be on top of such a 'QMTest API'.
> >
> > If fact, if there's "qmtest report" that takes N test results and N
> > expected result and produces a report, then I can write a script that
> > takes N toolset names and a expectation file in Boost format and produces
> > N separate expected result files, which can be used by "qmtest report".
> >
> > Seems workable.
>
> I think qmtest should store the expectation within the result file as
> annotation, so 'qmtest report' would only have to load N result files.

So, the procedure would be:

   qmtest run -O expected1.qmr -o results1.qmr
   qmtest run -O expected2.qmr -o results2.qmr
   qmtest report results1.qmr results2.qmr -o report.html

And the "qmtest run" invocation will use the expectedN.qmr files to add 
annotations to test results? I think that's fine.
   
- Volodya


From seefeld at sympatico.ca  Mon Apr 18 14:45:59 2005
From: seefeld at sympatico.ca (Stefan Seefeld)
Date: Mon, 18 Apr 2005 10:45:59 -0400
Subject: [qmtest] The state of GUI
In-Reply-To: <200504181827.01738.ghost@cs.msu.su>
References: <200504041141.22921.ghost@cs.msu.su> <200504181042.38832.ghost@cs.msu.su> <4263B3EF.5090308@sympatico.ca> <200504181827.01738.ghost@cs.msu.su>
Message-ID: <4263C827.8040008@sympatico.ca>

Vladimir Prus wrote:

> So, the procedure would be:
> 
>    qmtest run -O expected1.qmr -o results1.qmr
>    qmtest run -O expected2.qmr -o results2.qmr
>    qmtest report results1.qmr results2.qmr -o report.html
> 
> And the "qmtest run" invocation will use the expectedN.qmr files to add 
> annotations to test results? I think that's fine.

Exactly. To add some detail, you would probably run:

qmtest run -c configuration1 -O expected1.qmr -o results1.qmr
qmtest run -c configuration2 -O expected1.qmr -o results1.qmr

...

qmtest report -o report.xml results1.qmr results2.qmr ...

to generate an xml report. I have a sample xslt file that
generates some html out of this, but as requirements and taste
vary widely users may roll their own.

Also note that the result files the 'report' command takes
as input may come from different platforms, so you could
actually set up a 'result aggregator' that your users submit
their test results to, so they can get included into the report.

Regards,
		Stefan



