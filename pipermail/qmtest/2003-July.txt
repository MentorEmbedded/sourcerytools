From njs at pobox.com  Thu Jul  3 02:53:24 2003
From: njs at pobox.com (Nathaniel Smith)
Date: Wed, 2 Jul 2003 19:53:24 -0700
Subject: [PATCH] Run metadata and first pass at SQL results storage.
Message-ID: <20030703025324.GA11475@njs.dhis.org>

Attached for review.  This patch also renames 'ResultSource' to
'ResultReader', since that better describes the interface we ended up
with.

-- Nathaniel

-- 
"...All of this suggests that if we wished to find a modern-day model
for British and American speech of the late eighteenth century, we could
probably do no better than Yosemite Sam."
-------------- next part --------------
diff -urN --exclude='*~' --exclude='.*' --exclude=CVS --exclude='*.pyo' --exclude='*.pyc' --exclude=build --exclude=GNUmakefile --exclude=config.log --exclude=config.status --exclude=setup_path.py --exclude=qm.sh --exclude=qm.spec qm-clean/ChangeLog qm-ResultSource/ChangeLog
--- qm-clean/ChangeLog	2003-06-27 20:16:03.000000000 -0700
+++ qm-ResultSource/ChangeLog	2003-07-02 19:50:20.000000000 -0700
@@ -1,3 +1,60 @@
+2003-07-02  Nathaniel Smith  <njs at codesourcery.com>
+
+	* qm/test/base.py (load_results): Rename 'ResultSource' to
+	'ResultReader' everywhere.
+	(extension_kinds): Rename "result_source" kind to
+	"result_reader".
+	(__extension_bases): Likewise.  And same for the imports just
+	above.
+	* qm/test/result_source.py: Rename to...
+	* qm/test/result_reader.py: ...this.  Also rename classes, etc.
+	* qm/test/file_result_source.py: Rename to...
+	* qm/test/file_result_reader.py: ...this.  Also rename classes,
+	etc.
+	
+	* qm/common.py (format_time_iso): New function.
+	* qm/db.py: New file.
+	* qm/extension.py (get_class_arguments): Mark's fixes to
+	support diamond inheritance.
+
+	* qm/test/classes/pickle_result_source.py: Remove.
+	(PickleResultSource): Move to...
+	* qm/test/classes/pickle_result_stream.py: ...here.
+	(PickleResultSource): Rename to 'PickleResultReader'.  Also
+	rewrite for new metadata handling regime.
+	(PickleResultStream): Rewrite for new metadata handling regime.
+	* qm/test/classes/xml_result_stream.py: Likewise.
+	(__init__): Use 'super'.
+	* qm/test/classes/xml_result_source.py: Rename to...
+	* qm/test/classes/xml_result_reader.py: ...this.  Also rename
+	classes, etc., and update for new metadata handling regime.
+	
+	* qm/test/result_stream.py (ResultStream.WriteAnnotation): New
+	function.
+	(ResultStream.WriteAllAnnotations): New function.
+	(ResultStream.WriteResult): More detail in docstring.
+	
+	* qm/test/classes/classes.qmc: Add SQL result handling classes,
+	rename 'ResultSource' to 'ResultReader'.
+	
+	* qm/test/cmdline.py (QMTest.__ExecuteSummarize): Copy
+	metadata.
+	* qm/test/execution_engine.py: import time.
+	(ExecutionEngine.Run): Create and write metadata.
+	* qm/test/web.py (StorageResultsStream.__init__): Take
+	arguments.  Use 'super'.  Handle annotations.
+	(StorageResultsStream.GetAnnotations): New method.
+	(StorageResultsStream.WriteAnnotations: New method.
+	(QMTestServer.__init__): Pass arguments to
+	'StorageResultsStream.__init__'.
+	(QMTestServer.HandleClearResults): Likewise.
+	(QMTestServer.HandleSubmitResults): Likewise.  Write out
+	annotations.
+	(QMTestServer.HandleSaveResults): Write out annotations.
+	
+	* scripts/create-test-database.py: New file.
+	* qm/test/classes/sql_result_stream.py: New file.
+	
 2003-06-27  Nathaniel Smith  <njs at codesourcery.com>
 
 	* qm/test/classes/classes.qmc: Add
diff -urN --exclude='*~' --exclude='.*' --exclude=CVS --exclude='*.pyo' --exclude='*.pyc' --exclude=build --exclude=GNUmakefile --exclude=config.log --exclude=config.status --exclude=setup_path.py --exclude=qm.sh --exclude=qm.spec qm-clean/qm/common.py qm-ResultSource/qm/common.py
--- qm-clean/qm/common.py	2003-05-16 00:31:18.000000000 -0700
+++ qm-ResultSource/qm/common.py	2003-07-02 13:15:14.000000000 -0700
@@ -647,6 +647,20 @@
            "%(hour)02d:%(minute)02d %(time_zone)s" % locals()
 
 
+def format_time_iso(time_secs):
+    """Generate a ISO8601-compliant formatted date and time.
+
+    The output is in the format "YYYY-MM-DDThh:mm:ss+TZ", where TZ is
+    a timezone specifier.  We always normalize to UTC (and hence
+    always use the special timezone specifier "Z"), to get proper
+    sorting behaviour.
+
+    'time_secs' -- the time to be formatted, as returned by
+                   e.g. 'time.time()'."""
+
+    return time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime(time_secs))
+
+
 def make_unique_tag():
     """Return a unique tag string."""
 
diff -urN --exclude='*~' --exclude='.*' --exclude=CVS --exclude='*.pyo' --exclude='*.pyc' --exclude=build --exclude=GNUmakefile --exclude=config.log --exclude=config.status --exclude=setup_path.py --exclude=qm.sh --exclude=qm.spec qm-clean/qm/db.py qm-ResultSource/qm/db.py
--- qm-clean/qm/db.py	1969-12-31 16:00:00.000000000 -0800
+++ qm-ResultSource/qm/db.py	2003-07-02 18:54:23.000000000 -0700
@@ -0,0 +1,96 @@
+########################################################################
+#
+# File:   db.py
+# Author: Nathaniel Smith <njs at codesourcery.com
+# Date:   2003-06-13
+#
+# Contents:
+#   A few simple functions to help with connecting to SQL databases
+#   and using the DB 2.0 API.
+#
+# Copyright (c) 2003 by CodeSourcery, LLC.  All rights reserved. 
+#
+# For license terms see the file COPYING.
+#
+########################################################################
+
+########################################################################
+# Imports
+########################################################################
+
+import os
+
+########################################################################
+# Functions
+########################################################################
+
+class Connection:
+    """A little wrapper around a DB 2.0 connection that preserves a
+    reference to the containing module, and provides a minimal
+    interface to said connection.  This is useful because it gives us
+    a hook to attach our SQL-quoting code in to."""
+
+    def __init__(self, module, connection):
+
+        self._module = module
+        self._connection = connection
+
+    def close(self):
+
+        self._connection.close()
+
+    def commit(self):
+
+        self._connection.commit()
+
+    def rollback(self):
+
+        self._connection.rollback()
+
+
+    def execute(self, sql):
+        """Creates a cursor in our database and uses it to execute
+        the given SQL; returns the cursor.
+
+        If this database requires any overall quoting of the given SQL
+        (for instance, doubling of %'s), this will be performed in this
+        method.
+        
+        """
+
+        if self._module.paramstyle in ["format", "pyformat"]:
+            sql = sql.replace("%", "%%")
+        cursor = self._connection.cursor()
+        
+        cursor.execute(sql)
+        return cursor
+        
+
+def connect(modname, *args, **moreargs):
+    """Use the given DB 2.0 module to connect to a database.  Does not
+    return a DB 2.0 database, but rather a `ModuledConnection', which
+    acts like a DB 2.0 database but supports `execute()' instead of
+    `cursor()'."""
+
+    module = __import__(modname,
+                        globals(),
+                        locals(),
+                        ["dummy element to make __import__ behave"])
+    cxn = module.connect(*args, **moreargs)
+    return Connection(module, cxn)
+    
+
+def quotestr(string):
+    """Quotes a string for SQL."""
+
+    # Replace each ' with '', then surround with more 's.  Also double
+    # backslashes.  It'd be nice to handle things like quoting non-ASCII
+    # characters (by replacing them with octal escapes), but we don't.
+    return "'" + string.replace("'", "''").replace("\\", "\\\\") + "'"
+
+########################################################################
+# Local Variables:
+# mode: python
+# indent-tabs-mode: nil
+# fill-column: 72
+# End:
diff -urN --exclude='*~' --exclude='.*' --exclude=CVS --exclude='*.pyo' --exclude='*.pyc' --exclude=build --exclude=GNUmakefile --exclude=config.log --exclude=config.status --exclude=setup_path.py --exclude=qm.sh --exclude=qm.spec qm-clean/qm/extension.py qm-ResultSource/qm/extension.py
--- qm-clean/qm/extension.py	2003-06-16 16:33:29.000000000 -0700
+++ qm-ResultSource/qm/extension.py	2003-07-02 17:45:09.000000000 -0700
@@ -133,12 +133,7 @@
         arguments = []
         dictionary = {}
         # Start with the most derived class.
-        classes = [extension_class]
-        while classes:
-            # Pull the first class off the list.
-            c = classes.pop(0)
-            # Add all of the new base classes to the end of the list.
-            classes.extend(c.__bases__)
+        for c in extension_class.__mro__:
             # Add the arguments from this class.
             new_arguments = c.__dict__.get("arguments", [])
             for a in new_arguments:
diff -urN --exclude='*~' --exclude='.*' --exclude=CVS --exclude='*.pyo' --exclude='*.pyc' --exclude=build --exclude=GNUmakefile --exclude=config.log --exclude=config.status --exclude=setup_path.py --exclude=qm.sh --exclude=qm.spec qm-clean/qm/test/base.py qm-ResultSource/qm/test/base.py
--- qm-clean/qm/test/base.py	2003-06-25 02:03:35.000000000 -0700
+++ qm-ResultSource/qm/test/base.py	2003-07-02 18:49:45.000000000 -0700
@@ -347,7 +347,7 @@
 
     'database' -- The current database.
 
-    returns -- A 'ResultSource' object."""
+    returns -- A 'ResultReader' object."""
 
     # For backwards compatibility, look at the first few bytes of the
     # file to see if it is an XML results file.
@@ -355,16 +355,16 @@
     file.seek(0)
     
     if tag == "<?xml":
-        source_cls = \
-         get_extension_class("xml_result_source.XMLResultSource",
-                             "result_source",
+        reader_cls = \
+         get_extension_class("xml_result_reader.XMLResultReader",
+                             "result_reader",
                              database)
     else:
-        source_cls = \
-         get_extension_class("pickle_result_source.PickleResultSource",
-                             "result_source",
+        reader_cls = \
+         get_extension_class("pickle_result_stream.PickleResultReader",
+                             "result_reader",
                              database)
-    return source_cls({"file": file})
+    return reader_cls({"file": file})
 
 
 def _result_from_dom(node):
@@ -425,7 +425,7 @@
 extension_kinds = [ 'database',
                     'label',
                     'resource',
-                    'result_source',
+                    'result_reader',
                     'result_stream',
                     'target',
                     'test', ]
@@ -444,7 +444,7 @@
 import qm.test.database
 import qm.label
 import qm.test.resource
-import qm.test.result_source
+import qm.test.result_reader
 import qm.test.result_stream
 import qm.test.target
 import qm.test.test
@@ -453,7 +453,7 @@
     'database' : qm.test.database.Database,
     'label' : qm.label.Label,
     'resource' : qm.test.resource.Resource,
-    'result_source' : qm.test.result_source.ResultSource,
+    'result_reader' : qm.test.result_reader.ResultReader,
     'result_stream' : qm.test.result_stream.ResultStream,
     'target' : qm.test.target.Target,
     'test' : qm.test.test.Test
diff -urN --exclude='*~' --exclude='.*' --exclude=CVS --exclude='*.pyo' --exclude='*.pyc' --exclude=build --exclude=GNUmakefile --exclude=config.log --exclude=config.status --exclude=setup_path.py --exclude=qm.sh --exclude=qm.spec qm-clean/qm/test/classes/classes.qmc qm-ResultSource/qm/test/classes/classes.qmc
--- qm-clean/qm/test/classes/classes.qmc	2003-06-27 20:15:46.000000000 -0700
+++ qm-ResultSource/qm/test/classes/classes.qmc	2003-07-02 18:48:57.000000000 -0700
@@ -18,8 +18,10 @@
 <class kind="label">python_label.PythonLabel</class>
 <class kind="result_stream">text_result_stream.TextResultStream</class>
 <class kind="result_stream">xml_result_stream.XMLResultStream</class>
-<class kind="result_source">xml_result_source.XMLResultSource</class>
+<class kind="result_reader">xml_result_reader.XMLResultReader</class>
 <class kind="result_stream">pickle_result_stream.PickleResultStream</class>
-<class kind="result_source">pickle_result_source.PickleResultSource</class>
+<class kind="result_reader">pickle_result_stream.PickleResultReader</class>
+<class kind="result_stream">sql_result_stream.SQLResultStream</class>
+<class kind="result_reader">sql_result_stream.SQLResultReader</class>
 <class kind="result_stream">dejagnu_stream.DejaGNUStream</class>
 </class-directory>
diff -urN --exclude='*~' --exclude='.*' --exclude=CVS --exclude='*.pyo' --exclude='*.pyc' --exclude=build --exclude=GNUmakefile --exclude=config.log --exclude=config.status --exclude=setup_path.py --exclude=qm.sh --exclude=qm.spec qm-clean/qm/test/classes/pickle_result_source.py qm-ResultSource/qm/test/classes/pickle_result_source.py
--- qm-clean/qm/test/classes/pickle_result_source.py	2003-06-25 02:03:35.000000000 -0700
+++ qm-ResultSource/qm/test/classes/pickle_result_source.py	1969-12-31 16:00:00.000000000 -0800
@@ -1,53 +0,0 @@
-########################################################################
-#
-# File:   pickle_result_source.py
-# Author: Nathaniel Smith
-# Date:   2003-06-23
-#
-# Contents:
-#   PickleResultSource
-#
-# Copyright (c) 2003 by CodeSourcery, LLC.  All rights reserved. 
-#
-# For license terms see the file COPYING.
-#
-########################################################################
-
-########################################################################
-# Imports
-########################################################################
-
-import cPickle
-from qm.test.file_result_source import FileResultSource
-
-########################################################################
-# Classes
-########################################################################
-
-class PickleResultSource(FileResultSource):
-    """A 'PickleResultSource' reads in results from pickle files.
-
-    See also 'PickleResultStream', which does the reverse."""
-
-    def __init__(self, arguments):
-
-        super(PickleResultSource, self).__init__(arguments)
-        self.__unpickler = cPickle.Unpickler(self.file)
-
-
-    def GetResult(self):
-
-        try:
-            return self.__unpickler.load()
-        except EOFError:
-            return None
-        except cPickle.UnpicklingError:
-            # This is raised at EOF if file is a StringIO.
-            return None
-
-########################################################################
-# Local Variables:
-# mode: python
-# indent-tabs-mode: nil
-# fill-column: 72
-# End:
diff -urN --exclude='*~' --exclude='.*' --exclude=CVS --exclude='*.pyo' --exclude='*.pyc' --exclude=build --exclude=GNUmakefile --exclude=config.log --exclude=config.status --exclude=setup_path.py --exclude=qm.sh --exclude=qm.spec qm-clean/qm/test/classes/pickle_result_stream.py qm-ResultSource/qm/test/classes/pickle_result_stream.py
--- qm-clean/qm/test/classes/pickle_result_stream.py	2003-06-21 00:58:44.000000000 -0700
+++ qm-ResultSource/qm/test/classes/pickle_result_stream.py	2003-07-02 18:48:16.000000000 -0700
@@ -5,7 +5,7 @@
 # Date:   11/25/2002
 #
 # Contents:
-#   PickleResultStream
+#   PickleResultStream, PickleResultReader
 #
 # Copyright (c) 2002, 2003 by CodeSourcery, LLC.  All rights reserved. 
 #
@@ -16,15 +16,35 @@
 ########################################################################
 
 import cPickle
+import struct
 import qm.fields
 from   qm.test.file_result_stream import FileResultStream
+from   qm.test.file_result_reader import FileResultReader
+
+########################################################################
+# Constants
+########################################################################
+
+# A nice subtlety is that because of how extension classes are loaded,
+# we can't use the standard trick of using a nonce class for our
+# sentinel, because the unpickler won't be able to find the class
+# definition.  But 'None' has no other meaning in our format, so works
+# fine.
+_annotation_sentinel = None
+"""The sentinel value that marks the beginning of an annotation."""
+
+# Network byte order, 4 byte unsigned int
+_int_format = "!I"
+_int_size = struct.calcsize(_int_format)
 
 ########################################################################
 # Classes
 ########################################################################
 
 class PickleResultStream(FileResultStream):
-    """A 'PickleResultStream' writes out results as Python pickles."""
+    """A 'PickleResultStream' writes out results as Python pickles.
+
+    See also 'PickleResultReader', which does the reverse."""
 
     _max_pinned_results = 1000
     """A limit on how many `Result's to pin in memory at once.
@@ -40,6 +60,31 @@
     technique causes a very minor slowdown on small result streams,
     and a substantial speedup on large result streams."""
 
+    _format_version = 1
+    """The version number of the format we write.
+
+    This is bumped every time the format is changed, to make sure that
+    we can retain backwards compatibility.
+
+    "Version 0" contains no version number, and is simply a bunch
+    of 'Result's pickled one after another.
+
+    "Version 1", and all later versions, contain a pickled version
+    number as the first thing in the file.  In version 1, this is
+    followed by a 4-byte unsigned integer in network byte order giving
+    the address of the first annotation, followed by the file proper.
+    The file proper is composed of a bunch of pickled 'Result's,
+    followed by a pickled sentinel value (None), followed by a 4-byte
+    unsigned integer in network-byte order, followed by the beginning
+    of a new pickle whose first item is a annotation tuple.  An
+    annotation tuple is a tuple of n items, the first of which is a
+    string describing tagging the type of annotation, and the rest of
+    which have an interpretation that depends on the tag found.  The
+    only tag currently defined is "annotation", which is followed by
+    two string elements giving respectively the key and the value.
+    The 4-byte integers always point to the file address of the next
+    such integer, except for the last, which has a value of 0; they
+    are used to quickly find all annotations."""
 
     arguments = [
         qm.fields.IntegerField(
@@ -73,11 +118,42 @@
 
         # Initialize the base class.
         super(PickleResultStream, self).__init__(arguments)
-        # Create a pickler.
-        self.__pickler = cPickle.Pickler(self.file, self.protocol_version)
+        # Create initial pickler.
+        self._ResetPickler()
         # We haven't processed any `Result's yet.
         self.__processed = 0
 
+        # Write out version number.
+        self.__pickler.dump(self._format_version)
+        # We have no previous annotations.
+        self.__last_annotation = None
+        # Write out annotation header.
+        self._WriteAnnotationPtr()
+
+
+    def _ResetPickler(self):
+
+        self.__pickler = cPickle.Pickler(self.file, self.protocol_version)
+
+
+    def _WriteAnnotationPtr(self):
+
+        new_annotation = self.file.tell()
+        if self.__last_annotation is not None:
+            self.file.seek(self.__last_annotation)
+            self.file.write(struct.pack(_int_format, new_annotation))
+            self.file.seek(new_annotation)
+        self.file.write(struct.pack(_int_format, 0))
+        self.__last_annotation = new_annotation
+        self._ResetPickler()
+
+
+    def WriteAnnotation(self, key, value):
+
+        self.__pickler.dump(_annotation_sentinel)
+        self._WriteAnnotationPtr()
+        self.__pickler.dump(("annotation", key, value))
+
 
     def WriteResult(self, result):
 
@@ -87,3 +163,114 @@
         # cache.
         if not self.__processed % self._max_pinned_results:
             self.__pickler.clear_memo()
+
+
+            
+class PickleResultReader(FileResultReader):
+    """A 'PickleResultReader' reads in results from pickle files.
+
+    See also 'PickleResultStream', which does the reverse."""
+
+    def __init__(self, arguments):
+
+        super(PickleResultReader, self).__init__(arguments)
+        self._ResetUnpickler()
+
+        self._annotations = {}
+
+        # Check for a version number
+        try:
+            version = self.__unpickler.load()
+        except (EOFError, cPickle.UnpicklingError):
+            # This file is empty, no more handling needed.
+            return
+        
+        if not isinstance(version, int):
+            # Version 0 file, no version number; in fact, we're
+            # holding a 'Result'.  So we have no metadata to load and
+            # should just rewind.
+            self.file.seek(0)
+            self._ResetPickler()
+        elif version == 1:
+            self._ReadMetadata()
+        else:
+            raise QMException, "Unknown format version %i" % (version,)
+
+
+    def _ResetUnpickler(self):
+
+        self.__unpickler = cPickle.Unpickler(self.file)
+
+
+    def _ReadAddress(self):
+
+        raw = self.file.read(_int_size)
+        return struct.unpack(_int_format, raw)[0]
+        
+
+    def _ReadMetadata(self):
+
+        # We've read in the version number; next few bytes are the
+        # address of the first annotation.
+        addr = self._ReadAddress()
+        # That advanced the read head to the first 'Result'; save this
+        # spot to return to later.
+        first_result_addr = self.file.tell()
+        while addr:
+            # Go the the address.
+            self.file.seek(addr)
+            # First four bytes are the next address.
+            addr = self._ReadAddress()
+            # Then we restart the pickle stream...
+            self._ResetUnpickler()
+            # ...and read in the annotation here.
+            annotation_tuple = self.__unpickler.load()
+            kind = annotation_tuple[0]
+            if kind == "annotation":
+                (key, value) = annotation_tuple[1:]
+                self._annotations[key] = value
+            else:
+                print "Unknown annotation type '%s'; ignoring" % (kind,)
+            # Now loop back and jump to the next address.
+
+        # Finally, rewind back to the beginning for the reading of
+        # 'Result's.
+        self.file.seek(first_result_addr)
+        self._ResetUnpickler()
+
+
+    def GetAnnotations(self):
+
+        return self._annotations
+
+
+    def GetResult(self):
+
+        while 1:
+            try:
+                thing = self.__unpickler.load()
+            except EOFError:
+                return None
+            except cPickle.UnpicklingError:
+                # This is raised at EOF if file is a StringIO.
+                return None
+            else:
+                if thing is _annotation_sentinel:
+                    # We're looking for results, but this is an annotation,
+                    # so skip over it.
+                    # By skipping past the address...
+                    self.file.seek(_int_size, 1)
+                    self._ResetUnpickler()
+                    # ...and the annotation itself.
+                    self.__unpickler.noload()
+                    # Now loop.
+                else:
+                    # We actually got a 'Result'.
+                    return thing
+
+########################################################################
+# Local Variables:
+# mode: python
+# indent-tabs-mode: nil
+# fill-column: 72
+# End:
diff -urN --exclude='*~' --exclude='.*' --exclude=CVS --exclude='*.pyo' --exclude='*.pyc' --exclude=build --exclude=GNUmakefile --exclude=config.log --exclude=config.status --exclude=setup_path.py --exclude=qm.sh --exclude=qm.spec qm-clean/qm/test/classes/sql_result_stream.py qm-ResultSource/qm/test/classes/sql_result_stream.py
--- qm-clean/qm/test/classes/sql_result_stream.py	1969-12-31 16:00:00.000000000 -0800
+++ qm-ResultSource/qm/test/classes/sql_result_stream.py	2003-07-02 19:15:19.000000000 -0700
@@ -0,0 +1,252 @@
+########################################################################
+#
+# File:   sql_result_stream.py
+# Author: Nathaniel Smith <njs at codesourcery.com>
+# Date:   2003-06-13
+#
+# Contents:
+#   SQLResultStream, SQLResultSource
+#
+# Copyright (c) 2003 by CodeSourcery, LLC.  All rights reserved. 
+#
+# For license terms see the file COPYING.
+#
+########################################################################
+
+########################################################################
+# Imports
+########################################################################
+
+import qm.fields
+from qm.extension          import Extension
+from qm.test.result_stream import ResultStream
+from qm.test.result_reader import ResultReader
+from qm.db                 import quotestr, connect
+from qm.test.result        import Result
+
+########################################################################
+# Classes
+########################################################################
+
+class SQLConnected(Extension):
+    """Mixin class for classes that need a database connection."""
+
+    arguments = [
+        qm.fields.TextField(
+            name = "db_name",
+            title = "Database name",
+            description = "The PostgreSQL database to connect to.",
+            verbatim = "true",
+            default_value = ""),
+        qm.fields.TextField(
+            name = "db_module",
+            title = "Database module",
+            description = "The DB 2.0 module to use.",
+            verbatim = "true",
+            default_value = "pgdb"),
+        qm.fields.PythonField(
+            name = "connection"),
+    ]
+
+    def __init__(self, arguments):
+        super(SQLConnected, self).__init__(arguments)
+
+        if not self.connection:
+            self.connection = connect(self.db_module,
+                                      database=self.db_name)
+
+
+
+class SQLResultStream(ResultStream, SQLConnected):
+    """A `SQLResultStream' writes results out to an SQL database.
+    
+    This class currently supports PostgreSQL only."""
+
+
+    def __init__(self, arguments):
+        super(SQLResultStream, self).__init__(arguments)
+
+        run_id_cursor = self.connection.execute("""
+            SELECT nextval('run_id_seq');
+            """)
+        (self._run_id,) = run_id_cursor.fetchone()
+
+        self.connection.execute("""
+            INSERT INTO runs (run_id) VALUES (%i)
+            """ % (self._run_id,))
+
+
+    def WriteAnnotation(self, key, value):
+
+        self.connection.execute("""
+            INSERT INTO run_annotations (run_id, key, value)
+            VALUES (%i, %s, %s)
+            """ % (self._run_id, quotestr(key), quotestr(value)))
+        
+
+    def WriteResult(self, result):
+
+        self.connection.execute("""
+            INSERT INTO results (run_id, result_id, kind, outcome)
+            VALUES (%i, %s, %s, %s)
+            """ % (self._run_id,
+                   quotestr(result.GetId()),
+                   quotestr(result.GetKind()),
+                   quotestr(result.GetOutcome())))
+
+        for key, value in result.items():
+            self.connection.execute("""
+                INSERT INTO result_annotations (run_id,
+                                                result_id,
+                                                result_kind,
+                                                key,
+                                                value)
+                VALUES (%i, %s, %s, %s, %s)
+                """ % (self._run_id,
+                       quotestr(result.GetId()),
+                       quotestr(result.GetKind()),
+                       quotestr(key),
+                       quotestr(value)))
+
+
+    def Summarize(self):
+
+        self.connection.commit()
+
+
+
+class _Buffer:
+    """A little buffering iterator with one-element rewind."""
+
+    def __init__(self, size, get_more):
+        """Create a '_Buffer'.
+
+        'size' -- the number of items to hold in the buffer at a time.
+
+        'get_more' -- a function taking a number as its sole argument;
+                      should return a list of that many new items (or as
+                      many items are left, whichever is less).
+        """
+
+        self.size = size
+        self.get_more = get_more
+        self.buffer = get_more(size)
+        self.idx = 0
+        # Needed for rewinding over buffer refills:
+        self.last = None
+
+
+    def next(self):
+        """Returns the next item, refilling the buffer if necessary."""
+
+        idx = self.idx
+        if idx == len(self.buffer):
+            self.buffer = self.get_more(self.size)
+            self.idx = 0
+            idx = 0
+        if not self.buffer:
+            raise StopIteration
+        self.idx += 1
+        self.last = self.buffer[idx]
+        return self.buffer[idx]
+
+
+    def rewind(self):
+
+        if self.idx == 0:
+            self.buffer.insert(0, self.last)
+        else:
+            self.idx -= 1
+
+
+    def __iter__(self):
+
+        return self
+
+
+
+class SQLResultReader(ResultReader, SQLConnected):
+    """A `SQLResultReader' reads result in from an SQL database.
+
+    This class currently supports PostgreSQL only."""
+
+    arguments = [
+        qm.fields.IntegerField(
+            name = "run_id",
+            title = "Run ID",
+        ),
+    ]
+
+    def __init__(self, arguments):
+        super(SQLResultReader, self).__init__(arguments)
+
+        self._batch_size = 1000
+
+        self._LoadAnnotations()
+        self._SetupResultCursors()
+
+
+    def _LoadAnnotations(self):
+
+        cursor = self.connection.execute("""
+            SELECT key, value FROM run_annotations
+                              WHERE run_id = %i
+            """ % (self.run_id))
+
+        self._annotations = dict(iter(cursor.fetchone, None))
+
+
+    def GetAnnotations(self):
+
+        return self._annotations
+
+
+    def _SetupResultCursors(self):
+    
+        # Set up our two result cursors.
+        self.connection.execute("""
+            DECLARE results_c CURSOR FOR
+                SELECT result_id, kind, outcome FROM results
+                                                WHERE run_id = %i
+                ORDER BY result_id, kind
+            """ % (self.run_id,))
+        self.connection.execute("""
+            DECLARE annote_c CURSOR FOR
+                SELECT result_id, result_kind, key, value
+                FROM result_annotations WHERE run_id = %i
+                ORDER BY result_id, result_kind
+            """ % (self.run_id,))
+
+        def get_more_results(num):
+            return self.connection.execute("""
+                       FETCH FORWARD %i FROM results_c
+                   """ % (num,)).fetchall()
+        def get_more_annotations(num):
+            return self.connection.execute("""
+                       FETCH FORWARD %i FROM annote_c
+                   """ % (num,)).fetchall()
+
+        self._r_buffer = _Buffer(self._batch_size, get_more_results)
+        self._a_buffer = _Buffer(self._batch_size, get_more_annotations)
+        
+
+    def GetResult(self):
+
+        try:
+            id, kind, outcome = self._r_buffer.next()
+        except StopIteration:
+            return None
+        annotations = {}
+        for result_id, result_kind, key, value in self._a_buffer:
+            if (result_id, result_kind) != (id, kind):
+                self._a_buffer.rewind()
+                break
+            annotations[key] = value
+        return Result(kind, id, outcome, annotations)
+
+########################################################################
+# Local Variables:
+# mode: python
+# indent-tabs-mode: nil
+# fill-column: 72
+# End:
diff -urN --exclude='*~' --exclude='.*' --exclude=CVS --exclude='*.pyo' --exclude='*.pyc' --exclude=build --exclude=GNUmakefile --exclude=config.log --exclude=config.status --exclude=setup_path.py --exclude=qm.sh --exclude=qm.spec qm-clean/qm/test/classes/xml_result_reader.py qm-ResultSource/qm/test/classes/xml_result_reader.py
--- qm-clean/qm/test/classes/xml_result_reader.py	1969-12-31 16:00:00.000000000 -0800
+++ qm-ResultSource/qm/test/classes/xml_result_reader.py	2003-07-02 18:52:31.000000000 -0700
@@ -0,0 +1,95 @@
+########################################################################
+#
+# File:   xml_result_reader.py
+# Author: Nathaniel Smith
+# Date:   2003-06-23
+#
+# Contents:
+#   XMLResultReader
+#
+# Copyright (c) 2003 by CodeSourcery, LLC.  All rights reserved. 
+#
+# For license terms see the file COPYING.
+#
+########################################################################
+
+########################################################################
+# Imports
+########################################################################
+
+import qm.xmlutil
+from   qm.test.file_result_reader import FileResultReader
+from   qm.test.result import Result
+
+########################################################################
+# Classes
+########################################################################
+
+class XMLResultReader(FileResultReader):
+    """Reads in 'Result's from an XML-formatted results file.
+
+    To write such a file, see 'XMLResultStream'."""
+
+    def __init__(self, arguments):
+
+        super(XMLResultReader, self).__init__(arguments)
+
+        document = qm.xmlutil.load_xml(self.file)
+        node = document.documentElement
+        results = qm.xmlutil.get_children(node, "result")
+        self.__node_iterator = iter(results)
+
+        # Read out annotations
+        self._annotations = {}
+        annotation_nodes = qm.xmlutil.get_children(node, "annotation")
+        for node in annotation_nodes:
+            key = node.getAttribute("key")
+            value = qm.xmlutil.get_dom_text(node)
+            self._annotations[key] = value
+
+
+    def GetAnnotations(self):
+
+        return self._annotations
+
+
+    def _result_from_dom(self, node):
+        """Extract a result from a DOM node.
+
+        'node' -- A DOM node corresponding to a "result" element.
+
+        returns -- A 'Result' object."""
+
+        assert node.tagName == "result"
+        # Extract the outcome.
+        outcome = qm.xmlutil.get_child_text(node, "outcome")
+        # Extract the test ID.
+        test_id = node.getAttribute("id")
+        kind = node.getAttribute("kind")
+        # Build a Result.
+        result = Result(kind, test_id, outcome)
+        # Extract properties, one for each property element.
+        for property_node in node.getElementsByTagName("property"):
+            # The name is stored in an attribute.
+            name = property_node.getAttribute("name")
+            # The value is stored in the child text node.
+            value = qm.xmlutil.get_dom_text(property_node)
+            # Store it.
+            result[name] = value
+
+        return result
+
+
+    def GetResult(self):
+
+        try:
+            return self._result_from_dom(self.__node_iterator.next())
+        except StopIteration:
+            return None
+
+########################################################################
+# Local Variables:
+# mode: python
+# indent-tabs-mode: nil
+# fill-column: 72
+# End:
diff -urN --exclude='*~' --exclude='.*' --exclude=CVS --exclude='*.pyo' --exclude='*.pyc' --exclude=build --exclude=GNUmakefile --exclude=config.log --exclude=config.status --exclude=setup_path.py --exclude=qm.sh --exclude=qm.spec qm-clean/qm/test/classes/xml_result_source.py qm-ResultSource/qm/test/classes/xml_result_source.py
--- qm-clean/qm/test/classes/xml_result_source.py	2003-06-25 02:03:35.000000000 -0700
+++ qm-ResultSource/qm/test/classes/xml_result_source.py	1969-12-31 16:00:00.000000000 -0800
@@ -1,82 +0,0 @@
-########################################################################
-#
-# File:   xml_result_source.py
-# Author: Nathaniel Smith
-# Date:   2003-06-23
-#
-# Contents:
-#   XMLResultSource
-#
-# Copyright (c) 2003 by CodeSourcery, LLC.  All rights reserved. 
-#
-# For license terms see the file COPYING.
-#
-########################################################################
-
-########################################################################
-# Imports
-########################################################################
-
-import qm.xmlutil
-from   qm.test.file_result_source import FileResultSource
-from   qm.test.result import Result
-
-########################################################################
-# Classes
-########################################################################
-
-class XMLResultSource(FileResultSource):
-    """Reads in 'Result's from an XML-formatted results file.
-
-    To write such a file, see 'XMLResultStream'."""
-
-    def __init__(self, arguments):
-
-        super(XMLResultSource, self).__init__(arguments)
-
-        document = qm.xmlutil.load_xml(self.file)
-        node = document.documentElement
-        results = qm.xmlutil.get_children(node, "result")
-        self.__node_iterator = iter(results)
-
-
-    def _result_from_dom(self, node):
-        """Extract a result from a DOM node.
-
-        'node' -- A DOM node corresponding to a "result" element.
-
-        returns -- A 'Result' object."""
-
-        assert node.tagName == "result"
-        # Extract the outcome.
-        outcome = qm.xmlutil.get_child_text(node, "outcome")
-        # Extract the test ID.
-        test_id = node.getAttribute("id")
-        kind = node.getAttribute("kind")
-        # Build a Result.
-        result = Result(kind, test_id, outcome)
-        # Extract properties, one for each property element.
-        for property_node in node.getElementsByTagName("property"):
-            # The name is stored in an attribute.
-            name = property_node.getAttribute("name")
-            # The value is stored in the child text node.
-            value = qm.xmlutil.get_dom_text(property_node)
-            # Store it.
-            result[name] = value
-
-        return result
-
-
-    def GetResult(self):
-
-        try:
-            return self._result_from_dom(self.__node_iterator.next())
-        except StopIteration:
-            return None
-
-########################################################################
-# Local Variables:
-# mode: python
-# indent-tabs-mode: nil
-# fill-column: 72
-# End:
diff -urN --exclude='*~' --exclude='.*' --exclude=CVS --exclude='*.pyo' --exclude='*.pyc' --exclude=build --exclude=GNUmakefile --exclude=config.log --exclude=config.status --exclude=setup_path.py --exclude=qm.sh --exclude=qm.spec qm-clean/qm/test/classes/xml_result_stream.py qm-ResultSource/qm/test/classes/xml_result_stream.py
--- qm-clean/qm/test/classes/xml_result_stream.py	2003-04-13 23:06:40.000000000 -0700
+++ qm-ResultSource/qm/test/classes/xml_result_stream.py	2003-07-02 13:18:34.000000000 -0700
@@ -36,7 +36,7 @@
     def __init__(self, arguments):
 
         # Initialize the base class.
-        FileResultStream.__init__(self, arguments)
+        super(XMLResultStream, self).__init__(arguments)
         
         # Create an XML document, since the DOM API requires you
         # to have a document when you create a node.
@@ -53,6 +53,17 @@
         self.file.write("<results>\n")
 
 
+    def WriteAnnotation(self, key, value):
+
+            element = self.__document.createElement("annotation")
+            element.setAttribute("key", key)
+            text = self.__document.createTextNode(value)
+            element.appendChild(text)
+            element.writexml(self.file)
+            # Following increases readability of output:
+            self.file.write("\n")
+
+
     def WriteResult(self, result):
         """Output a test or resource result.
 
diff -urN --exclude='*~' --exclude='.*' --exclude=CVS --exclude='*.pyo' --exclude='*.pyc' --exclude=build --exclude=GNUmakefile --exclude=config.log --exclude=config.status --exclude=setup_path.py --exclude=qm.sh --exclude=qm.spec qm-clean/qm/test/cmdline.py qm-ResultSource/qm/test/cmdline.py
--- qm-clean/qm/test/cmdline.py	2003-06-25 02:06:06.000000000 -0700
+++ qm-ResultSource/qm/test/cmdline.py	2003-07-02 00:42:20.000000000 -0700
@@ -1227,6 +1227,10 @@
         # written.
         streams = self.__GetResultStreams(suite_ids)
         
+        # Send the annotations through.
+        for s in streams:
+            s.WriteAllAnnotations(results.GetAnnotations())
+
         # Get the expected outcomes.
         outcomes = self.__GetExpectedOutcomes()
 
diff -urN --exclude='*~' --exclude='.*' --exclude=CVS --exclude='*.pyo' --exclude='*.pyc' --exclude=build --exclude=GNUmakefile --exclude=config.log --exclude=config.status --exclude=setup_path.py --exclude=qm.sh --exclude=qm.spec qm-clean/qm/test/execution_engine.py qm-ResultSource/qm/test/execution_engine.py
--- qm-clean/qm/test/execution_engine.py	2003-06-16 16:33:29.000000000 -0700
+++ qm-ResultSource/qm/test/execution_engine.py	2003-07-02 02:03:25.000000000 -0700
@@ -27,6 +27,7 @@
 from   result import *
 import select
 import sys
+import time
 
 ########################################################################
 # Classes
@@ -133,6 +134,12 @@
 
         returns -- True if any tests had unexpected outcomes."""
 
+        # Write out all the currently known annotations.
+        start_time_str = qm.common.format_time_iso(time.time())
+        for rs in self.__result_streams:
+            rs.WriteAllAnnotations(self.__context)
+            rs.WriteAnnotation("qmtest.run.start_time", start_time_str)
+
         # Start all of the targets.
         for target in self.__targets:
             target.Start(self.__response_queue, self)
@@ -153,7 +160,9 @@
             
             # Let all of the result streams know that the test run is
             # complete.
+            end_time_str = qm.common.format_time_iso(time.time())
             for rs in self.__result_streams:
+                rs.WriteAnnotation("qmtest.run.end_time", end_time_str)
                 rs.Summarize()
 
         return self.__any_unexpected_outcomes
diff -urN --exclude='*~' --exclude='.*' --exclude=CVS --exclude='*.pyo' --exclude='*.pyc' --exclude=build --exclude=GNUmakefile --exclude=config.log --exclude=config.status --exclude=setup_path.py --exclude=qm.sh --exclude=qm.spec qm-clean/qm/test/file_result_reader.py qm-ResultSource/qm/test/file_result_reader.py
--- qm-clean/qm/test/file_result_reader.py	1969-12-31 16:00:00.000000000 -0800
+++ qm-ResultSource/qm/test/file_result_reader.py	2003-07-02 18:50:34.000000000 -0700
@@ -0,0 +1,78 @@
+########################################################################
+#
+# File:   file_result_reader.py
+# Author: Nathaniel Smith
+# Date:   2003-06-23
+#
+# Contents:
+#   FileResultReader
+#
+# Copyright (c) 2003 by CodeSourcery, LLC.  All rights reserved. 
+#
+# For license terms see the file COPYING.
+#
+########################################################################
+
+########################################################################
+# Imports
+########################################################################
+
+import qm.fields
+from qm.test.result_reader import ResultReader
+import sys
+
+########################################################################
+# Classes
+########################################################################
+
+class FileResultReader(ResultReader):
+    """A 'FileResultReader' gets its input from a file.
+
+    A 'FileResultReader' is an abstract base class for other result
+    reader classes that read results from a single file.  The file
+    from which results should be read can be specified using either
+    the 'filename' argument or the 'file' argument.  The latter is for
+    use by QMTest internally."""
+
+
+    arguments = [
+        qm.fields.TextField(
+            name = "filename",
+            title = "File Name",
+            description = """The name of the file.
+
+            All results will be read from the file indicated.  If no
+            filename is specified, or the filename specified is "-",
+            the standard input will be used.""",
+            verbatim = "true",
+            default_value = ""),
+        qm.fields.PythonField(
+            name = "file"),
+    ]
+
+    _is_binary_file = 0
+    """If true, the file written is a binary file.
+
+    This flag can be overridden by derived classes."""
+    
+    def __init__(self, arguments):
+
+        super(FileResultReader, self).__init__(arguments)
+
+        if not self.file:
+            if self.filename and self.filename != "-":
+                if self._is_binary_file:
+                    mode = "rb"
+                else:
+                    mode = "r"
+                self.file = open(self.filename, mode, 0)
+            else:
+                self.file = sys.stdin
+
+
+########################################################################
+# Local Variables:
+# mode: python
+# indent-tabs-mode: nil
+# fill-column: 72
+# End:
diff -urN --exclude='*~' --exclude='.*' --exclude=CVS --exclude='*.pyo' --exclude='*.pyc' --exclude=build --exclude=GNUmakefile --exclude=config.log --exclude=config.status --exclude=setup_path.py --exclude=qm.sh --exclude=qm.spec qm-clean/qm/test/file_result_source.py qm-ResultSource/qm/test/file_result_source.py
--- qm-clean/qm/test/file_result_source.py	2003-06-25 02:03:35.000000000 -0700
+++ qm-ResultSource/qm/test/file_result_source.py	1969-12-31 16:00:00.000000000 -0800
@@ -1,78 +0,0 @@
-########################################################################
-#
-# File:   file_result_source.py
-# Author: Nathaniel Smith
-# Date:   2003-06-23
-#
-# Contents:
-#   FileResultSource
-#
-# Copyright (c) 2003 by CodeSourcery, LLC.  All rights reserved. 
-#
-# For license terms see the file COPYING.
-#
-########################################################################
-
-########################################################################
-# Imports
-########################################################################
-
-import qm.fields
-from qm.test.result_source import ResultSource
-import sys
-
-########################################################################
-# Classes
-########################################################################
-
-class FileResultSource(ResultSource):
-    """A 'FileResultSource' gets its input from a file.
-
-    A 'FileResultSource' is an abstract base class for other result
-    source classes that read results from a single file.  The file
-    from which results should be read can be specified using either
-    the 'filename' argument or the 'file' argument.  The latter is for
-    use by QMTest internally."""
-
-
-    arguments = [
-        qm.fields.TextField(
-            name = "filename",
-            title = "File Name",
-            description = """The name of the file.
-
-            All results will be read from the file indicated.  If no
-            filename is specified, or the filename specified is "-",
-            the standard input will be used.""",
-            verbatim = "true",
-            default_value = ""),
-        qm.fields.PythonField(
-            name = "file"),
-    ]
-
-    _is_binary_file = 0
-    """If true, the file written is a binary file.
-
-    This flag can be overridden by derived classes."""
-    
-    def __init__(self, arguments):
-
-        super(FileResultSource, self).__init__(arguments)
-
-        if not self.file:
-            if self.filename and self.filename != "-":
-                if self._is_binary_file:
-                    mode = "rb"
-                else:
-                    mode = "r"
-                self.file = open(self.filename, mode, 0)
-            else:
-                self.file = sys.stdin
-
-
-########################################################################
-# Local Variables:
-# mode: python
-# indent-tabs-mode: nil
-# fill-column: 72
-# End:
diff -urN --exclude='*~' --exclude='.*' --exclude=CVS --exclude='*.pyo' --exclude='*.pyc' --exclude=build --exclude=GNUmakefile --exclude=config.log --exclude=config.status --exclude=setup_path.py --exclude=qm.sh --exclude=qm.spec qm-clean/qm/test/result_reader.py qm-ResultSource/qm/test/result_reader.py
--- qm-clean/qm/test/result_reader.py	1969-12-31 16:00:00.000000000 -0800
+++ qm-ResultSource/qm/test/result_reader.py	2003-07-02 18:51:00.000000000 -0700
@@ -0,0 +1,64 @@
+########################################################################
+#
+# File:   result_reader.py
+# Author: Nathaniel Smith
+# Date:   2003-06-23
+#
+# Contents:
+#   QMTest ResultReader class.
+#
+# Copyright (c) 2003 by CodeSourcery, LLC.  All rights reserved. 
+#
+# For license terms see the file COPYING.
+#
+########################################################################
+
+########################################################################
+# Imports
+########################################################################
+
+import qm.extension
+
+########################################################################
+# Classes
+########################################################################
+
+class ResultReader(qm.extension.Extension):
+    """A 'ResultReader' provides access to stored test results.
+
+    For instance, a 'ResultReader' may load 'Result's from a pickle
+    file or an XML file.
+
+    This is an abstract class.
+
+    See also 'ResultStream'."""
+
+    kind = "result_reader"
+
+    def GetAnnotations(self):
+        """Return this run's dictionary of annotations."""
+
+        # For backwards compatibility, don't raise an exception.
+        return {}
+
+
+    def GetResult(self):
+        """Return the next 'Result' from this reader.
+
+        returns -- A 'Result', or 'None' if there are no more results.
+        """
+
+        raise NotImplementedError
+
+
+    def __iter__(self):
+        """A 'ResultReader' can be iterated over."""
+
+        return iter(self.GetResult, None)
+
+########################################################################
+# Local Variables:
+# mode: python
+# indent-tabs-mode: nil
+# fill-column: 72
+# End:
diff -urN --exclude='*~' --exclude='.*' --exclude=CVS --exclude='*.pyo' --exclude='*.pyc' --exclude=build --exclude=GNUmakefile --exclude=config.log --exclude=config.status --exclude=setup_path.py --exclude=qm.sh --exclude=qm.spec qm-clean/qm/test/result_source.py qm-ResultSource/qm/test/result_source.py
--- qm-clean/qm/test/result_source.py	2003-06-25 02:03:35.000000000 -0700
+++ qm-ResultSource/qm/test/result_source.py	1969-12-31 16:00:00.000000000 -0800
@@ -1,57 +0,0 @@
-########################################################################
-#
-# File:   result_source.py
-# Author: Nathaniel Smith
-# Date:   2003-06-23
-#
-# Contents:
-#   QMTest ResultSource class.
-#
-# Copyright (c) 2003 by CodeSourcery, LLC.  All rights reserved. 
-#
-# For license terms see the file COPYING.
-#
-########################################################################
-
-########################################################################
-# Imports
-########################################################################
-
-import qm.extension
-
-########################################################################
-# Classes
-########################################################################
-
-class ResultSource(qm.extension.Extension):
-    """A 'ResultSource' provides access to stored test results.
-
-    For instance, a 'ResultSource' may load 'Result's from a pickle
-    file or an XML file.
-
-    This is an abstract class.
-
-    See also 'ResultStream'."""
-
-    kind = "result_source"
-
-    def GetResult(self):
-        """Return the next 'Result' from this source.
-
-        returns -- A 'Result', or 'None' if there are no more results.
-        """
-
-        raise NotImplementedError
-
-
-    def __iter__(self):
-        """A 'ResultSource' can be iterated over."""
-
-        return iter(self.GetResult, None)
-
-########################################################################
-# Local Variables:
-# mode: python
-# indent-tabs-mode: nil
-# fill-column: 72
-# End:
diff -urN --exclude='*~' --exclude='.*' --exclude=CVS --exclude='*.pyo' --exclude='*.pyc' --exclude=build --exclude=GNUmakefile --exclude=config.log --exclude=config.status --exclude=setup_path.py --exclude=qm.sh --exclude=qm.spec qm-clean/qm/test/result_stream.py qm-ResultSource/qm/test/result_stream.py
--- qm-clean/qm/test/result_stream.py	2003-05-16 00:31:18.000000000 -0700
+++ qm-ResultSource/qm/test/result_stream.py	2003-07-02 00:40:38.000000000 -0700
@@ -52,9 +52,39 @@
            name = "suite_ids"),
         ]
     
+    def WriteAnnotation(self, key, value):
+        """Output an annotation for this run.
+
+        Subclasses should override this if they want to store/display
+        annotations; the default implementation simply discards them.
+
+        'key' -- the key value as a string.
+
+        'value' -- the value of this annotation as a string."""
+
+        pass
+    
+
+    def WriteAllAnnotations(self, annotations):
+        """Output all annotations in 'annotations' to this stream.
+
+        Currently this is the same as making repeated calls to
+        'WriteAnnotation', but in the future, as special annotation
+        types like timestamps are added, this will do the work of
+        dispatching to functions like 'WriteTimestamp'.
+
+        Should not be overridden by subclasses."""
+
+        for key, value in annotations.iteritems():
+            self.WriteAnnotation(key, value)
+
+
     def WriteResult(self, result):
         """Output a test result.
 
+        Subclasses must override this method; the default
+        implementation raises a 'NotImplementedError'.
+
         'result' -- A 'Result'."""
 
         raise NotImplementedError
diff -urN --exclude='*~' --exclude='.*' --exclude=CVS --exclude='*.pyo' --exclude='*.pyc' --exclude=build --exclude=GNUmakefile --exclude=config.log --exclude=config.status --exclude=setup_path.py --exclude=qm.sh --exclude=qm.spec qm-clean/qm/test/web/web.py qm-ResultSource/qm/test/web/web.py
--- qm-clean/qm/test/web/web.py	2003-06-25 02:03:35.000000000 -0700
+++ qm-ResultSource/qm/test/web/web.py	2003-07-02 19:50:05.000000000 -0700
@@ -1203,13 +1203,19 @@
     A 'StorageResultsStream' does not write any output.  It simply
     stores the results for future display."""
 
-    def __init__(self):
+    def __init__(self, arguments):
         """Construct a 'StorageResultsStream'."""
 
-        ResultStream.__init__(self, {})
+        super(StorageResultsStream, self).__init__(arguments)
         self.__test_results = {}
         self.__test_results_in_order = []
         self.__resource_results = {}
+        # Put in a note.  No software currently pays any attention to
+        # this key, but it's useful to mark runs that were done from
+        # the GUI, because they may be an amalgamation of multiple
+        # runs, and therefore cannot be trusted to describe a single
+        # version of the software under test.
+        self.__annotations = { "qmtest.created_from_gui": "true" }
         # The stream is not finished yet.
         self.__is_finished = 0
         
@@ -1220,6 +1226,17 @@
         self.__lock = Lock()
 
 
+    def GetAnnotations(self):
+        """Return the annotations for this run."""
+
+        return self.__annotations
+
+
+    def WriteAnnotation(self, key, value):
+
+        self.__annotations[key] = value
+
+
     def WriteResult(self, result):
         """Output a test result.
 
@@ -1573,7 +1590,7 @@
 
         self.__expected_outcomes = expectations
         # There are no results yet.        
-        self.__results_stream = StorageResultsStream()
+        self.__results_stream = StorageResultsStream({})
         self.__results_stream.Summarize()
         # There is no execution thread.
         self.__execution_thread = None
@@ -1646,7 +1663,7 @@
         # Eliminate the old results stream.
         del self.__results_stream
         # And create a new one.
-        self.__results_stream = StorageResultsStream()
+        self.__results_stream = StorageResultsStream({})
         self.__results_stream.Summarize()
 
         # Redirect to the main page.
@@ -1898,6 +1915,8 @@
         # Create a results stream for storing the results.
         rsc = qm.test.cmdline.get_qmtest().GetFileResultStreamClass()
         rs = rsc({ "file" : s })
+        # Write all the annotations.
+        rs.WriteAllAnnotations(self.__results_stream.GetAnnotations())
         # Write all the results.
         for r in self.__results_stream.GetTestResults().values():
             rs.WriteResult(r)
@@ -2331,7 +2350,9 @@
         # Read the results.
         results = qm.test.base.load_results(f, self.GetDatabase())
         # Enter them into a new results stream.
-        self.__results_stream = StorageResultsStream()
+        self.__results_stream = StorageResultsStream({})
+        annotations = results.GetAnnotations()
+        self.__results_stream.WriteAllAnnotations(annotations)
         for r in results:
             self.__results_stream.WriteResult(r)
         self.__results_stream.Summarize()
Binary files qm-clean/results.qmr and qm-ResultSource/results.qmr differ
diff -urN --exclude='*~' --exclude='.*' --exclude=CVS --exclude='*.pyo' --exclude='*.pyc' --exclude=build --exclude=GNUmakefile --exclude=config.log --exclude=config.status --exclude=setup_path.py --exclude=qm.sh --exclude=qm.spec qm-clean/scripts/create-results-database.py qm-ResultSource/scripts/create-results-database.py
--- qm-clean/scripts/create-results-database.py	1969-12-31 16:00:00.000000000 -0800
+++ qm-ResultSource/scripts/create-results-database.py	2003-07-02 18:29:53.000000000 -0700
@@ -0,0 +1,103 @@
+#!/usr/bin/env python
+
+########################################################################
+#
+# File:   create-results-database.py
+# Author: Nathaniel Smith
+# Date:   2003-07-02
+#
+# Contents:
+#   Script to set up a PostgreSQL results database.
+#
+# Copyright (c) 2003 by CodeSourcery, LLC.  All rights reserved. 
+#
+# For license terms see the file COPYING.
+#
+########################################################################
+
+########################################################################
+# Imports
+########################################################################
+
+import sys
+import pgdb
+
+########################################################################
+# Script
+########################################################################
+
+if len(sys.argv) != 2:
+    print "Usage: %s <database_name>" % (sys.argv[0],)
+    sys.exit(1)
+
+dbname = sys.argv[1]
+cxn = pgdb.connect(database=dbname)
+cursor = cxn.cursor()
+
+cursor.execute("""
+    CREATE TABLE db_schema_version (
+        version INT
+    )
+    """)
+cursor.execute("""
+    INSERT INTO db_schema_version (version) VALUES (1)
+    """)
+
+cursor.execute("""
+    CREATE SEQUENCE run_id_seq
+    """)
+
+cursor.execute("""
+    CREATE TABLE runs (
+        run_id INT PRIMARY KEY
+    )
+    """)
+
+cursor.execute("""
+    CREATE TABLE run_annotations (
+        run_id INT NOT NULL,
+        key TEXT NOT NULL,
+        value TEXT NOT NULL,
+        FOREIGN KEY (run_id) REFERENCES runs (run_id),
+        PRIMARY KEY (run_id, key)
+    )
+    """)
+        
+cursor.execute("""
+    CREATE TABLE results (
+        run_id INT NOT NULL,
+        result_id TEXT NOT NULL,
+        kind TEXT NOT NULL,
+        outcome TEXT NOT NULL,
+        FOREIGN KEY (run_id) REFERENCES runs (run_id),
+        PRIMARY KEY (run_id, result_id, kind)
+    )
+    """)
+cursor.execute("""
+    CREATE INDEX results_outcome_idx ON results (run_id, outcome)
+    """)
+cursor.execute("""
+    CREATE INDEX results_kind_idx ON results (run_id, kind)
+    """)
+
+cursor.execute("""
+    CREATE TABLE result_annotations (
+        run_id INT NOT NULL,
+        result_id TEXT NOT NULL,
+        result_kind TEXT NOT NULL,
+        key TEXT NOT NULL,
+        value TEXT NOT NULL,
+        FOREIGN KEY (run_id, result_id, result_kind)
+            REFERENCES results (run_id, result_id, kind),
+        PRIMARY KEY (run_id, result_id, result_kind, key)
+    )
+    """)
+
+cxn.commit()
+
+########################################################################
+# Local Variables:
+# mode: python
+# indent-tabs-mode: nil
+# fill-column: 72
+# End:

From mark at codesourcery.com  Thu Jul  3 06:20:59 2003
From: mark at codesourcery.com (Mark Mitchell)
Date: 02 Jul 2003 23:20:59 -0700
Subject: [qmtest] [PATCH] Run metadata and first pass at SQL results
	storage.
In-Reply-To: <20030703025324.GA11475@njs.dhis.org>
References: <20030703025324.GA11475@njs.dhis.org>
Message-ID: <1057213258.3330.0.camel@minax.codesourcery.com>

On Wed, 2003-07-02 at 19:53, Nathaniel Smith wrote:
> Attached for review.  This patch also renames 'ResultSource' to
> 'ResultReader', since that better describes the interface we ended up
> with.

Nathaniel --

This patch is very good.  I'm a little pedantic (as you'll see below),
but the bottom line is that this should go in, after a few very minor
changes get made.  Nice work!

-- Mark

+	* qm/extension.py (get_class_arguments): Mark's fixes to
+	support diamond inheritance.

This is too cryptic.  If you really want to give me credit, put my
name in the ChangeLog entry.  But there's no need for that -- just say
"Support diamond inheritance."

+    'time_secs' -- the time to be formatted, as returned by
+                   e.g. 'time.time()'."""

We don't indent the second line: just write:

   'time_secs' -- the time to be formatted, as returned by
   e.g. 'time.time()'."""

+class Connection:
+    """A little wrapper around a DB 2.0 connection that preserves a
+    reference to the containing module, and provides a minimal
+    interface to said connection.  This is useful because it gives us
+    a hook to attach our SQL-quoting code in to."""

Every class or method should have a 1-line description and then some
details, like so:

   """A wrapper around a DB 2.0 connection.

   An instance of 'Connection' preserves a reference to the containing
   module, and provides a minimal interface to said connection.  This
   is useful because it gives us a hook to attach our SQL-quoting code
   in to."""

But then, showing massive amounts of uptightness, we'd fix this
because (a) we don't use "this" as a noun, only as an ajdective, and
(b) we avoid the first person in comments.  Unfortunately, these
commandments have not always been obeyed. :-(  But, we want something
like the following for the second sentence:

  In addition, a 'Connection' quotes SQL queries as necessary for the
  underlying DB 2.0 connection.

+def connect(modname, *args, **moreargs):

Why not make this the __init__ method for Connection?

+                        ["dummy element to make __import__ behave"])

Wasteful: should just be [""] with a comment:

  # There must be at least one element or __import__ will crash.

+def quotestr(string):
+    """Quotes a string for SQL."""
+
+    # Replace each ' with '', then surround with more 's.  Also double
+    # backslashes.  It'd be nice to handle things like quoting
non-ASCII
+    # characters (by replacing them with octal escapes), but we don't.
+    return "'" + string.replace("'", "''").replace("\\", "\\\\") + "'"

Two comments:

(1) Does it make sense to make this a member of Connection?

(2) The:

    'string' -- ???

    returns -- ???

    comments are missing.

+# A nice subtlety is that because of how extension classes are loaded,

Let's not be editorial: it's a subtlety, but not a nice one.

+_annotation_sentinel = None

Shouldn't these be packaged up into a mix-in for
PickleResult{Stream,Reader}?

+    "Version 1", and all later versions, contain a pickled version
+    number as the first thing in the file.  In version 1, this is

Thanks for spelling this out!

+        elif version == 1:
+            self._ReadMetadata()

I'm not sure why we read this at __init__ time rather than lazily when
GetAnnotations is called, but that can be changed easily later if it
needs to be.

+class SQLConnected(Extension):
+    """Mixin class for classes that need a database connection."""

Shouldn't this be a module-private class? (e.g., _SQL_Connected?)

+        for result_id, result_kind, key, value in self._a_buffer:
+            if (result_id, result_kind) != (id, kind):
+                self._a_buffer.rewind()
+                break

Interesting -- I see that you avoid a separate SELECT per result.

+class XMLResultReader(FileResultReader):

Should this go in xml_result_stream.py?  (It seems to make sense to
group the readers and writers together?)

+                rs.WriteAnnotation("qmtest.run.end_time", end_time_str)

That's a slight extension of the "foo.bar" format we've used until
now; we've never had two "." characters in an annotation.  I think
that's OK, though; it's a natural generalization and nothing depends
on that.

     A 'StorageResultsStream' does not write any output.  It simply
     stores the results for future display."""

-    def __init__(self):
+    def __init__(self, arguments):

What's the rationale for this change?

+        # Put in a note.  No software currently pays any attention to
+        # this key, but it's useful to mark runs that were done from
+        # the GUI, because they may be an amalgamation of multiple
+        # runs, and therefore cannot be trusted to describe a single
+        # version of the software under test.
+        self.__annotations = { "qmtest.created_from_gui": "true" }

Let's leave this out for now.  It's not that I disagree, but I'm also
not sure that I agree, and we might want some kind of more general
"where did this run come from?" field.




From njs at pobox.com  Thu Jul  3 08:27:28 2003
From: njs at pobox.com (Nathaniel Smith)
Date: Thu, 3 Jul 2003 01:27:28 -0700
Subject: [qmtest] [PATCH] Run metadata and first pass at SQL results storage.
In-Reply-To: <1057213258.3330.0.camel@minax.codesourcery.com>
References: <20030703025324.GA11475@njs.dhis.org> <1057213258.3330.0.camel@minax.codesourcery.com>
Message-ID: <20030703082728.GB21062@njs.dhis.org>

On Wed, Jul 02, 2003 at 11:20:59PM -0700, Mark Mitchell wrote:
> +	* qm/extension.py (get_class_arguments): Mark's fixes to
> +	support diamond inheritance.
> 
> This is too cryptic.  If you really want to give me credit, put my
> name in the ChangeLog entry.  But there's no need for that -- just say
> "Support diamond inheritance."

Fixed.

> +    'time_secs' -- the time to be formatted, as returned by
> +                   e.g. 'time.time()'."""
> 
> We don't indent the second line: just write:
> 
>    'time_secs' -- the time to be formatted, as returned by
>    e.g. 'time.time()'."""

Hmm.  I feel like I must have done this other places, too, but I'll
watch out for it.  (Fixed here.)

> +class Connection:
> +    """A little wrapper around a DB 2.0 connection that preserves a
> +    reference to the containing module, and provides a minimal
> +    interface to said connection.  This is useful because it gives us
> +    a hook to attach our SQL-quoting code in to."""
> 
> Every class or method should have a 1-line description and then some
> details, like so:
> 
>    """A wrapper around a DB 2.0 connection.
> 
>    An instance of 'Connection' preserves a reference to the containing
>    module, and provides a minimal interface to said connection.  This
>    is useful because it gives us a hook to attach our SQL-quoting code
>    in to."""
> 
> But then, showing massive amounts of uptightness, we'd fix this
> because (a) we don't use "this" as a noun, only as an ajdective, and
> (b) we avoid the first person in comments.  Unfortunately, these
> commandments have not always been obeyed. :-(  But, we want something
> like the following for the second sentence:
> 
>   In addition, a 'Connection' quotes SQL queries as necessary for the
>   underlying DB 2.0 connection.

Whoops, right.  This file was written a few weeks ago, before I had
picked up some of this stuff, and I missed it when cleaning up the
formatting this evening.  Fixed.

> +def connect(modname, *args, **moreargs):
> 
> Why not make this the __init__ method for Connection?

Eh, why not indeed?  Fixed.

> +                        ["dummy element to make __import__ behave"])
> 
> Wasteful: should just be [""] with a comment:
> 
>   # There must be at least one element or __import__ will crash.

I'm not entirely sure how that's less wasteful (I guess a touch of
memory?), but sure.  That way I can also make it more informative,
because this is a really weird quirk of __import__.

        # Last argument must be a non-empty list or __import__ will
        # return the wrong module.

(You see, __import__("foo.bar", ...) returns a reference to foo if the
last argument is an empty list (the default), and foo.bar otherwise.
This is the only effect of the last argument.  No, really, you can
check the docs, it's true!)

> +def quotestr(string):
> +    """Quotes a string for SQL."""
> +
> +    # Replace each ' with '', then surround with more 's.  Also double
> +    # backslashes.  It'd be nice to handle things like quoting
> non-ASCII
> +    # characters (by replacing them with octal escapes), but we don't.
> +    return "'" + string.replace("'", "''").replace("\\", "\\\\") + "'"
> 
> Two comments:
> 
> (1) Does it make sense to make this a member of Connection?

Interesting question.  I think not, because it has no association with
any particular connection, it's just a way to quote SQL strings.  And
it's already in the SQL support module's namespace.

> (2) The:
> 
>     'string' -- ???
> 
>     returns -- ???
> 
>     comments are missing.

Fixed.  Also cleaned up the formatting in the rest of the module a bit
more...

> +# A nice subtlety is that because of how extension classes are loaded,
> 
> Let's not be editorial: it's a subtlety, but not a nice one.

Fair enough :-).  Fixed.

> +_annotation_sentinel = None
> 
> Shouldn't these be packaged up into a mix-in for
> PickleResult{Stream,Reader}?

That seems excessive cleverness.  They're already in a private
(module) namespace...

> +    "Version 1", and all later versions, contain a pickled version
> +    number as the first thing in the file.  In version 1, this is
> 
> Thanks for spelling this out!
> 
> +        elif version == 1:
> +            self._ReadMetadata()
> 
> I'm not sure why we read this at __init__ time rather than lazily when
> GetAnnotations is called, but that can be changed easily later if it
> needs to be.

I'm lazier than it is, I guess.  But yes, let's wait for a reason
before adding the complexity.

> +class SQLConnected(Extension):
> +    """Mixin class for classes that need a database connection."""
> 
> Shouldn't this be a module-private class? (e.g., _SQL_Connected?)

Sure, that's a bit more aesthetic.  Fixed.

> +        for result_id, result_kind, key, value in self._a_buffer:
> +            if (result_id, result_kind) != (id, kind):
> +                self._a_buffer.rewind()
> +                break
> 
> Interesting -- I see that you avoid a separate SELECT per result.

And even a separate FETCH with the buffering stuff, yeah.  Makes
things work much faster, while retaining scalability.

> +class XMLResultReader(FileResultReader):
> 
> Should this go in xml_result_stream.py?  (It seems to make sense to
> group the readers and writers together?)

I was wondering about this, since both the pickle and SQL streams went
that way.  Sure, fixed.

> +                rs.WriteAnnotation("qmtest.run.end_time", end_time_str)
> 
> That's a slight extension of the "foo.bar" format we've used until
> now; we've never had two "." characters in an annotation.  I think
> that's OK, though; it's a natural generalization and nothing depends
> on that.

Nod.  I just wanted to avoid polluting the top-level namespace too
much, considering we're likely to be stuck forevermore with whatever
we do now...

>      A 'StorageResultsStream' does not write any output.  It simply
>      stores the results for future display."""
> 
> -    def __init__(self):
> +    def __init__(self, arguments):
> 
> What's the rationale for this change?

Mostly it's because I made all the necessary changes for the first
version of the patch, when 'ResultStream' took annotations as
arguments.  But I left it in because it wasn't any work to do so, and
it makes 'StorageResultStream' consistent with every other extension
class we have.  Could take it out again if you like, but it seemed
like a zero-loss/small-win sort of thing.

> +        # Put in a note.  No software currently pays any attention to
> +        # this key, but it's useful to mark runs that were done from
> +        # the GUI, because they may be an amalgamation of multiple
> +        # runs, and therefore cannot be trusted to describe a single
> +        # version of the software under test.
> +        self.__annotations = { "qmtest.created_from_gui": "true" }
> 
> Let's leave this out for now.  It's not that I disagree, but I'm also
> not sure that I agree, and we might want some kind of more general
> "where did this run come from?" field.

Hmm.  Does it hurt to leave it in, though?  It's not intended as part
of any grand scheme, and doesn't preclude any grand schemes of the
future; in fact, as the comment says, it currently does absolutely
nothing.  It's just that I wanted to make sure that when someone's
trying to track down the history of some bug, six months after the
fact in a database that has perhaps been used by some of the less
perfect mortals among us, they would have the possiblity of saying
"wait a minute -- this data might be lying to me".  This seemed an
effective and innocuous way to achieve that.

I should sleep, I'm starting to wax lyrical.  I'll commit tomorrow...

And I suppose we should talk some about the writing of external test
suite wrappers; seems to me that the biggest problem is that there's
really no way to represent the dependencies at the moment.  The
build scripts are run as tests, but we need them to "act like
resources" when it comes time to run the tests in their test suites...
I suppose we could just ignore that for now and accept that it won't
work to run certain subsets of the tests, but we should at least
decide...

-- Nathaniel

-- 
Eternity is very long, especially towards the end.
  -- Woody Allen


From ghost at cs.msu.su  Thu Jul  3 12:52:06 2003
From: ghost at cs.msu.su (Vladimir Prus)
Date: Thu, 3 Jul 2003 16:52:06 +0400
Subject: [PATCH] buglet with attachements
Message-ID: <200307031652.06426.ghost@cs.msu.su>


I've just updated to CVS HEAD of QMTest, and have troubles with attachements.
I get stacktrace saying that TemporaryAttachmentStore instances has no __index 
field. 

The AttachmentStore.GetIndex is called from qm/fields.py (line 1246), but I 
can't find any code that sets __index filed. The problematic place is the 
only place where GetIndex is called.

Fixing that, I get exception on saving test. The attached patch fixes both 
problems.

Changelog entry:

* qm/attachment.py
  (AttachmentStore.GetIndex): Remove.

* qm/fields.py
  (SetField.FormatValueAsHtml): Don't call GetIndex()

* qm/test/clases/xml_database.py
   (XMLDatabase. __MakeDataFilePath): Call some member on self, not on
   nonexisted self.__database.

- Volodya

-------------- next part --------------
A non-text attachment was scrubbed...
Name: attachments.diff
Type: text/x-diff
Size: 2213 bytes
Desc: not available
URL: <http://sourcerytools.com/pipermail/qmtest/attachments/20030703/82ea635a/attachment.diff>

From ghost at cs.msu.su  Thu Jul  3 13:04:01 2003
From: ghost at cs.msu.su (Vladimir Prus)
Date: Thu, 3 Jul 2003 17:04:01 +0400
Subject: [PATCH] can't delete tests
Message-ID: <200307031704.01393.ghost@cs.msu.su>


I don't seem to be able to delete test using CVS HEAD. The problem is that 
HTML contains:

 function delete_item() {
    None
   }

which, naturally, does not work. The fix is really simple:

RCS file: /home/qm/Repository/qm/qm/test/web/web.py,v
retrieving revision 1.68
diff -u -r1.68 web.py
--- qm/test/web/web.py	25 Jun 2003 09:06:32 -0000	1.68
+++ qm/test/web/web.py	3 Jul 2003 13:01:16 -0000
@@ -1081,7 +1081,7 @@
         message = """
         <p>Are you sure you want to delete the %s %s?</p>
         """ % (self.type, item_id)
-        self.server.MakeConfirmationDialog(message, delete_url)
+        return self.server.MakeConfirmationDialog(message, delete_url)
 
 
- Volodya



From mark at codesourcery.com  Thu Jul  3 16:51:19 2003
From: mark at codesourcery.com (Mark Mitchell)
Date: 03 Jul 2003 09:51:19 -0700
Subject: [qmtest] [PATCH] can't delete tests
In-Reply-To: <200307031704.01393.ghost@cs.msu.su>
References: <200307031704.01393.ghost@cs.msu.su>
Message-ID: <1057251078.3330.16.camel@minax.codesourcery.com>

On Thu, 2003-07-03 at 06:04, Vladimir Prus wrote:
> I don't seem to be able to delete test using CVS HEAD. The problem is that 
> HTML contains:
> 
>  function delete_item() {
>     None
>    }
> 
> which, naturally, does not work. The fix is really simple:

Thanks, applied!

(If you have time, it's nice if you can submit a ChangeLog entry with
the patch and put the patch in an attachment, rather than in the body
text.  But we're grateful for patches in any case!)

-- 
Mark Mitchell <mark at codesourcery.com>
CodeSourcery, LLC



From mark at codesourcery.com  Thu Jul  3 16:54:22 2003
From: mark at codesourcery.com (Mark Mitchell)
Date: 03 Jul 2003 09:54:22 -0700
Subject: [qmtest] [PATCH] buglet with attachements
In-Reply-To: <200307031652.06426.ghost@cs.msu.su>
References: <200307031652.06426.ghost@cs.msu.su>
Message-ID: <1057251262.3330.18.camel@minax.codesourcery.com>

On Thu, 2003-07-03 at 05:52, Vladimir Prus wrote:
> I've just updated to CVS HEAD of QMTest, and have troubles with attachements.
> I get stacktrace saying that TemporaryAttachmentStore instances has no __index 
> field. 
> 
> The AttachmentStore.GetIndex is called from qm/fields.py (line 1246), but I 
> can't find any code that sets __index filed. The problematic place is the 
> only place where GetIndex is called.
> 
> Fixing that, I get exception on saving test. The attached patch fixes both 
> problems.

Thanks!  When I reworked that code, I obviously missed a few spots.  I'm
very glad you found and fixed the problems.

Patch applied.

Yours,

-- 
Mark Mitchell <mark at codesourcery.com>
CodeSourcery, LLC



From ghost at cs.msu.su  Fri Jul  4 11:02:18 2003
From: ghost at cs.msu.su (Vladimir Prus)
Date: Fri, 4 Jul 2003 15:02:18 +0400
Subject: [qmtest] [PATCH] can't delete tests
In-Reply-To: <1057251078.3330.16.camel@minax.codesourcery.com>
References: <200307031704.01393.ghost@cs.msu.su> <1057251078.3330.16.camel@minax.codesourcery.com>
Message-ID: <200307041502.18963.ghost@cs.msu.su>

Mark Mitchell wrote:
> On Thu, 2003-07-03 at 06:04, Vladimir Prus wrote:
> > I don't seem to be able to delete test using CVS HEAD. The problem is
> > that HTML contains:
> >
> >  function delete_item() {
> >     None
> >    }
> >
> > which, naturally, does not work. The fix is really simple:
>
> Thanks, applied!

Thanks.

> (If you have time, it's nice if you can submit a ChangeLog entry with
> the patch and put the patch in an attachment, rather than in the body
> text.  But we're grateful for patches in any case!)

Sure. For this patch specifically, I though it's too small to write changelog 
at all. Will try to write changelog for all future patches. 

- Volodya



From ghost at cs.msu.su  Wed Jul  9 11:05:16 2003
From: ghost at cs.msu.su (Vladimir Prus)
Date: Wed, 9 Jul 2003 15:05:16 +0400
Subject: Context handling bug?
Message-ID: <200307091505.16681.ghost@cs.msu.su>


Hello,
I seem to have trouble using context with current CVS HEAD. Symptoms: I've got 
command.ShellCommandTest test and it makes use of QMV_root shell variable. 
The context file contains the definition of "root" variable, which should be 
translated to QMV_root. However, by the time test class' Run method is 
called, the variable is not here.

It disappears in target.py:201, which reads:

            context = Context(context)

The problem is that "Context" __init__ method stores passed context in member 
variable __context. It correctly overrides __getitem__ to look both in self 
and in self.__context, but it seems does not override necessary iteration 
methods. As the result, this code from command.py 
(ExecTestBase.MakeEnvironment):

        for key, value in context.items():
            if "." not in key and type(value) == types.StringType:
                name = "QMV_" + key
                environment[name] = value

does not see the assingments in context.__context.

Unfortunately, I cannot quickly fix this, because I never did emulation of 
mappings. Any ideas what should be added?

- Volodya





From njs at codesourcery.com  Wed Jul  9 22:08:45 2003
From: njs at codesourcery.com (Nathaniel Smith)
Date: Wed, 9 Jul 2003 15:08:45 -0700
Subject: [qmtest] Context handling bug?
In-Reply-To: <200307091505.16681.ghost@cs.msu.su>
References: <200307091505.16681.ghost@cs.msu.su>
Message-ID: <20030709220845.GA18154@njs.dhis.org>

On Wed, Jul 09, 2003 at 03:05:16PM +0400, Vladimir Prus wrote:
[Context.items() doesn't work right.]

Not too hard to fix; attached is a little patch that should make
things work right (it passes the test suite, but I haven't tested it
beyond that).  Let us know if it works for you...

(This patch leaves Context broken wrt other iteration interfaces, but
I'll get those later...)

-- Nathaniel

-- 
"Of course, the entire effort is to put oneself
 Outside the ordinary range
 Of what are called statistics."
  -- Stephan Spender
-------------- next part --------------
diff -urN --exclude='*~' --exclude='.*' --exclude=CVS --exclude='*.pyo' --exclude='*.pyc' --exclude=build --exclude=GNUmakefile --exclude=config.log --exclude=config.status --exclude=setup_path.py --exclude=qm.sh --exclude=qm.spec qm-clean/ChangeLog qm-context-items/ChangeLog
--- qm-clean/ChangeLog	2003-07-03 12:28:22.000000000 -0700
+++ qm-context-items/ChangeLog	2003-07-09 14:50:06.000000000 -0700
@@ -1,3 +1,7 @@
+2003-07-09  Nathaniel Smith  <njs at codesourcery.com>
+
+	* qm/test/context.py (Context.items): New method.
+
 2003-07-02  Nathaniel Smith  <njs at codesourcery.com>
 
 	* qm/test/base.py (load_results): Rename 'ResultSource' to
diff -urN --exclude='*~' --exclude='.*' --exclude=CVS --exclude='*.pyo' --exclude='*.pyc' --exclude=build --exclude=GNUmakefile --exclude=config.log --exclude=config.status --exclude=setup_path.py --exclude=qm.sh --exclude=qm.spec qm-clean/qm/test/context.py qm-context-items/qm/test/context.py
--- qm-clean/qm/test/context.py	2003-06-21 00:57:48.000000000 -0700
+++ qm-context-items/qm/test/context.py	2003-07-09 14:49:40.000000000 -0700
@@ -165,6 +165,20 @@
                 raise ContextException(key)
 
 
+    def items(self):
+
+        if self.__context is None:
+            return super(Context, self).items()
+        else:
+            # Have to be careful, because self.__context and self may
+            # contain different values for the same keys, and the values
+            # defined in self should override the values defined in
+            # self.__context.
+            unified_dict = dict(self.__context.items())
+            unified_dict.update(self)
+            return unified_dict.items()
+
+
     # Helper methods.
 
     def GetAddedProperties(self):

From mark at codesourcery.com  Wed Jul  9 23:02:46 2003
From: mark at codesourcery.com (Mark Mitchell)
Date: 09 Jul 2003 16:02:46 -0700
Subject: [qmtest] Context handling bug?
In-Reply-To: <20030709220845.GA18154@njs.dhis.org>
References: <200307091505.16681.ghost@cs.msu.su> 
	<20030709220845.GA18154@njs.dhis.org>
Message-ID: <1057791767.3520.27.camel@doubledemon.codesourcery.com>

On Wed, 2003-07-09 at 15:08, Nathaniel Smith wrote:
> On Wed, Jul 09, 2003 at 03:05:16PM +0400, Vladimir Prus wrote:
> [Context.items() doesn't work right.]
> 
> Not too hard to fix; attached is a little patch that should make
> things work right (it passes the test suite, but I haven't tested it
> beyond that).  Let us know if it works for you...

Looks good to me -- check it in!

Thanks,

-- 
Mark Mitchell
CodeSourcery, LLC
mark at codesourcery.com



From ghost at cs.msu.su  Mon Jul 14 07:23:21 2003
From: ghost at cs.msu.su (Vladimir Prus)
Date: Mon, 14 Jul 2003 11:23:21 +0400
Subject: [PATCH] gui->run broken in HEAD
Message-ID: <200307141123.21349.ghost@cs.msu.su>


Hello,
I can't get any tests to execute from GUI, using CVS HEAD.
Here's what I see in stdout:

File "/home/ghost/build/Tools/qm-up-to-date/qm/test/result_stream.py", line 
79, in WriteAllAnnotations
    self.WriteAnnotation(key, value)
  File "/home/ghost/build/Tools/qm-up-to-date/qm/test/web/web.py", line 1243, 
in WriteAnnotation
    self.__annotations[key] = value
  File "/home/ghost/build/Tools/qm-up-to-date/qm/extension.py", line 113, in 
__getattr__
    raise AttributeError, name
AttributeError: _StorageResultsStream__annotations

I'm guessing it's because commit from

"2003-07-02  Nathaniel Smith  <njs at codesourcery.com>"
says: 
* qm/test/web.py (StorageResultsStream.__init__): Use 'super'.
        Handle annotations.

has added StorageResultStream.WriteAnnotations, but initialization of 
__annocations was omitted. The attached trivial patch fixes the problem 
(unless I've missed something).

Changelog entry:
Fix a nit which prevented running tests from GUI.

* qm/test/web/web.py (StorageResultStream.__init__): Initialize __attributes 
   member.

Patch attached.


- Volodya



-------------- next part --------------
A non-text attachment was scrubbed...
Name: annotations.diff
Type: text/x-diff
Size: 616 bytes
Desc: not available
URL: <http://sourcerytools.com/pipermail/qmtest/attachments/20030714/fe10fc41/attachment.diff>

From ghost at cs.msu.su  Mon Jul 14 07:40:37 2003
From: ghost at cs.msu.su (Vladimir Prus)
Date: Mon, 14 Jul 2003 11:40:37 +0400
Subject: [PATCH] unbreak test directory page
Message-ID: <200307141140.37290.ghost@cs.msu.su>


The patch should be obvious: a function with two parameters used to be called 
with only one.

Changelog entry:
Fix a nit which broken test directory page.

* qt/test/web/web.py
   (DirPage.GetTestResultsForDirectory): Call __IsLabelInDirectory with
    two parameters, as it should be, not with one.

Patch attached.

BTW, I wonder if you have any automated tests for QMTest GUI itself?  I'm not  
aware of any free tools which can help, or have the time to help with it 
right now, but ultimately, extra tests would be a big help to QMTest.

- Volodya
-------------- next part --------------
A non-text attachment was scrubbed...
Name: dir.diff
Type: text/x-diff
Size: 694 bytes
Desc: not available
URL: <http://sourcerytools.com/pipermail/qmtest/attachments/20030714/fdbb097a/attachment.diff>

From ghost at cs.msu.su  Mon Jul 14 07:57:47 2003
From: ghost at cs.msu.su (Vladimir Prus)
Date: Mon, 14 Jul 2003 11:57:47 +0400
Subject: [PATCH] yet another directory handling problem
Message-ID: <200307141157.47273.ghost@cs.msu.su>


Apologies, but my last patch is not 100% complete. It stops exceptions, but 
there was another logic problem in HEAD that I've missed.
DirPage.__IsLabelInDirectory returns 0 when called with "asm.macro" is id and 
"asm" as directory, and that's not good.

Changelog:
* qm/test/web/web.py
   (DirPage.__IsLabelInDirectory): Fix typo.

Patch attached.

- Volodya
-------------- next part --------------
A non-text attachment was scrubbed...
Name: dir2.diff
Type: text/x-diff
Size: 684 bytes
Desc: not available
URL: <http://sourcerytools.com/pipermail/qmtest/attachments/20030714/22109c6c/attachment.diff>

From ghost at cs.msu.su  Mon Jul 14 08:25:32 2003
From: ghost at cs.msu.su (Vladimir Prus)
Date: Mon, 14 Jul 2003 12:25:32 +0400
Subject: Problem loading expectation
Message-ID: <200307141225.32295.ghost@cs.msu.su>


Hello,
seems like CVS HEAD cannot load expectation file written by previous QMTest 
versions. The symptoms are that I specify -O option but no expectation appear 
anywhere.

Debugging further, I see that PickleResultStream.GetResult catches 
cPickle.UnpicklingError exception, which, when printed, reads:

    unpickling stack underflow

I don't really know what this means, and don't know what to do about it. 
Anyway, is this a known issue? I see some code for handling different 
versions of pickled result stream, so maybe it's just a bug?

- Volodya



From ghost at cs.msu.su  Mon Jul 14 12:25:58 2003
From: ghost at cs.msu.su (Vladimir Prus)
Date: Mon, 14 Jul 2003 16:25:58 +0400
Subject: [PATCH] fix reading attachments data 
Message-ID: <200307141625.58376.ghost@cs.msu.su>


Hello,
I think reading of attachment data is broken in HEAD.

I have

 arguments = [
        qm.fields.AttachmentField(
            name="history",

and later:

        expected_trace = self.history.GetData()

The last line does not work:

   exceptions.IOError: [Errno 2] No such file or directory:  
      'history.qms/test13.itrace' 

I think it's caused by this change:
  
2003-01-03  Mark Mitchell  <mark at codesourcery.com>

        * qm/test/database.py (ItemDescriptor.__init__): Do not set
        __working_directory.
        (ItemDescriptor.SetWorkingDirectory): Remove.
        (ItemDescriptor.GetWorkingDirectory): Likewise.
        (ItemDescriptor._Execute): Do not change directories.
        * qm/test/classes/xml_database.py (XMLDatabase.__LoadItem): Do not
        use SetWorkingDirectory.

The FileAttachmentStore does not know its own location, and all attachments 
are stored using relative location to test db. Since there's no change of 
directory now, opening of attachment files does not work.

Actually, I'm not sure why the above change was made (and changelog does not 
say about motivation), but assuming it's for a good reason, the attached 
patch fixed the problem I've run into.

Changelog entry:

Fix reading of attachment data.

* qm/attachment.py: 
   (FileAttachmentStore.__init__): New argument 'root'.
   (FileAttachmentStore.GetDataFile): Treat location as relative to
     'self.root'.
   (TemporaryAttachmentStore.__init__): Pass own path to parent's __init__.

* qm/test/classes/xml_database.py
   (XMLDatabase.__init__): Pass path to FileAttachmentStore.__init__

Patch attached.

- Volodya
-------------- next part --------------
A non-text attachment was scrubbed...
Name: attachments.diff
Type: text/x-diff
Size: 2265 bytes
Desc: not available
URL: <http://sourcerytools.com/pipermail/qmtest/attachments/20030714/d043ce71/attachment.diff>

From njs at pobox.com  Tue Jul 15 00:12:00 2003
From: njs at pobox.com (Nathaniel Smith)
Date: Mon, 14 Jul 2003 17:12:00 -0700
Subject: [qmtest] Problem loading expectation
In-Reply-To: <200307141225.32295.ghost@cs.msu.su>
References: <200307141225.32295.ghost@cs.msu.su>
Message-ID: <20030715001159.GB5026@njs.dhis.org>

On Mon, Jul 14, 2003 at 12:25:32PM +0400, Vladimir Prus wrote:
> 
> Hello,
> seems like CVS HEAD cannot load expectation file written by previous QMTest 
> versions. The symptoms are that I specify -O option but no expectation appear 
> anywhere.
> 
> Debugging further, I see that PickleResultStream.GetResult catches 
> cPickle.UnpicklingError exception, which, when printed, reads:
> 
>     unpickling stack underflow
> 
> I don't really know what this means, and don't know what to do about it. 
> Anyway, is this a known issue? I see some code for handling different 
> versions of pickled result stream, so maybe it's just a bug?

I have no idea what caused that error, but I did find a stupid bug
that prevented old-style file loading from working right.  Attached is
the patch I've just committed to CVS, and you can see if it fixes your
problem...

-- Nathaniel

-- 
Details are all that matters; God dwells there, and you never get to
see Him if you don't struggle to get them right. -- Stephen Jay Gould


From njs at pobox.com  Tue Jul 15 02:59:07 2003
From: njs at pobox.com (Nathaniel Smith)
Date: Mon, 14 Jul 2003 19:59:07 -0700
Subject: [qmtest] Problem loading expectation
In-Reply-To: <20030715001159.GB5026@njs.dhis.org>
References: <200307141225.32295.ghost@cs.msu.su> <20030715001159.GB5026@njs.dhis.org>
Message-ID: <20030715025907.GA27342@njs.dhis.org>

On Mon, Jul 14, 2003 at 05:12:00PM -0700, Nathaniel Smith wrote:
[the attachment-eating spiders strike again]

-- Nathaniel

-- 
"But in Middle-earth, the distinct accusative case disappeared from
the speech of the Noldor (such things happen when you are busy
fighting Orcs, Balrogs, and Dragons)."
-------------- next part --------------
diff -urN --exclude='*~' --exclude='.*' --exclude=CVS --exclude='*.pyo' --exclude='*.pyc' --exclude=build --exclude=GNUmakefile --exclude=config.log --exclude=config.status --exclude=setup_path.py --exclude=qm.sh --exclude=qm.spec qm-clean/ChangeLog qm-pickle-reading/ChangeLog
--- qm-clean/ChangeLog	2003-07-09 17:33:44.000000000 -0700
+++ qm-pickle-reading/ChangeLog	2003-07-14 17:05:24.000000000 -0700
@@ -1,3 +1,12 @@
+2003-07-14  Nathaniel Smith  <njs at codesourcery.com>
+
+	* qm/test/classes/pickle_result_stream.py
+	(PickleResultReader.GetResult): Don't catch UnpicklingError;
+	it's not necessary with Python 2.2.
+	(PickleResultReader.__init__): Likewise.
+	Also, fix typo (call self._ResetUnpickler, not
+	self._ResetPickler).
+	
 2003-07-09  Nathaniel Smith  <njs at codesourcery.com>
 
 	* qm/test/context.py (Context.items): New method.
diff -urN --exclude='*~' --exclude='.*' --exclude=CVS --exclude='*.pyo' --exclude='*.pyc' --exclude=build --exclude=GNUmakefile --exclude=config.log --exclude=config.status --exclude=setup_path.py --exclude=qm.sh --exclude=qm.spec qm-clean/qm/test/classes/pickle_result_stream.py qm-pickle-reading/qm/test/classes/pickle_result_stream.py
--- qm-clean/qm/test/classes/pickle_result_stream.py	2003-07-03 12:28:22.000000000 -0700
+++ qm-pickle-reading/qm/test/classes/pickle_result_stream.py	2003-07-14 17:04:14.000000000 -0700
@@ -181,7 +181,7 @@
         # Check for a version number
         try:
             version = self.__unpickler.load()
-        except (EOFError, cPickle.UnpicklingError):
+        except EOFError:
             # This file is empty, no more handling needed.
             return
         
@@ -190,7 +190,7 @@
             # holding a 'Result'.  So we have no metadata to load and
             # should just rewind.
             self.file.seek(0)
-            self._ResetPickler()
+            self._ResetUnpickler()
         elif version == 1:
             self._ReadMetadata()
         else:
@@ -251,9 +251,6 @@
                 thing = self.__unpickler.load()
             except EOFError:
                 return None
-            except cPickle.UnpicklingError:
-                # This is raised at EOF if file is a StringIO.
-                return None
             else:
                 if thing is _annotation_sentinel:
                     # We're looking for results, but this is an annotation,

From ghost at cs.msu.su  Tue Jul 15 04:45:50 2003
From: ghost at cs.msu.su (Vladimir Prus)
Date: Tue, 15 Jul 2003 08:45:50 +0400
Subject: [qmtest] Problem loading expectation
In-Reply-To: <20030715001159.GB5026@njs.dhis.org>
References: <200307141225.32295.ghost@cs.msu.su> <20030715001159.GB5026@njs.dhis.org>
Message-ID: <200307150845.50265.ghost@cs.msu.su>

Hi Nathaniel,

> > Debugging further, I see that PickleResultStream.GetResult catches
> > cPickle.UnpicklingError exception, which, when printed, reads:
> >
> >     unpickling stack underflow
> >
> > I don't really know what this means, and don't know what to do about it.
> > Anyway, is this a known issue? I see some code for handling different
> > versions of pickled result stream, so maybe it's just a bug?
>
> I have no idea what caused that error, but I did find a stupid bug
> that prevented old-style file loading from working right.  Attached is
> the patch I've just committed to CVS, and you can see if it fixes your
> problem...

No luck. I get the following:

Traceback (most recent call last):
  File "/home/ghost/build/Tools/qm-up-to-date/qm/test/qmtest.py", line 110, in 
?
    exit_code = main()
  File "/home/ghost/build/Tools/qm-up-to-date/qm/test/qmtest.py", line 91, in 
main
    exit_code = command.Execute()
  File "/home/ghost/build/Tools/qm-up-to-date/qm/test/cmdline.py", line 674, 
in Execute
    return method()
  File "/home/ghost/build/Tools/qm-up-to-date/qm/test/cmdline.py", line 1445, 
in __ExecuteServer
    self.__GetExpectedOutcomes())
  File "/home/ghost/build/Tools/qm-up-to-date/qm/test/cmdline.py", line 1522, 
in __GetExpectedOutcomes
    self.GetDatabase())
  File "/home/ghost/build/Tools/qm-up-to-date/qm/test/base.py", line 351, in 
load_outcomes
    results = load_results(file, database)
  File "/home/ghost/build/Tools/qm-up-to-date/qm/test/base.py", line 384, in 
load_results
    return reader_cls({"file": file})
  File 
"/home/ghost/build/Tools/qm-up-to-date/qm/test/classes/pickle_result_stream.py", 
line 183, in __init__
    version = self.__unpickler.load()
cPickle.UnpicklingError: <class 'qm.test.context.Context'> is not safe for 
unpickling


- Volodya



From ghost at cs.msu.su  Tue Jul 15 05:33:03 2003
From: ghost at cs.msu.su (Vladimir Prus)
Date: Tue, 15 Jul 2003 09:33:03 +0400
Subject: [PATCH] fix "save expectation" command
Message-ID: <200307150933.03302.ghost@cs.msu.su>


The command is broken because since 2003-05-09 the Result.__init__ method does 
not have "context" parameter, while QMTestSever.HandleSaveExpectations tries 
to pass it.

Changelog:

* qm/test/web/web.py (QMTestServer.HandleSaveExpectations): Do not pass
  'context' parameter to Result.__init__, since that parameter now does not
   exist.

Patch attached.

- Volodya
-------------- next part --------------
A non-text attachment was scrubbed...
Name: expectations.diff
Type: text/x-diff
Size: 652 bytes
Desc: not available
URL: <http://sourcerytools.com/pipermail/qmtest/attachments/20030715/364305bb/attachment.diff>

From ghost at cs.msu.su  Tue Jul 15 05:36:27 2003
From: ghost at cs.msu.su (Vladimir Prus)
Date: Tue, 15 Jul 2003 09:36:27 +0400
Subject: [qmtest] Problem loading expectation
In-Reply-To: <200307150845.50265.ghost@cs.msu.su>
References: <200307141225.32295.ghost@cs.msu.su> <20030715001159.GB5026@njs.dhis.org> <200307150845.50265.ghost@cs.msu.su>
Message-ID: <200307150936.27106.ghost@cs.msu.su>

Vladimir Prus wrote:
> > I have no idea what caused that error, but I did find a stupid bug
> > that prevented old-style file loading from working right.  Attached is
> > the patch I've just committed to CVS, and you can see if it fixes your
> > problem...
>
> No luck. I get the following:
>
> Traceback (most recent call last):
>   File "/home/ghost/build/Tools/qm-up-to-date/qm/test/qmtest.py", line 110,

[....]

>     version = self.__unpickler.load()
> cPickle.UnpicklingError: <class 'qm.test.context.Context'> is not safe for
> unpickling

FWIW, I've worked this around and successfully loaded the expectation file 
after making the following change:

--- qm/test/context.py  10 Jul 2003 00:33:02 -0000      1.11
+++ qm/test/context.py  15 Jul 2003 05:32:13 -0000
@@ -66,6 +66,9 @@
     temporary directory at the same time.  There is no guarantee that
     the temporary directory is empty, however; it may contain files
     left behind by the execution of other 'Runnable' objects."""
+
+    __safe_for_unpickling__ = 1
+

     def __init__(self, context = None):
         """Construct a new context.


OTOH, I have no idea
- why it's necessary
- why Result class is readed OK, but Context is not.

Can anybody shed any light on this?

- Volodya



From njs at pobox.com  Tue Jul 15 06:50:35 2003
From: njs at pobox.com (Nathaniel Smith)
Date: Mon, 14 Jul 2003 23:50:35 -0700
Subject: [qmtest] Problem loading expectation
In-Reply-To: <200307150936.27106.ghost@cs.msu.su>
References: <200307141225.32295.ghost@cs.msu.su> <20030715001159.GB5026@njs.dhis.org> <200307150845.50265.ghost@cs.msu.su> <200307150936.27106.ghost@cs.msu.su>
Message-ID: <20030715065035.GA1088@njs.dhis.org>

On Tue, Jul 15, 2003 at 09:36:27AM +0400, Vladimir Prus wrote:
> Vladimir Prus wrote:
[...]
> >     version = self.__unpickler.load()
> > cPickle.UnpicklingError: <class 'qm.test.context.Context'> is not safe for
> > unpickling
> 
> FWIW, I've worked this around and successfully loaded the expectation file 
> after making the following change:
[...]
> OTOH, I have no idea
> - why it's necessary
> - why Result class is readed OK, but Context is not.
> 
> Can anybody shed any light on this?

No, I really can't.  All your error messages seem entirely impossible
to me :-).

Can you send, either to the list or to me personally,
   - the value of your Python's sys.version,
   - where you got your Python, just in case (e.g., "Redhat 8.0
     package")
   - the exact version of the qmtest you're using (e.g., "HEAD,
     updated yesterday")
   - and a file you're having these errors on?

-- Nathaniel

-- 
"...these, like all words, have single, decontextualized meanings: everyone
knows what each of these words means, everyone knows what constitutes an
instance of each of their referents.  Language is fixed.  Meaning is
certain.  Santa Claus comes down the chimney at midnight on December 24."
  -- The Language War, Robin Lakoff


From ghost at cs.msu.su  Tue Jul 15 07:24:05 2003
From: ghost at cs.msu.su (Vladimir Prus)
Date: Tue, 15 Jul 2003 11:24:05 +0400
Subject: [qmtest] Problem loading expectation
In-Reply-To: <20030715065035.GA1088@njs.dhis.org>
References: <200307141225.32295.ghost@cs.msu.su> <200307150936.27106.ghost@cs.msu.su> <20030715065035.GA1088@njs.dhis.org>
Message-ID: <200307151124.05085.ghost@cs.msu.su>

Nathaniel Smith wrote:

> > OTOH, I have no idea
> > - why it's necessary
> > - why Result class is readed OK, but Context is not.
> >
> > Can anybody shed any light on this?
>
> No, I really can't.  All your error messages seem entirely impossible
> to me :-).
>
> Can you send, either to the list or to me personally,
>    - the value of your Python's sys.version,

'2.2.3+ (#1, Jul  5 2003, 11:04:18) \n[GCC 3.3.1 20030626 (Debian 
prerelease)]'

>    - where you got your Python, just in case (e.g., "Redhat 8.0
>      package")

From Debian package. Version: 2.2.3-3

>    - the exact version of the qmtest you're using (e.g., "HEAD,
>      updated yesterday")

HEAD, updated right now. No local modifications.

>    - and a file you're having these errors on?

   http://zigzag.cs.msu.su:7813/expectations.qmr

Just in case you'd need test db, it at

   http://zigzag.cs.msu.su:7813/tdb.tar.bz2

The exception is thrown immediately on running:

/home/ghost/build/Tools/qm-cvs/qm/test/qmtest -D tdb gui -O expectations.qmr

Thanks,
Volodya



From mark at codesourcery.com  Tue Jul 15 18:02:48 2003
From: mark at codesourcery.com (Mark Mitchell)
Date: 15 Jul 2003 11:02:48 -0700
Subject: [qmtest] [PATCH] gui->run broken in HEAD
In-Reply-To: <200307141123.21349.ghost@cs.msu.su>
References: <200307141123.21349.ghost@cs.msu.su>
Message-ID: <1058292167.15765.53.camel@minax.codesourcery.com>

On Mon, 2003-07-14 at 00:23, Vladimir Prus wrote:
> Hello,
> I can't get any tests to execute from GUI, using CVS HEAD.
> Here's what I see in stdout:
> 
> File "/home/ghost/build/Tools/qm-up-to-date/qm/test/result_stream.py", line 
> 79, in WriteAllAnnotations
>     self.WriteAnnotation(key, value)
>   File "/home/ghost/build/Tools/qm-up-to-date/qm/test/web/web.py", line 1243, 
> in WriteAnnotation
>     self.__annotations[key] = value
>   File "/home/ghost/build/Tools/qm-up-to-date/qm/extension.py", line 113, in 
> __getattr__
>     raise AttributeError, name
> AttributeError: _StorageResultsStream__annotations
> 
> I'm guessing it's because commit from
> 
> "2003-07-02  Nathaniel Smith  <njs at codesourcery.com>"
> says: 
> * qm/test/web.py (StorageResultsStream.__init__): Use 'super'.
>         Handle annotations.
> 
> has added StorageResultStream.WriteAnnotations, but initialization of 
> __annocations was omitted. The attached trivial patch fixes the problem 
> (unless I've missed something).
> 
> Changelog entry:
> Fix a nit which prevented running tests from GUI.
> 
> * qm/test/web/web.py (StorageResultStream.__init__): Initialize __attributes 
>    member.

I applied this patch.

Thanks!

-- 
Mark Mitchell <mark at codesourcery.com>
CodeSourcery, LLC



From mark at codesourcery.com  Tue Jul 15 18:06:03 2003
From: mark at codesourcery.com (Mark Mitchell)
Date: 15 Jul 2003 11:06:03 -0700
Subject: [qmtest] [PATCH] fix "save expectation" command
In-Reply-To: <200307150933.03302.ghost@cs.msu.su>
References: <200307150933.03302.ghost@cs.msu.su>
Message-ID: <1058292363.15765.55.camel@minax.codesourcery.com>

On Mon, 2003-07-14 at 22:33, Vladimir Prus wrote:
> The command is broken because since 2003-05-09 the Result.__init__ method does 
> not have "context" parameter, while QMTestSever.HandleSaveExpectations tries 
> to pass it.

I applied this patch.

Thanks!

-- 
Mark Mitchell <mark at codesourcery.com>
CodeSourcery, LLC



From mark at codesourcery.com  Tue Jul 15 18:10:02 2003
From: mark at codesourcery.com (Mark Mitchell)
Date: 15 Jul 2003 11:10:02 -0700
Subject: [qmtest] [PATCH] yet another directory handling problem
In-Reply-To: <200307141157.47273.ghost@cs.msu.su>
References: <200307141157.47273.ghost@cs.msu.su>
Message-ID: <1058292602.15765.58.camel@minax.codesourcery.com>

On Mon, 2003-07-14 at 00:57, Vladimir Prus wrote:
> Apologies, but my last patch is not 100% complete. It stops exceptions, but 
> there was another logic problem in HEAD that I've missed.
> DirPage.__IsLabelInDirectory returns 0 when called with "asm.macro" is id and 
> "asm" as directory, and that's not good.

I appplied this patch and the other one.  Thanks!

> BTW, I wonder if you have any automated tests for QMTest GUI itself?

We don't have any such tests.

For a while now, I've been wanting to write a test class which would at
least visit particular URLs and post forms; the test would pass if
QMTest did not crash after running through the URLs and forms.  That
would not validate the appearance in the GUI, but it would at least
prevent the kinds of things you've been fixing with your recent patches.

Yours,

-- 
Mark Mitchell <mark at codesourcery.com>
CodeSourcery, LLC



From mark at codesourcery.com  Tue Jul 15 18:50:16 2003
From: mark at codesourcery.com (Mark Mitchell)
Date: 15 Jul 2003 11:50:16 -0700
Subject: [qmtest] [PATCH] fix reading attachments data
In-Reply-To: <200307141625.58376.ghost@cs.msu.su>
References: <200307141625.58376.ghost@cs.msu.su>
Message-ID: <1058295016.15765.73.camel@minax.codesourcery.com>

On Mon, 2003-07-14 at 05:25, Vladimir Prus wrote:
> Hello,
> I think reading of attachment data is broken in HEAD.

I checked in this variant of your patch.  Thanks!

-- 
Mark Mitchell <mark at codesourcery.com>
CodeSourcery, LLC
-------------- next part --------------
2003-07-15  Vladimir Prus <ghost at cs.msu.su>
	    Mark Mitchell <mark at codesourcery.com>

	* qm/attachment.py (AttachmentStore.__init__): Remove.
	(FileAttachmentStore.__init__): New method.
	(FileAttachmentStore.GetDataFile): Support relative paths.
	(TemporaryAttachmentStore.__init__): Adjust accordingly.
	(TemporaryAttachmentStore.GetDataFile): Remove.
	* qm/test/classes/xml_database.py (XMLDatabase.__init__): Use the
	database root when creating the attachment store.

Index: qm/attachment.py
===================================================================
RCS file: /home/sc/Repository/qm/qm/attachment.py,v
retrieving revision 1.18
diff -c -5 -p -r1.18 attachment.py
*** qm/attachment.py	3 Jul 2003 16:52:40 -0000	1.18
--- qm/attachment.py	15 Jul 2003 18:45:55 -0000
*************** class AttachmentStore(object):
*** 184,199 ****
  
      Please note that the 'AttachmentStore' interface provides methods
      for retrieving attachment data only; not for storing it.  The
      interface for storing may be defined in any way by implementations."""
  
-     def __init__(self):
-         """Create a new 'AttachmentStore'."""
- 
-         pass
- 
-     
      def GetData(self, location):
          """Return the data for an attachment.
  
          returns -- A string containing the attachment data."""
  
--- 184,193 ----
*************** class AttachmentStore(object):
*** 256,265 ****
--- 250,270 ----
  class FileAttachmentStore(AttachmentStore):
      """An attachment store based on the file system.
  
      The locations are the names of files in the file system."""
  
+     def __init__(self, root = None):
+         """Construct a new 'FileAttachmentStore'
+ 
+         'root' -- If not 'None', the root directory for the store.  All
+         locations are relative to this directory.  If 'None', all
+         locations are relative to the current directory."""
+         
+         super(AttachmentStore, self).__init__()
+         self.__root = root
+         
+         
      def GetData(self, location):
  
          # Open the file.
          f = open(self.GetDataFile(location))
          # Read the contents.
*************** class FileAttachmentStore(AttachmentStor
*** 270,280 ****
          return s
  
  
      def GetDataFile(self, location):
  
!         return location
  
  
      def GetSize(self, location):
          
          return os.stat(self.GetDataFile(location))[6]
--- 275,288 ----
          return s
  
  
      def GetDataFile(self, location):
  
!         if root is not None:
!             return os.path.join(root, location)
!         else:
!             return location
  
  
      def GetSize(self, location):
          
          return os.stat(self.GetDataFile(location))[6]
*************** class TemporaryAttachmentStore(FileAttac
*** 318,338 ****
      def __init__(self):
          """Construct a temporary attachment store.
  
          The store is initially empty."""
  
-         # Initialize the base class.
-         super(TemporaryAttachmentStore, self).__init__()
          # Construct a temporary directory in which to store attachment
          # data.
          self.__tmpdir = temporary_directory.TemporaryDirectory()
!         self.__path = self.__tmpdir.GetPath()
! 
! 
!     def GetDataFile(self, location):
! 
!         return os.path.join(self.__path, location)
  
  
      def HandleUploadRequest(self, request):
          """Handle a web request to upload attachment data.
  
--- 326,341 ----
      def __init__(self):
          """Construct a temporary attachment store.
  
          The store is initially empty."""
  
          # Construct a temporary directory in which to store attachment
          # data.
          self.__tmpdir = temporary_directory.TemporaryDirectory()
!         # Initialize the base class.
!         path = self.__tmpdir.GetPath()
!         super(TemporaryAttachmentStore, self).__init__(path)
  
  
      def HandleUploadRequest(self, request):
          """Handle a web request to upload attachment data.
  
Index: qm/test/classes/xml_database.py
===================================================================
RCS file: /home/sc/Repository/qm/qm/test/classes/xml_database.py,v
retrieving revision 1.13
diff -c -5 -p -r1.13 xml_database.py
*** qm/test/classes/xml_database.py	3 Jul 2003 16:52:40 -0000	1.13
--- qm/test/classes/xml_database.py	15 Jul 2003 18:45:55 -0000
*************** class XMLDatabase(ExtensionDatabase):
*** 51,61 ****
      def __init__(self, path, arguments):
  
          # Initialize base classes.
          ExtensionDatabase.__init__(self, path, arguments)
          # Create an AttachmentStore for this database.
!         self.__store = qm.attachment.FileAttachmentStore()
  
  
      def _GetTestFromPath(self, test_id, test_path):
          try:
              return self.__LoadItem(test_id, test_path,
--- 51,61 ----
      def __init__(self, path, arguments):
  
          # Initialize base classes.
          ExtensionDatabase.__init__(self, path, arguments)
          # Create an AttachmentStore for this database.
!         self.__store = qm.attachment.FileAttachmentStore(path)
  
  
      def _GetTestFromPath(self, test_id, test_path):
          try:
              return self.__LoadItem(test_id, test_path,

From njs at pobox.com  Tue Jul 15 23:51:46 2003
From: njs at pobox.com (Nathaniel Smith)
Date: Tue, 15 Jul 2003 16:51:46 -0700
Subject: [PATCH] add --debug switch
Message-ID: <20030715235146.GA15206@njs.dhis.org>

Little patch to add a --debug switch, allowing one to run QMTest under
pdb.

-- Nathaniel

-- 
"If you can explain how you do something, then you're very very bad at it."
  -- John Hopfield
-------------- next part --------------
diff -urN --exclude='*~' --exclude='.*' --exclude=CVS --exclude='*.pyo' --exclude='*.pyc' --exclude=build --exclude=GNUmakefile --exclude=config.log --exclude=config.status --exclude=setup_path.py --exclude=qm.sh --exclude=qm.spec qm-clean/ChangeLog qm-debug/ChangeLog
--- qm-clean/ChangeLog	2003-07-15 15:45:35.000000000 -0700
+++ qm-debug/ChangeLog	2003-07-15 16:45:14.000000000 -0700
@@ -1,3 +1,10 @@
+2003-07-15  Nathaniel Smith  <njs at codesourcery.com>
+
+	* qm/test/cmdline.py (QMTest.debug_option_spec): New --debug
+	option.
+	(QMTest.global_options_spec): Add --debug option.
+	(QMTest.Execute): Check for --debug option.
+
 2003-07-15  Vladimir Prus <ghost at cs.msu.su>
 	    Mark Mitchell <mark at codesourcery.com>
 
diff -urN --exclude='*~' --exclude='.*' --exclude=CVS --exclude='*.pyo' --exclude='*.pyc' --exclude=build --exclude=GNUmakefile --exclude=config.log --exclude=config.status --exclude=setup_path.py --exclude=qm.sh --exclude=qm.spec qm-clean/qm/test/cmdline.py qm-debug/qm/test/cmdline.py
--- qm-clean/qm/test/cmdline.py	2003-07-03 12:28:22.000000000 -0700
+++ qm-debug/qm/test/cmdline.py	2003-07-15 16:25:35.000000000 -0700
@@ -139,6 +139,13 @@
         "Path to the test database."
         )
 
+    debug_option_spec = (
+        None,
+        "debug",
+        None,
+        "Start QMTest in a debugger"
+        )
+
     extension_output_option_spec = (
         "o",
         "output",
@@ -304,6 +311,7 @@
         verbose_option_spec,
         version_option_spec,
         db_path_option_spec,
+        debug_option_spec,
         ]
 
     commands_spec = [
@@ -655,6 +663,12 @@
 
         error_occurred = 0
         
+        # If the user requested a debugger, start it now, immediately
+        # before their command is executed.
+        if self.HasGlobalOption("debug"):
+            import pdb
+            pdb.set_trace()
+
         # Dispatch to the appropriate method.
         if self.__command == "create-tdb":
             self.__ExecuteCreateTdb(db_path)

From mark at codesourcery.com  Wed Jul 16 00:02:06 2003
From: mark at codesourcery.com (Mark Mitchell)
Date: 15 Jul 2003 17:02:06 -0700
Subject: [qmtest] [PATCH] add --debug switch
In-Reply-To: <20030715235146.GA15206@njs.dhis.org>
References: <20030715235146.GA15206@njs.dhis.org>
Message-ID: <1058313726.15765.133.camel@minax.codesourcery.com>

On Tue, 2003-07-15 at 16:51, Nathaniel Smith wrote:
> Little patch to add a --debug switch, allowing one to run QMTest under
> pdb.

This patch is fine, but it needs a corresponding update to the reference
manual to document the new command-line option.

+        "Start QMTest in a debugger"

You're missing a period at the end of that.  And I think "the debugger"
would be marginally better than "a debugger"; it's not like the user
gets to pick which debugger.

Thanks,

-- 
Mark Mitchell <mark at codesourcery.com>
CodeSourcery, LLC



From ghost at cs.msu.su  Wed Jul 16 04:51:03 2003
From: ghost at cs.msu.su (Vladimir Prus)
Date: Wed, 16 Jul 2003 08:51:03 +0400
Subject: [qmtest] [PATCH] yet another directory handling problem
In-Reply-To: <1058292602.15765.58.camel@minax.codesourcery.com>
References: <200307141157.47273.ghost@cs.msu.su> <1058292602.15765.58.camel@minax.codesourcery.com>
Message-ID: <200307160851.04028.ghost@cs.msu.su>

Mark Mitchell wrote:

> > BTW, I wonder if you have any automated tests for QMTest GUI itself?
>
> We don't have any such tests.
>
> For a while now, I've been wanting to write a test class which would at
> least visit particular URLs and post forms; the test would pass if
> QMTest did not crash after running through the URLs and forms.  That
> would not validate the appearance in the GUI, but it would at least
> prevent the kinds of things you've been fixing with your recent patches.

I've being thinking about just the same: some class which load request data 
from a file and sends it to a server. Such data probably can be captured by 
writing tiny proxy server using Python HTTP server class, which proxy will 
record all communication. Too bad I can't find the time to try this idea 
right now.

- Volodya




From ghost at cs.msu.su  Wed Jul 16 04:55:20 2003
From: ghost at cs.msu.su (Vladimir Prus)
Date: Wed, 16 Jul 2003 08:55:20 +0400
Subject: [qmtest] [PATCH] fix "save expectation" command
In-Reply-To: <1058292363.15765.55.camel@minax.codesourcery.com>
References: <200307150933.03302.ghost@cs.msu.su> <1058292363.15765.55.camel@minax.codesourcery.com>
Message-ID: <200307160855.20983.ghost@cs.msu.su>

Mark Mitchell wrote:
> On Mon, 2003-07-14 at 22:33, Vladimir Prus wrote:
> > The command is broken because since 2003-05-09 the Result.__init__ method
> > does not have "context" parameter, while
> > QMTestSever.HandleSaveExpectations tries to pass it.
>
> I applied this patch.

Oh... the "help" command in interpreter did not tell that os.path.join handles 
absolute paths correctly, so I checked for absolute paths explicitly.

BTW, could you tell why QMTest uses:

    super(TemporaryAttachmentStore, self).__init__(path)

instead of 

    FileAttachmentStore.__init__(self, path)

Is it to handle the case when superclass changes? I must admit this is the 
first time I see "super" builtin function.

- Volodya



From mark at codesourcery.com  Wed Jul 16 05:39:21 2003
From: mark at codesourcery.com (Mark Mitchell)
Date: 15 Jul 2003 22:39:21 -0700
Subject: [qmtest] [PATCH] fix "save expectation" command
In-Reply-To: <200307160855.20983.ghost@cs.msu.su>
References: <200307150933.03302.ghost@cs.msu.su>
	 <1058292363.15765.55.camel@minax.codesourcery.com>
	 <200307160855.20983.ghost@cs.msu.su>
Message-ID: <1058333961.3520.1.camel@minax.codesourcery.com>

On Tue, 2003-07-15 at 21:55, Vladimir Prus wrote:
> Mark Mitchell wrote:
> > On Mon, 2003-07-14 at 22:33, Vladimir Prus wrote:
> > > The command is broken because since 2003-05-09 the Result.__init__ method
> > > does not have "context" parameter, while
> > > QMTestSever.HandleSaveExpectations tries to pass it.
> >
> > I applied this patch.
> 
> BTW, could you tell why QMTest uses:
> 
>     super(TemporaryAttachmentStore, self).__init__(path)
> 
> instead of 
> 
>     FileAttachmentStore.__init__(self, path)

This is a new feature in Python that, as you say, avoids having to
explicitly name the super class.  It would be even nicer if you could
just say "super(self).__init__(path)", but you can't, and there are some
technical reasons for that.

It also makes the call look more like an ordinary method call; it's more
like self.foo(x), rather than Class.foo(self, x).

Yours, 

-- 
Mark Mitchell <mark at codesourcery.com>
CodeSourcery, LLC



From njs at pobox.com  Wed Jul 16 07:17:48 2003
From: njs at pobox.com (Nathaniel Smith)
Date: Wed, 16 Jul 2003 00:17:48 -0700
Subject: [qmtest] [PATCH] fix "save expectation" command
In-Reply-To: <1058333961.3520.1.camel@minax.codesourcery.com>
References: <200307150933.03302.ghost@cs.msu.su> <1058292363.15765.55.camel@minax.codesourcery.com> <200307160855.20983.ghost@cs.msu.su> <1058333961.3520.1.camel@minax.codesourcery.com>
Message-ID: <20030716071748.GA16734@njs.dhis.org>

On Tue, Jul 15, 2003 at 10:39:21PM -0700, Mark Mitchell wrote:
> On Tue, 2003-07-15 at 21:55, Vladimir Prus wrote:
> > Mark Mitchell wrote:
> > > On Mon, 2003-07-14 at 22:33, Vladimir Prus wrote:
> > > > The command is broken because since 2003-05-09 the Result.__init__ method
> > > > does not have "context" parameter, while
> > > > QMTestSever.HandleSaveExpectations tries to pass it.
> > >
> > > I applied this patch.
> > 
> > BTW, could you tell why QMTest uses:
> > 
> >     super(TemporaryAttachmentStore, self).__init__(path)
> > 
> > instead of 
> > 
> >     FileAttachmentStore.__init__(self, path)
> 
> This is a new feature in Python that, as you say, avoids having to
> explicitly name the super class.  It would be even nicer if you could
> just say "super(self).__init__(path)", but you can't, and there are some
> technical reasons for that.

It also works correctly in the case of multiple inheritance, which
wasn't really possible to get right before.
http://www.python.org/2.2.1/descrintro.html is probably the best
current documentation on this stuff. 

You could actually use a little metaclass to make sugar like
self.__super.__init__(path) work, but perhaps that's getting a bit
overkill.

-- Nathaniel

-- 
"...these, like all words, have single, decontextualized meanings: everyone
knows what each of these words means, everyone knows what constitutes an
instance of each of their referents.  Language is fixed.  Meaning is
certain.  Santa Claus comes down the chimney at midnight on December 24."
  -- The Language War, Robin Lakoff


From ghost at cs.msu.su  Thu Jul 17 10:38:51 2003
From: ghost at cs.msu.su (Vladimir Prus)
Date: Thu, 17 Jul 2003 14:38:51 +0400
Subject: [PATCH/2] "reload extensions" command
Message-ID: <200307171438.51686.ghost@cs.msu.su>


Hello,
some time ago we've discussed "realod extension" command, which would reload 
all extension modules in QMTest, and will help in cases where I edit custom 
test class and don't want to restart entire QMTest GUI.

I think we've agreed such command will be usefully, so here's a patch. It's 
not 100% complete, because:

- it does not document the new command
- I could have missed something

I'd appreciate feedback on this patch, and if it's considered OK, I'll tweak 
documentation, too.

Changelog:
* qm/test/base.py
   (__loaded_extensions): New variable.
   (get_extension_class_from_directory): Update __loaded_extensions
   (reload_extension_module): New function.

* qm/test/share/dtml/navigation-bar.dtml
  (reload_extensions): New function

* qm/test/web/web.py
   (QMTestPage.__init__): Add "Reload extension" item to the "File" menu.
   (QMTestServer.__init__): Register "reload-extensions" script.
   (QMTestServer.HandleReloadExtensions): New method.

Patch attached.

TIA,
Volodya



-------------- next part --------------
A non-text attachment was scrubbed...
Name: reload_extensions.diff
Type: text/x-diff
Size: 4967 bytes
Desc: not available
URL: <http://sourcerytools.com/pipermail/qmtest/attachments/20030717/93ca1982/attachment.diff>

From ghost at cs.msu.su  Thu Jul 17 12:01:04 2003
From: ghost at cs.msu.su (Vladimir Prus)
Date: Thu, 17 Jul 2003 16:01:04 +0400
Subject: [PATCH] fix attachments
Message-ID: <200307171601.04224.ghost@cs.msu.su>


There wes a typo.

Changelog

* qm/attachment.py 
   (FileAttachmentStore.GetDataFile): Fix typo.

Patch attached.

- Volodya
-------------- next part --------------
A non-text attachment was scrubbed...
Name: attachment.diff
Type: text/x-diff
Size: 579 bytes
Desc: not available
URL: <http://sourcerytools.com/pipermail/qmtest/attachments/20030717/27c1506d/attachment.diff>

From mark at codesourcery.com  Mon Jul 21 20:42:02 2003
From: mark at codesourcery.com (Mark Mitchell)
Date: 21 Jul 2003 13:42:02 -0700
Subject: [qmtest] [PATCH] fix attachments
In-Reply-To: <200307171601.04224.ghost@cs.msu.su>
References: <200307171601.04224.ghost@cs.msu.su>
Message-ID: <1058820123.19997.24.camel@doubledemon.codesourcery.com>

On Thu, 2003-07-17 at 05:01, Vladimir Prus wrote:
> 
> There wes a typo.

Indeed.

I bit the bullet and created a test case to make sure that we have
*some* basic testing for attachments in our testsuite.

Thanks for the patch!

-- 
Mark Mitchell
CodeSourcery, LLC
mark at codesourcery.com
-------------- next part --------------
2003-07-21  Vladimir Prus <ghost at cs.msu.su>
	    Mark Mitchell <mark at codesourcery.com>

	* qm/attachment.py (FileAttachmentStore.GetDataFile): Fix typo.
	* tests/regress/attachment1/attachment1: New file.
	* tests/regress/attachment1/results.qmr: Likewise.
	* tests/regress/attachment1/test.qmt: Likewise.
	* tests/regress/attachment1/QMTest/.cvsignore: Likewise.
	* tests/regress/attachment1/QMTest/attachment_test.py: Likewise.
	* tests/regress/attachment1/QMTest/classes.qmc: Likewise.

Index: qm/attachment.py
===================================================================
RCS file: /home/sc/Repository/qm/qm/attachment.py,v
retrieving revision 1.19
diff -c -5 -p -r1.19 attachment.py
*** qm/attachment.py	15 Jul 2003 18:48:21 -0000	1.19
--- qm/attachment.py	21 Jul 2003 19:35:36 -0000
*************** class FileAttachmentStore(AttachmentStor
*** 275,286 ****
          return s
  
  
      def GetDataFile(self, location):
  
!         if root is not None:
!             return os.path.join(root, location)
          else:
              return location
  
  
      def GetSize(self, location):
--- 275,286 ----
          return s
  
  
      def GetDataFile(self, location):
  
!         if self.__root is not None:
!             return os.path.join(self.__root, location)
          else:
              return location
  
  
      def GetSize(self, location):
Index: tests/regress/attachment1/attachment1
===================================================================
RCS file: tests/regress/attachment1/attachment1
diff -N tests/regress/attachment1/attachment1
*** /dev/null	1 Jan 1970 00:00:00 -0000
--- tests/regress/attachment1/attachment1	21 Jul 2003 19:35:36 -0000
***************
*** 0 ****
--- 1 ----
+ The quick brown fox jumped over the lazy dog.
Index: tests/regress/attachment1/results.qmr
===================================================================
RCS file: tests/regress/attachment1/results.qmr
diff -N tests/regress/attachment1/results.qmr
Binary files /dev/null and results.qmr differ
Index: tests/regress/attachment1/test1.qmt
===================================================================
RCS file: tests/regress/attachment1/test1.qmt
diff -N tests/regress/attachment1/test1.qmt
*** /dev/null	1 Jan 1970 00:00:00 -0000
--- tests/regress/attachment1/test1.qmt	21 Jul 2003 19:35:36 -0000
***************
*** 0 ****
--- 1,2 ----
+ <?xml version="1.0" ?>
+ <extension class="attachment_test.AttachmentTest" kind="test"><argument name="prerequisites"><set/></argument><argument name="target_group"><text>.*</text></argument><argument name="attachment"><attachment><description>attachment1</description><mime-type>application/octet-stream</mime-type><filename>attachment1</filename><location>attachment1</location></attachment></argument><argument name="resources"><set/></argument></extension>
Index: tests/regress/attachment1/QMTest/.cvsignore
===================================================================
RCS file: tests/regress/attachment1/QMTest/.cvsignore
diff -N tests/regress/attachment1/QMTest/.cvsignore
*** /dev/null	1 Jan 1970 00:00:00 -0000
--- tests/regress/attachment1/QMTest/.cvsignore	21 Jul 2003 19:35:36 -0000
***************
*** 0 ****
--- 1,3 ----
+ *.pyc
+ *.pyo
+ 
Index: tests/regress/attachment1/QMTest/attachment_test.py
===================================================================
RCS file: tests/regress/attachment1/QMTest/attachment_test.py
diff -N tests/regress/attachment1/QMTest/attachment_test.py
*** /dev/null	1 Jan 1970 00:00:00 -0000
--- tests/regress/attachment1/QMTest/attachment_test.py	21 Jul 2003 19:35:36 -0000
***************
*** 0 ****
--- 1,43 ----
+ ########################################################################
+ #
+ # File:   attachment_test.py
+ # Author: Mark Mitchell
+ # Date:   2003-07-21
+ #
+ # Contents:
+ #   Test classes for tests written in Python.
+ #
+ # Copyright (c) 2003 by CodeSourcery, LLC.  All rights reserved. 
+ #
+ # For license terms see the file COPYING.
+ #
+ ########################################################################
+ 
+ ########################################################################
+ # Imports
+ ########################################################################
+ 
+ import qm.fields
+ from   qm.test.test import Test
+ 
+ ########################################################################
+ # Classes
+ ########################################################################
+ 
+ class AttachmentTest(Test):
+     """An 'AttachmentTest' has a single attachment.
+ 
+     This test class is used to validate QMTest's attachment processing."""
+     
+     arguments = [
+         qm.fields.AttachmentField(
+             name="attachment"
+             )
+         ]
+ 
+ 
+     def Run(self, context, result):
+ 
+         if (self.attachment.GetData()
+             != "The quick brown fox jumped over the lazy dog.\n"):
+             result.Fail("Incorrect attachment contents.")
Index: tests/regress/attachment1/QMTest/classes.qmc
===================================================================
RCS file: tests/regress/attachment1/QMTest/classes.qmc
diff -N tests/regress/attachment1/QMTest/classes.qmc
*** /dev/null	1 Jan 1970 00:00:00 -0000
--- tests/regress/attachment1/QMTest/classes.qmc	21 Jul 2003 19:35:36 -0000
***************
*** 0 ****
--- 1,2 ----
+ <?xml version="1.0" ?>
+ <class-directory><class kind="test">attachment_test.AttachmentTest</class></class-directory>
\ No newline at end of file
Index: tests/regress/attachment1/QMTest/configuration
===================================================================
RCS file: tests/regress/attachment1/QMTest/configuration
diff -N tests/regress/attachment1/QMTest/configuration
*** /dev/null	1 Jan 1970 00:00:00 -0000
--- tests/regress/attachment1/QMTest/configuration	21 Jul 2003 19:35:36 -0000
***************
*** 0 ****
--- 1,2 ----
+ <?xml version="1.0" ?>
+ <extension class="xml_database.XMLDatabase" kind="database"/>
\ No newline at end of file

From ghost at cs.msu.su  Tue Jul 22 08:25:26 2003
From: ghost at cs.msu.su (Vladimir Prus)
Date: Tue, 22 Jul 2003 12:25:26 +0400
Subject: [qmtest] [PATCH] fix attachments
In-Reply-To: <1058820123.19997.24.camel@doubledemon.codesourcery.com>
References: <200307171601.04224.ghost@cs.msu.su> <1058820123.19997.24.camel@doubledemon.codesourcery.com>
Message-ID: <200307221225.26781.ghost@cs.msu.su>

Mark Mitchell wrote:
> On Thu, 2003-07-17 at 05:01, Vladimir Prus wrote:
> > There wes a typo.
>
> Indeed.
>
> I bit the bullet and created a test case to make sure that we have
> *some* basic testing for attachments in our testsuite.

That's great. I've took the time to figure out how QMTest self-tests work, so 
I might try submitting regression tests with future patches, if any.

Thanks,
Volodya



From mark at codesourcery.com  Tue Jul 22 17:28:18 2003
From: mark at codesourcery.com (Mark Mitchell)
Date: 22 Jul 2003 10:28:18 -0700
Subject: PATCH: Robustify prerequisites
Message-ID: <1058894898.19922.156.camel@doubledemon.codesourcery.com>

Nathaniel found a GUI bug last night, and that sent me down a bit of a
rabbit hole finding other little places where we could be more robust. 
(For example, we would try to create a "<select>" tag with no contained
"<option>" tags, which is not valid HTML.)

I also added a test case for the tuple bug that Nathaniel found last
night.

-- 
Mark Mitchell
CodeSourcery, LLC
mark at codesourcery.com
-------------- next part --------------
A non-text attachment was scrubbed...
Name: diffs
Type: text/x-patch
Size: 21374 bytes
Desc: not available
URL: <http://sourcerytools.com/pipermail/qmtest/attachments/20030722/7161bec6/attachment.bin>

From ghost at cs.msu.su  Thu Jul 24 11:51:16 2003
From: ghost at cs.msu.su (Vladimir Prus)
Date: Thu, 24 Jul 2003 15:51:16 +0400
Subject: Results GUI suggestion
Message-ID: <200307241551.16967.ghost@cs.msu.su>


I'd like to propose a minor enhanmenent to the GUI. Say I've just run 80 tests 
(which is indeed so). One has failed, I've fixed the test and rerun it. Good, 
but I'm shown the list of resutls, starting with the first test. So, to 
quickly find if that one tests failed again or not, I have to click 7 times 
on the nagivation arrow.

It's it possible to either:
1. Add new results to the front of results list, or
2. Show the last 10 results by default, not first.

I think the latter idea is better. Now, URL like 

    http://127.0.0.1:54980/test/show-results?start=71

causes a certain part of results to be shown. It's possible to make

    http://127.0.0.1:54980/test/show-results?start=-1

show last 10 results. Then, the "run test" command will redirect to the latter 
URL.

Is this reasonable?

- Volodya



From faught at tejasconsulting.com  Thu Jul 24 15:39:42 2003
From: faught at tejasconsulting.com (Danny Faught)
Date: Thu, 24 Jul 2003 10:39:42 -0500
Subject: test in tutorial fails on Windows
References: <200307241551.16967.ghost@cs.msu.su>
Message-ID: <3F1FFDBE.2070903@tejasconsulting.com>

I just started looking at QMTest 2.0.3 in earnest in order to use it as 
an example in a tutorial I'm giving at a conference.  So far I'm very
impressed.

I'm going through the tutorial on Windows 2000 now, and I'm getting 
stuck.  On the page at 
http://www.codesourcery.com/qm/qmtest_downloads/qm-2.0.3/manual.html/sec-testtut-modifying.html, 
  it says "Click on the Home link to return to the main QMTest page."  I 
haven't found a Home link on the web interface.

In any case, it's not too hard to get to a page where I can do File->New 
Test.  I create a test that tries to do "echo test", but the result when 
I run it is an error: "pywintypes.api_error: (2, 'CreateProcess', 'The 
system cannot find the file specified.')"  It appears that echo is a DOS 
builtin rather than a separate utility as on Unix.

Here's the source for command.test1:

<?xml version="1.0" ?>
<extension class="command.ExecTest" kind="test"><argument 
name="environment"><set/></argument><argument 
name="program"><text>echo</text></argument><argument 
name="resources"><set/></argument><argument 
name="arguments"><set><text>test</text>
</set></argument><argument name="stderr"><text/></argument><argument 
name="stdout"><text>test
</text></argument><argument 
name="prerequisites"><set/></argument><argument name
="stdin"><text/></argument><argument 
name="exit_code"><integer>0</integer></argument><argument 
name="target_group"><text>.*</text></argument></extension>

It works better using the command.ShellCommandTest class instead.  In 
that case, the test runs, but I can't get it to match the output -

-------------
command.test2
Outcome Cause
FAIL Unexpected standard output.
Annotation Value
ExecTest.expected_stdout

test

ExecTest.stdout

test

qmtest.target local
--------------

There is a carriage return in the expected output, like the tutorial says:

<?xml version="1.0" ?>
<extension class="command.ShellCommandTest" kind="test"><argument 
name="environment"><set/></argument><argument 
name="target_group"><text>.*</text></argument><argument 
name="command"><text>echo test</text></argument><argument name="stderr">
<text/></argument><argument name="stdout"><text>test
</text></argument><argument 
name="prerequisites"><set/></argument><argument 
name="stdin"><text/></argument><argument 
name="exit_code"><integer>0</integer></argument><argument 
name="resources"><set/></argument></extension>

Any pointers would be appreciated.  I'm using ActiveState Python 2.2.2.
-- 
Danny R. Faught
Tejas Software Consulting
publisher of Open Testware Reviews -
   http://tejasconsulting.com/open-testware/



From mark at codesourcery.com  Fri Jul 25 19:30:08 2003
From: mark at codesourcery.com (Mark Mitchell)
Date: 25 Jul 2003 12:30:08 -0700
Subject: [qmtest] Results GUI suggestion
In-Reply-To: <200307241551.16967.ghost@cs.msu.su>
References: <200307241551.16967.ghost@cs.msu.su>
Message-ID: <1059161415.1548.41.camel@doubledemon.codesourcery.com>

On Thu, 2003-07-24 at 04:51, Vladimir Prus wrote:
> 
> I'd like to propose a minor enhanmenent to the GUI. Say I've just run 80 tests 
> (which is indeed so). One has failed, I've fixed the test and rerun it. Good, 
> but I'm shown the list of resutls, starting with the first test. So, to 
> quickly find if that one tests failed again or not, I have to click 7 times 
> on the nagivation arrow.
> 
> It's it possible to either:
> 1. Add new results to the front of results list, or
> 2. Show the last 10 results by default, not first.
> 
> I think the latter idea is better. Now, URL like 
> 
>     http://127.0.0.1:54980/test/show-results?start=71
> 
> causes a certain part of results to be shown. It's possible to make
> 
>     http://127.0.0.1:54980/test/show-results?start=-1
> 
> show last 10 results. Then, the "run test" command will redirect to the latter 
> URL.
> 
> Is this reasonable?

I think it's great that you've identified this usability issue.
 
How about giving each result a number, and then having clicking on the
title of that column flip the order?  In other words:

  #     Result ID        Outcome
  =     =========        =======
  1     foo              PASS
  2     bar              FAIL

If you click on the "#", it would flip to:

  #     Result ID        Outcome
  =     =========        =======
  2     bar              FAIL
  1     foo              PASS

Would that work well for you?

-- 
Mark Mitchell
CodeSourcery, LLC
mark at codesourcery.com



From njs at pobox.com  Mon Jul 28 23:18:22 2003
From: njs at pobox.com (Nathaniel Smith)
Date: Mon, 28 Jul 2003 16:18:22 -0700
Subject: [PATCH] New scheduler.
Message-ID: <20030728231822.GA16482@njs.dhis.org>

QMTest's current test scheduler is not very scalable at all.  This
scheduler aims to be much more scalable, and also more correct (it
should be able, for example, to properly handle the case where a test
is not runnable on any available target).  I still need to add some
more explanation and fix up the test suite (currently
regressions.cycle5 is giving a spurious failure, and also I need to
add some more tests), but this is a big enough change I wouldn't mind
some comments first.

Ignore the profiler bit, that's just for testing.

-- Nathaniel

-- 
.i dei jitfa fanmo xatra

This email may be read aloud.
-------------- next part --------------
diff -urN --exclude='*~' --exclude='.*' --exclude=CVS --exclude='*.pyo' --exclude='*.pyc' --exclude=build --exclude=GNUmakefile --exclude=config.log --exclude=config.status --exclude=setup_path.py --exclude=qm.sh --exclude=qmtest --exclude=qm.spec --exclude='*.dtd' --exclude=CATALOG --exclude=thread_target --exclude=process_target --exclude='*.qmr' qm-clean/ChangeLog qm-efficient-scheduling/ChangeLog
--- qm-clean/ChangeLog	2003-07-24 16:54:20.000000000 -0700
+++ qm-efficient-scheduling/ChangeLog	2003-07-26 04:57:00.000000000 -0700
@@ -1,3 +1,7 @@
+2003-07-26  Nathaniel Smith  <njs at codesourcery.com>
+
+	* qm/test/execution_engine.py: Rewrite scheduling logic.
+
 2003-07-24  Nathaniel Smith  <njs at codesourcery.com>
 
 	* GNUmakefile.in (RELLIBDIR): Don't add slashes to prefix when
diff -urN --exclude='*~' --exclude='.*' --exclude=CVS --exclude='*.pyo' --exclude='*.pyc' --exclude=build --exclude=GNUmakefile --exclude=config.log --exclude=config.status --exclude=setup_path.py --exclude=qm.sh --exclude=qmtest --exclude=qm.spec --exclude='*.dtd' --exclude=CATALOG --exclude=thread_target --exclude=process_target --exclude='*.qmr' qm-clean/qm/test/cmdline.py qm-efficient-scheduling/qm/test/cmdline.py
--- qm-clean/qm/test/cmdline.py	2003-07-24 14:12:32.000000000 -0700
+++ qm-efficient-scheduling/qm/test/cmdline.py	2003-07-28 13:29:09.000000000 -0700
@@ -1397,7 +1397,14 @@
         engine = ExecutionEngine(database, test_ids, context, targets,
                                  result_streams,
                                  self.__GetExpectedOutcomes())
-        return engine.Run()
+        if os.environ.has_key("QM_PROFILE"):
+            import hotshot
+            profiler = hotshot.Profile(os.environ["QM_PROFILE"])
+            retval = profiler.runcall(engine.Run)
+            profiler.close()
+            return retval
+        else:
+            return engine.Run()
                                                     
 
     def __ExecuteServer(self):
diff -urN --exclude='*~' --exclude='.*' --exclude=CVS --exclude='*.pyo' --exclude='*.pyc' --exclude=build --exclude=GNUmakefile --exclude=config.log --exclude=config.status --exclude=setup_path.py --exclude=qm.sh --exclude=qmtest --exclude=qm.spec --exclude='*.dtd' --exclude=CATALOG --exclude=thread_target --exclude=process_target --exclude='*.qmr' qm-clean/qm/test/execution_engine.py qm-efficient-scheduling/qm/test/execution_engine.py
--- qm-clean/qm/test/execution_engine.py	2003-07-03 12:28:22.000000000 -0700
+++ qm-efficient-scheduling/qm/test/execution_engine.py	2003-07-28 12:40:22.000000000 -0700
@@ -22,6 +22,7 @@
 import qm.queue
 from   qm.test.base import *
 import qm.test.cmdline
+import qm.test.database
 from   qm.test.context import *
 import qm.xmlutil
 from   result import *
@@ -33,19 +34,113 @@
 # Classes
 ########################################################################
 
+class _TestStatus(object):
+    """A '_TestStatus' object tracks the status of a test during a run.
+    """
+
+    # Implementation note: A 2-slot object takes less memory than the
+    # equivalent 2-element list or 2-element tuple (checked on Python
+    # 2.2.3), and provides a convenient place to package various bits of
+    # code.
+
+    __slots__ = "status", "parents"
+    # 'self.status' describes the current state of the test -- either a
+    # status indicator or a result outcome.
+    # 'self.parents' is either 'None', or a list of tests that have this
+    # test as a prerequisite.
+
+    CONSIDERING = "status_Considering"
+    RUNNABLE = "status_Runnable"
+
+    def __init__(self):
+
+        self.status = None
+        self.parents = None
+
+
+    # A test enters "Considering" state as soon as it has been pulled
+    # from the user's queue.  A test in this state will eventually run,
+    # even if it is never pulled from the queue again.  This is mainly
+    # used to discard already-seen tests when pulling them from the
+    # queue.
+    def MarkConsidering(self):
+
+        assert self.status is None
+        self.status = self.CONSIDERING
+
+
+    def Considering(self):
+
+        return self.status is self.CONSIDERING or self.Runnable()
+
+
+    # A tests enters "Runnable" state as soon as it gets put on the
+    # runnable queue.  A test in this state will run as soon as a free
+    # target opens up for it.  This is used to detect whether a test
+    # should still receive callbacks when a prerequisite finishes; if it
+    # has already become runnable, then it does not need to continue
+    # receiving callbacks.
+    def MarkRunnable(self):
+
+        assert self.status is self.CONSIDERING
+        self.status = self.RUNNABLE
+
+
+    def Runnable(self):
+
+        return self.status is self.RUNNABLE or self.Finished()
+
+
+    # A finished test is just that -- all done.  Any tests that had it
+    # as a prerequisite can now check their expected outcome against the
+    # real outcome.
+    def SetOutcome(self, outcome):
+
+        self.status = outcome
+
+
+    def Finished(self):
+
+        return self.status in Result.outcomes
+
+
+    def GetOutcome(self):
+        """Only valid to call this if 'Finished()' returns true."""
+
+        return self.status
+
+
+    def AddParent(self, parent_id):
+
+        if self.parents is None:
+            self.parents = []
+        self.parents.append(parent_id)
+
+
+    def ConsumeParents(self):
+
+        parents = self.parents
+        del self.parents
+        if parents is None:
+            return ()
+        else:
+            return parents
+
+
+
 class ExecutionEngine:
     """A 'ExecutionEngine' executes tests.
 
     A 'ExecutionEngine' object handles the execution of a collection
     of tests.
 
-    This class schedules the tests, plus the setup and cleanup of any
-    resources they require, across one or more targets.
+    This class schedules the tests across one or more targets.
 
     The shedule is determined dynamically as the tests are executed
     based on which targets are idle and which are not.  Therefore, the
     testing load should be reasonably well balanced, even across a
     heterogeneous network of testing machines."""
+
     
     def __init__(self,
                  database,
@@ -61,7 +156,7 @@
         
         'test_ids' -- A sequence of IDs of tests to run.  Where
         possible, the tests are started in the order specified.
-
+        
         'context' -- The context object to use when running tests.
 
         'targets' -- A sequence of 'Target' objects, representing
@@ -91,17 +186,13 @@
         
         # All of the targets are idle at first.
         self.__idle_targets = targets[:]
+        # And we haven't yet found any idle targets with nothing to do.
+        self.__stuck_targets = []
         # There are no responses from the targets yet.
         self.__response_queue = qm.queue.Queue(0)
         # There no pending or ready tests yet.
-        self.__pending = []
-        self.__ready = []
         self.__running = 0
 
-        # The descriptor graph has not yet been created.
-        self.__descriptors = {}
-        self.__descriptor_graph = {}
-        
         self.__any_unexpected_outcomes = 0
         
         # Termination has not yet been requested.
@@ -121,7 +212,7 @@
     def IsTerminationRequested(self):
         """Returns true if termination has been requested.
 
-        return -- True if Terminate has been called."""
+        returns -- True if Terminate has been called."""
 
         return self.__terminated
     
@@ -180,143 +271,425 @@
 
         self.__input_handlers[fd] = function
         
-    
+
     def _RunTests(self):
-        """Run all of the tests.
 
-        This function assumes that the targets have already been
-        started.
+        num_tests = len(self.__test_ids)
+
+        # No tests have been started yet.
+        self.__num_tests_started = 0
+
+        self.__tests_iterator = iter(self.__test_ids)
 
-        The tests are run in the order that they were presented --
-        modulo requirements regarding prerequisites and any
-        nondeterminism introduced by running tests in parallel."""
-
-        # Create a directed graph where each node is a pair
-        # (count, descriptor).  There is an edge from one node
-        # to another if the first node is a prerequisite for the
-        # second.  Begin by creating the nodes of the graph.
+        # A big table of all the tests we are to run, to track status
+        # information (and also to allow quick lookup of whether a
+        # listed prerequisite should actually be run).
+        self.__statuses = {}
         for id in self.__test_ids:
-            try:
-                descriptor = self.__database.GetTest(id)
-                self.__descriptors[id] = descriptor
-                self.__descriptor_graph[descriptor] = [0, []]
-                self.__pending.append(descriptor)
-            except:
-                result = Result(Result.TEST, id)
-                result.NoteException(cause = "Could not load test.",
-                                     outcome = Result.UNTESTED)
-                self._AddResult(result)
-                
-        # Create the edges.
-        for descriptor in self.__pending:
-            prereqs = descriptor.GetPrerequisites()
-            if prereqs:
-                for (prereq_id, outcome) in prereqs.items():
-                    if not self.__descriptors.has_key(prereq_id):
-                        # The prerequisite is not amongst the list of
-                        # tests to run.  In that case we do still run
-                        # the dependent test; it was explicitly
-                        # requested by the user.
-                        continue
-                    prereq_desc = self.__descriptors[prereq_id]
-                    self.__descriptor_graph[prereq_desc][1] \
-                        .append((descriptor, outcome))
-                    self.__descriptor_graph[descriptor][0] += 1
-
-            if not self.__descriptor_graph[descriptor][0]:
-                # A node with no prerequisites is ready.
-                self.__ready.append(descriptor)
-
-        # Iterate until there are no more tests to run.
-        while ((self.__pending or self.__ready)
-               and not self.IsTerminationRequested()):
-            # If there are no idle targets, block until we get a
-            # response.  There is nothing constructive we can do.
-            idle_targets = self.__idle_targets
-            if not idle_targets:
+            self.__statuses[id] = _TestStatus()
+
+        # The stack of tests whose prerequisites we are trying to
+        # satisfy.
+        self.__prereq_stack = []
+        # The same thing as a dict, for doing loop detection.
+        self.__ids_on_stack = {}
+
+        # No tests are currently runnable.  This is a dictionary indexed
+        # by target group.  Each element maps to a list of runnable
+        # tests in that target group.
+        self.__runnable = {}
+
+        while self.__num_tests_started < num_tests:
+            # Sweep through and clear any responses that have come
+            # back; this also updates the idle target list.
+            while self._CheckForResponse(wait=0):
+                pass
+
+            # Now look for idle targets.
+            if not self.__idle_targets:
+                if len(self.__stuck_targets) == len(self.__targets):
+                    # All targets are stuck.  This means that last time
+                    # through the loop they were all idle and no work
+                    # could be assigned to them; furthermore, no
+                    # prospect of further work has come in since then,
+                    # or they would have been moved back to the merely
+                    # idle list.  Therefore, any tests currently listed
+                    # as runnable have invalid targets and should be
+                    # marked UNTESTED.
+                    self._Trace("All targets stuck"
+                                " -- clearing runnable queue.")
+                    self._ClearAllRunnableTests("No matching target")
+                    continue
+                # Otherwise, we just need to block until there's work to
+                # do.
                 self._Trace("All targets are busy -- waiting.")
-                # Read a reply from the response_queue.
                 self._CheckForResponse(wait=1)
                 self._Trace("Response received.")
-                # Keep going.
+                # Found one; start over in hopes things will be better
+                # this time.
                 continue
 
-            # If there are no tests ready to run, but no tests are
-            # actually running at this time, we have
-            # a cycle in the dependency graph.  Pull the head off the
-            # pending queue and mark it UNTESTED, see if that helps.
-            if (not self.__ready and not self.__running):
-                descriptor = self.__pending[0]
-                self._Trace(("Dependency cycle, discarding %s."
-                             % descriptor.GetId()))
-                self.__pending.remove(descriptor)
-                self._AddUntestedResult(descriptor.GetId(),
+            # We are careful to only keep idle targets that we could
+            # find tests for, and to loop around again after feeding
+            # each target exactly one test.  If were to instead to give
+            # each target as many tests as it could take until it
+            # became idle, we would have problems in the serial case,
+            # because the target would never become idle, and we would
+            # never loop around to clear out the 'Result's queue.
+            new_idle = []
+            while self.__idle_targets:
+                target = self.__idle_targets.pop()
+                if not self._RunATestOn(target):
+                    # We couldn't find a test; rather than loop
+                    # around in circles, ignore this target until we
+                    # get some more runnable tests.
+                    self.__stuck_targets.append(target)
+                elif target.IsIdle():
+                    # Target is still idle.
+                    new_idle.append(target)
+            self.__idle_targets = new_idle
+
+        # Now all tests have been started; we just have wait for them
+        # all to finish.
+        while self.__running:
+            self._CheckForResponse(wait=1)
+
+
+    def _MakeRunnable(self, descriptor):
+        """Adds a test to the runnable queue."""
+
+        test_id = descriptor.GetId()
+        self._Trace("Test '%s' has become runnable." % (test_id,))
+        self.__statuses[test_id].MarkRunnable()
+
+        # Previously stuck targets may have something to do now, so
+        # unstick them all.
+        if self.__stuck_targets:
+            self.__idle_targets += self.__stuck_targets
+            self.__stuck_targets = []
+
+        target_group = descriptor.GetTargetGroup()
+        try:
+            stack = self.__runnable[target_group]
+        except KeyError:
+            stack = self.__runnable[target_group] = []
+        stack.append(test_id)
+
+
+    def _RunATestOn(self, target):
+        """Tries to run a test on the given target.
+
+        returns -- true on success, false on failure.  Failure means
+        that no test was found that could be run on this target."""
+
+        self._Trace("Looking for a test for target %s"
+                    % target.GetName())
+
+        for target_group, tests in self.__runnable.iteritems():
+            if target.IsInGroup(target_group) and tests:
+                test_id = tests.pop()
+                self._Trace("About to run '%s'." % (test_id,))
+                assert self.__statuses[test_id].Runnable()
+                try:
+                    descriptor = self.__database.GetTest(test_id)
+                except qm.test.database.NoSuchTestError:
+                    self._Trace("But it couldn't be loaded!")
+                    self._AddLoadErrorResult(test_id)
+                else:
+                    self.__num_tests_started += 1
+                    self.__running += 1
+                    target.RunTest(descriptor, self.__context)
+                return 1
+
+        # No already runnable tests.  Try to get new ones.
+        while 1:
+            descriptor = self._FindRunnableTest()
+            if descriptor is None:
+                # We're out of runnable tests altogether.
+                return 0
+            elif target.IsInGroup(descriptor.GetTargetGroup()):
+                # We found a test we can run.
+                id = descriptor.GetId()
+                self._Trace("Found runnable test '%s'" % (id,))
+                self.__num_tests_started += 1
+                self.__running += 1
+                target.RunTest(descriptor, self.__context)
+                return 1
+            else:
+                # We found a test that we can't run, but someone else
+                # can; put it on the queue for them and try again.
+                self._MakeRunnable(descriptor)
+
+
+    class _ConsumedTest(Exception):
+        """Thrown when a test is consumed instead of processed normally.
+
+        Generally this happens because an error was detected during said
+        processing, and the test was thrown out marked UNTESTED."""
+        
+        pass
+
+
+    def _FindRunnableTest(self):
+        """Attempt find at least one runnable test.
+
+        This will only return tests that are ready to be run
+        immediately, though as a side effect it will set up tests to
+        become automatically runnable later when the prerequisites they
+        depend on finish running.  All tests become runnable in one of
+        these two ways, and therefore this can be considered the core of
+        the scheduling algorithm.  It is only here that tests are pulled
+        from the user-provided list, and here is also where we detect
+        cycles.
+
+        returns -- the descriptor of the new runnable test, or 'None' if
+        no such test could be found."""
+
+        while 1:
+            if not self.__prereq_stack:
+                # We ran out of prerequisite tests, so pull a new one
+                # off the user's list.
+                try:
+                    test_id = self.__tests_iterator.next()
+                except StopIteration:
+                    # We're entirely out of fresh tests; give up.
+                    return None
+                if self.__statuses[test_id].Considering():
+                    # This test has already been handled (probably
+                    # because it's a prereq of a test already seen).
+                    continue
+                # We have a fresh test.
+                try:
+                    self._AddTestToStack(test_id)
+                except self._ConsumedTest:
+                    # Or maybe not.  Skip it and go on to the next.
+                    continue
+                self._Trace("Added new test %s to root of stack" % (test_id,))
+
+            descriptor, prereqs = self.__prereq_stack[-1]
+            # The prereqs list only mentions tests that still need to be
+            # run.
+            if prereqs:
+                # Pick a prereq and start over with it.
+                try:
+                    self._AddTestToStack(prereqs.pop())
+                    continue
+                except self._ConsumedTest:
+                    # If it ran immediately, start over with the same
+                    # one again.
+                    continue
+            else:
+                # This test is ready to come off the stack.
+                # Physically remove it from the stack.
+                test_id = descriptor.GetId()
+                del self.__ids_on_stack[test_id]
+                self.__prereq_stack.pop()
+
+                # Now, either it's ready to run, or needs to wait for
+                # some prereqs to finish.
+                try:
+                    waiting_on = self._IsWaitingFor(descriptor)
+                except self._ConsumedTest:
+                    # ... or else, it just disappears, so we try again.
+                    continue
+                
+                if not waiting_on:
+                    # We've finally found our runnable test.
+                    return descriptor
+                else:
+                    # This test will be runnable once we hear back from
+                    # the tests it depends on.  Ask them to notify it.
+                    for child_id in waiting_on:
+                        self.__statuses[child_id].AddParent(test_id)
+                    # But we still need our immediately runnable test,
+                    # so try again.
+                    continue
+
+            # Should never get here.
+            assert 0
+
+
+    def _AddTestToStack(self, test_id):
+        """Adds 'test_id' to the stack of current tests.
+
+        Updates the test status, sets up cycle detection, and suchlike.
+        May consume the passed in test; if so, will throw a
+        '_ConsumedTest' exception."""
+        
+        self._Trace("Trying to add %s to stack" % test_id)
+
+        # Update test status.
+        self.__statuses[test_id].MarkConsidering()
+
+        # Load the descriptor.
+        try:
+            descriptor = self.__database.GetTest(test_id)
+        except:
+            self._AddLoadErrorResult(test_id)
+            raise self._ConsumedTest, test_id, sys.exc_info()[2]
+
+        # Check for cycles.
+
+        # One might think that the way to detect a cycle was to check
+        # whether the test being added to the stack was already on the
+        # stack.  But in fact this doesn't work, because we have other
+        # logic to ensure that any test is only added to the stack once
+        # over the entire run.  This avoids extra work, but it thwarts
+        # the obvious cycle-detection check.  So instead we explicitly
+        # check the stack for all prerequisites of this test.
+        for prereq_id in descriptor.GetPrerequisites():
+            if prereq_id in self.__ids_on_stack:
+                self._Trace("Cycle detected (%s)" % (prereq_id,))
+                self._AddUntestedResult(test_id,
                                         qm.message("dependency cycle"))
-                self._UpdateDependentTests(descriptor, Result.UNTESTED)
+                raise self._ConsumedTest, "cycle detected"
+        self.__ids_on_stack[test_id] = None
+
+        # Finally, calculate the "interesting" prerequisites (the ones
+        # that we actually need to run).
+        def is_relevant(prereq):
+            return self.__statuses.has_key(prereq) \
+                   and not self.__statuses[prereq].Considering()
+        prereqs_iter = iter(descriptor.GetPrerequisites())
+        relevant_prereqs = filter(is_relevant, prereqs_iter)
+
+        # And store it all in the stack.
+        self.__prereq_stack.append((descriptor, relevant_prereqs))
+
+        
+    def _IsWaitingFor(self, test_descriptor):
+        """Finds the prerequisites 'test_descriptor' is waiting on.
+
+        Returns a list of id's of tests that need to complete before
+        'test_descriptor' can be run.  If any known outcomes are
+        violated, consumes the test and raises a '_ConsumedTest'
+        exception."""
+
+        id = test_descriptor.GetId()
+        needed = []
+
+        prereqs = test_descriptor.GetPrerequisites()
+        for prereq_id, outcome in prereqs.iteritems():
+            try:
+                prereq_status = self.__statuses[prereq_id]
+            except KeyError:
+                # This prerequisite is not being run at all.
                 continue
 
-            # There is at least one idle target.  Try to find something
-            # that it can do.
-            wait = 1
-            for descriptor in self.__ready:
-                for target in idle_targets:
-                    if target.IsInGroup(descriptor.GetTargetGroup()):
-                        # This test can be run on this target.  Remove
-                        # it from the ready list.
-                        self.__ready.remove(descriptor)
-                        # And from the pending list.
-                        try:
-                            self.__pending.remove(descriptor)
-                        except ValueError:
-                            # If the test is not pending, that means it
-                            # got pulled off for some reason
-                            # (e.g. breaking dependency cycles).  Don't
-                            # try to run it, it won't work.
-                            self._Trace(("Ready test %s not pending, skipped"
-                                         % descriptor.GetId()))
-                            wait = 0
-                            break
-
-                        # Output a trace message.
-                        self._Trace(("About to run %s."
-                                     % descriptor.GetId()))
-                        # Run it.
-                        self.__running += 1
-                        target.RunTest(descriptor, self.__context)
-                        # If the target is no longer idle, remove it
-                        # from the idle_targets list.
-                        if not target.IsIdle():
-                            self._Trace("Target is no longer idle.")
-                            self.__idle_targets.remove(target)
-                        else:
-                            self._Trace("Target is still idle.")
-                        # We have done something useful on this
-                        # iteration.
-                        wait = 0
-                        break
+            if prereq_status.Finished():
+                prereq_outcome = prereq_status.GetOutcome()
+                if outcome != prereq_outcome:
+                    # Failed prerequisite.
+                    self._AddUntestedResult \
+                        (id,
+                         qm.message("failed prerequisite"),
+                         {'qmtest.prequisite': prereq_id,
+                          'qmtest.outcome': prereq_outcome,
+                          'qmtest.expected_outcome': outcome })
+                    raise self._ConsumedTest
+                else:
+                    # Passed prerequisite, do nothing.
+                    pass
+            else:
+                # Unfinished prerequisite, make a note.
+                needed.append(prereq_id)
+        return needed
 
-                if not wait:
+
+    def _PrerequisiteFinishedCallback(self, test_id):
+        """Check 'test_id's prerequisites, and do the right thing.
+
+        This function is called whenever a test may have become
+        runnable, because a prerequisite's result became available.  It
+        is only called if we are not already runnable.
+
+        The "right thing" means if any prequisites fail, emit an
+        UNTESTED result; otherwise, if any prerequisites have unknown
+        result, do nothing; otherwise, add this test to the runnable
+        queue."""
+
+        self._Trace("%s had a prerequisite finish" % (test_id,))
+
+        try:
+            descriptor = self.__database.GetTest(test_id)
+        except:
+            self._AddLoadErrorResult(test_id)
+            return
+
+        try:
+            waiting_for = self._IsWaitingFor(descriptor)
+        except self._ConsumedTest:
+            return
+
+        if not waiting_for:
+            # All prerequisites ran and were satisfied.  This test can
+            # now run.
+            self._MakeRunnable(descriptor)
+            
+
+    def _HandleResult(self, result):
+        """Do processing associated with a new result.
+
+        'result' -- A 'Result' object representing the result of running
+        a test or resource."""
+
+        # Output a trace message.
+        id = result.GetId()
+        self._Trace("Recording %s result for %s." % (result.GetKind(), id))
+
+        # Find the target with the name indicated in the result.
+        if result.has_key(Result.TARGET):
+            for target in self.__targets:
+                if target.GetName() == result[Result.TARGET]:
                     break
+            else:
+                assert 0, ("No target %s exists (test id: %s)"
+                           % (result[Result.TARGET], id))
+        else:
+            # Not all results will have associated targets.  If the
+            # test was not run at all, there will be no associated
+            # target.
+            target = None
 
-            # Output a trace message.
-            self._Trace("About to check for a response in %s mode."
-                        % ((wait and "blocking") or "nonblocking"))
-                    
-            # See if any targets have finished their assignments.  If
-            # we did not schedule any additional work during this
-            # iteration of the loop, there's no point in continuing
-            # until some target finishes what it's doing.
-            self._CheckForResponse(wait=wait)
+        # Having no target is a rare occurrence; output a trace message.
+        if not target:
+            self._Trace("No target for %s." % result.GetId())
 
+        # This target might now be idle.
+        if (target
+            and target not in self.__idle_targets
+            and target not in self.__stuck_targets
+            and target.IsIdle()):
             # Output a trace message.
-            self._Trace("Done checking for responses.")
+            self._Trace("Target is now idle.\n")
+            self.__idle_targets.append(target)
+            
+        # For now, only tests have expectations or scheduling
+        # dependencies, so this next bit only applies to tests:
+        if result.GetKind() == Result.TEST:
+            # We now know this test's outcome, so record it in our global
+            # status dictionary.
+            test_status = self.__statuses[id]
+            test_status.SetOutcome(result.GetOutcome())
+
+            # And then poke all the tests that might have become runnable.
+            parents = test_status.ConsumeParents()
+            for parent_id in parents:
+                if not self.__statuses[parent_id].Runnable():
+                    self._PrerequisiteFinishedCallback(parent_id)
+
+            # Check for unexpected outcomes.
+            if result.GetKind() == Result.TEST:
+                if (self.__expectations.get(id, Result.PASS)
+                    != result.GetOutcome()):
+                    self.__any_unexpected_outcomes = 1
+            
+        # Output a trace message.
+        self._Trace("Writing result for %s to streams." % id)
 
-        # Any tests that are still pending are untested, unless there
-        # has been an explicit request that we exit immediately.
-        if not self.IsTerminationRequested():
-            for descriptor in self.__pending:
-                self._AddUntestedResult(descriptor.GetId(),
-                                        qm.message("execution terminated"))
+        # Report the result.
+        for rs in self.__result_streams:
+            rs.WriteResult(result)
 
 
     def _CheckForResponse(self, wait):
@@ -336,19 +709,12 @@
                 self._Trace("Got %s result for %s from queue."
                              % (result.GetKind(), result.GetId()))
                 # Handle it.
-                self._AddResult(result)
+                self._HandleResult(result)
                 if result.GetKind() == Result.TEST:
                     assert self.__running > 0
                     self.__running -= 1
                 # Output a trace message.
                 self._Trace("Recorded result.")
-                # If this was a test result, there may be other tests that
-                # are now eligible to run.
-                if result.GetKind() == Result.TEST:
-                    # Get the descriptor for this test.
-                    descriptor = self.__descriptors[result.GetId()]
-                    # Iterate through each of the dependent tests.
-                    self._UpdateDependentTests(descriptor, result.GetOutcome())
                 return result
             except qm.queue.Empty:
                 # If there is nothing in the queue, then this exception will
@@ -371,102 +737,20 @@
                 continue
 
 
-    def _UpdateDependentTests(self, descriptor, outcome):
-        """Update the status of tests that depend on 'node'.
+    ### Various methods to signal errors with particular tests.
 
-        'descriptor' -- A test descriptor.
 
-        'outcome' -- The outcome associated with the test.
+    def _AddErrorResult(self, result):
+        """All error results should be noted with this method.
 
-        If tests that depend on 'descriptor' required a particular
-        outcome, and 'outcome' is different, mark them as untested.  If
-        tests that depend on 'descriptor' are now eligible to run, add
-        them to the '__ready' queue."""
-
-        node = self.__descriptor_graph[descriptor]
-        for (d, o) in node[1]:
-            # Find the node for the dependent test.
-            n = self.__descriptor_graph[d]
-            # If some other prerequisite has already had an undesired
-            # outcome, there is nothing more to do.
-            if n[0] == 0:
-                continue
+        Error results are those that indicate that a test was not run.
+        This is important to keep an accurate count of how many tests
+        are left to run, and to ensure test status's are updated
+        correctly."""
 
-            # If the actual outcome is not the outcome that was
-            # expected, the dependent test cannot be run.
-            if outcome != o:
-                try:
-                    # This test will never be run.
-                    n[0] = 0
-                    self.__pending.remove(d)
-                    # Mark it untested.
-                    self._AddUntestedResult(d.GetId(),
-                                            qm.message("failed prerequisite"),
-                                            { 'qmtest.prequisite' :
-                                              descriptor.GetId(),
-                                              'qmtest.outcome' : outcome,
-                                              'qmtest.expected_outcome' : o })
-                    # Recursively remove tests that depend on d.
-                    self._UpdateDependentTests(d, Result.UNTESTED)
-                except ValueError:
-                    # This test has already been taken off the pending queue;
-                    # assume a result has already been recorded.  This can
-                    # happen when we're breaking dependency cycles.
-                    pass
-            else:
-                # Decrease the count associated with the node, if
-                # the test has not already been declared a failure.
-                n[0] -= 1
-                # If this was the last prerequisite, this test
-                # is now ready.
-                if n[0] == 0:
-                    self.__ready.append(d)
-                    
-    
-    def _AddResult(self, result):
-        """Report the result of running a test or resource.
-
-        'result' -- A 'Result' object representing the result of running
-        a test or resource."""
-
-        # Output a trace message.
-        self._Trace("Recording %s result for %s."
-                    % (result.GetKind(), result.GetId()))
-
-        # Find the target with the name indicated in the result.
-        if result.has_key(Result.TARGET):
-            for target in self.__targets:
-                if target.GetName() == result[Result.TARGET]:
-                    break
-        else:
-            # Not all results will have associated targets.  If the
-            # test was not run at all, there will be no associated
-            # target.
-            target = None
-
-        # Having no target is a rare occurrence; output a trace message.
-        if not target:
-            self._Trace("No target for %s." % result.GetId())
-                        
-        # Check for unexpected outcomes.
-        if result.GetKind() == Result.TEST  \
-           and (self.__expectations.get(result.GetId(), Result.PASS)
-                != result.GetOutcome()):
-            self.__any_unexpected_outcomes = 1
-            
-        # This target might now be idle.
-        if (target and target not in self.__idle_targets
-            and target.IsIdle()):
-            # Output a trace message.
-            self._Trace("Target is now idle.\n")
-            self.__idle_targets.append(target)
-
-        # Output a trace message.
-        self._Trace("Writing result for %s to streams." % result.GetId())
-
-        # Report the result.
-        for rs in self.__result_streams:
-            rs.WriteResult(result)
+        self.__num_tests_started += 1
+        self.__statuses[result.GetId()].MarkRunnable()
+        self._HandleResult(result)
 
 
     def _AddUntestedResult(self, test_name, cause, annotations={}):
@@ -482,7 +766,34 @@
         # Create the result.
         result = Result(Result.TEST, test_name, Result.UNTESTED, annotations)
         result[Result.CAUSE] = cause
-        self._AddResult(result)
+        self._AddErrorResult(result)
+
+
+    def _AddLoadErrorResult(self, test_id):
+        """Add a 'Result' indicating that loading 'test_id' failed.
+
+        Should be called from the catch block that caught the error, as
+        'Result.NoteException' is called."""
+
+        result = Result(Result.TEST, test_id)
+        result.NoteException(cause = "Could not load test.",
+                             outcome = Result.UNTESTED)
+        self._AddErrorResult(result)
+        
+
+    def _ClearAllRunnableTests(self, cause):
+        """Marks all currently runnable tests as UNTESTED.
+
+        This is called when it is detected that all tests currently in
+        the runnable queue can never by run, generally because their
+        target specification does not match any available target."""
+
+        for runnable_stack in self.__runnable.itervalues():
+            while runnable_stack:
+                self._AddUntestedResult(runnable_stack.pop(), cause)
+
+
+    ### Utility methods.
 
 
     def _Trace(self, message):

From njs at pobox.com  Tue Jul 29 11:18:03 2003
From: njs at pobox.com (Nathaniel Smith)
Date: Tue, 29 Jul 2003 04:18:03 -0700
Subject: Revised version of scheduler patch
Message-ID: <20030729111803.GA25976@njs.dhis.org>

Attached.  Once again, ignore the profile stuff, but otherwise, this
is a checkin candidate.  Passes all tests, including some new ones
that just hang the old scheduler.  Updated with a few of Mark's
comments from the first patch, plus some tweaks/bugfixes, plus a big
comment that I'd appreciate feedback on (especially -- does it make
any sense? -- but other comments welcome too).

Only caveat: I'm a little worried that this slows down overall test
throughput by as much as a factor of 2 -- some numbers suggest that --
but on the other hand, if your test runs are taking more than five
minutes or so, either you're doing some real work and scheduler
throughput isn't an issue, or you need the scalability fixes... (and
some profiling suggests that about even with near-trivial tests only
half the run time is actually spent in the scheduler, so speedup
beyond that isn't possible without changing other parts of QMTest
anyway).  I think the old scheduler may have just been extremely
efficient when there were no prerequisites to deal with and memory was
plentiful.)

-- Nathaniel

-- 
"But in Middle-earth, the distinct accusative case disappeared from
the speech of the Noldor (such things happen when you are busy
fighting Orcs, Balrogs, and Dragons)."
-------------- next part --------------
diff -urN --exclude='*~' --exclude='.*' --exclude=CVS --exclude='*.pyo' --exclude='*.pyc' --exclude=build --exclude=GNUmakefile --exclude=config.log --exclude=config.status --exclude=setup_path.py --exclude=qm.sh --exclude=qmtest --exclude=qm.spec --exclude='*.dtd' --exclude=CATALOG --exclude=thread_target --exclude=process_target --exclude='*.qmr' qm-clean/ChangeLog qm-efficient-scheduling/ChangeLog
--- qm-clean/ChangeLog	2003-07-24 16:54:20.000000000 -0700
+++ qm-efficient-scheduling/ChangeLog	2003-07-26 04:57:00.000000000 -0700
@@ -1,3 +1,7 @@
+2003-07-26  Nathaniel Smith  <njs at codesourcery.com>
+
+	* qm/test/execution_engine.py: Rewrite scheduling logic.
+
 2003-07-24  Nathaniel Smith  <njs at codesourcery.com>
 
 	* GNUmakefile.in (RELLIBDIR): Don't add slashes to prefix when
diff -urN --exclude='*~' --exclude='.*' --exclude=CVS --exclude='*.pyo' --exclude='*.pyc' --exclude=build --exclude=GNUmakefile --exclude=config.log --exclude=config.status --exclude=setup_path.py --exclude=qm.sh --exclude=qmtest --exclude=qm.spec --exclude='*.dtd' --exclude=CATALOG --exclude=thread_target --exclude=process_target --exclude='*.qmr' qm-clean/qm/test/cmdline.py qm-efficient-scheduling/qm/test/cmdline.py
--- qm-clean/qm/test/cmdline.py	2003-07-24 14:12:32.000000000 -0700
+++ qm-efficient-scheduling/qm/test/cmdline.py	2003-07-28 13:29:09.000000000 -0700
@@ -1397,7 +1397,14 @@
         engine = ExecutionEngine(database, test_ids, context, targets,
                                  result_streams,
                                  self.__GetExpectedOutcomes())
-        return engine.Run()
+        if os.environ.has_key("QM_PROFILE"):
+            import hotshot
+            profiler = hotshot.Profile(os.environ["QM_PROFILE"])
+            retval = profiler.runcall(engine.Run)
+            profiler.close()
+            return retval
+        else:
+            return engine.Run()
                                                     
 
     def __ExecuteServer(self):
diff -urN --exclude='*~' --exclude='.*' --exclude=CVS --exclude='*.pyo' --exclude='*.pyc' --exclude=build --exclude=GNUmakefile --exclude=config.log --exclude=config.status --exclude=setup_path.py --exclude=qm.sh --exclude=qmtest --exclude=qm.spec --exclude='*.dtd' --exclude=CATALOG --exclude=thread_target --exclude=process_target --exclude='*.qmr' qm-clean/qm/test/execution_engine.py qm-efficient-scheduling/qm/test/execution_engine.py
--- qm-clean/qm/test/execution_engine.py	2003-07-03 12:28:22.000000000 -0700
+++ qm-efficient-scheduling/qm/test/execution_engine.py	2003-07-29 03:59:27.000000000 -0700
@@ -22,6 +22,7 @@
 import qm.queue
 from   qm.test.base import *
 import qm.test.cmdline
+import qm.test.database
 from   qm.test.context import *
 import qm.xmlutil
 from   result import *
@@ -33,20 +34,268 @@
 # Classes
 ########################################################################
 
+class _TestStatus(object):
+    """A '_TestStatus' object tracks the status of a test during a run.
+    """
+
+    # Implementation note: A 2-slot object takes less memory than the
+    # equivalent 2-element list or 2-element tuple (checked on Python
+    # 2.2.3), and provides a convenient place to package various bits of
+    # code.
+
+    __slots__ = "status", "parents"
+    # 'self.status' describes the current state of the test -- either a
+    # status indicator or a result outcome.
+    # 'self.parents' is either 'None', or a list of tests that have this
+    # test as a prerequisite.
+
+    CONSIDERING = "status_Considering"
+    RUNNABLE = "status_Runnable"
+
+    def __init__(self):
+
+        self.status = None
+        self.parents = None
+
+
+    # A test enters "Considering" state as soon as it has been pulled
+    # from the user's queue.  A test in this state will eventually run,
+    # even if it is never pulled from the queue again.  This is mainly
+    # used to discard already-seen tests when pulling them from the
+    # queue.
+    def MarkConsidering(self):
+
+        assert self.status is None
+        self.status = self.CONSIDERING
+
+
+    def IsConsidering(self):
+
+        return self.status == self.CONSIDERING or self.IsRunnable()
+
+
+    # A tests enters "Runnable" state as soon as it gets put on the
+    # runnable queue.  A test in this state will run as soon as a free
+    # target opens up for it.  This is used to detect whether a test
+    # should still receive callbacks when a prerequisite finishes; if it
+    # has already become runnable, then it does not need to continue
+    # receiving callbacks.
+    def MarkRunnable(self):
+
+        # We may call 'MarkRunnable' twice, e.g. if there is an error
+        # executing a runnable test.
+        assert self.status in (self.CONSIDERING, self.RUNNABLE)
+        self.status = self.RUNNABLE
+
+
+    def IsRunnable(self):
+
+        return self.status == self.RUNNABLE or self.IsFinished()
+
+
+    # A finished test is just that -- all done.  Any tests that had it
+    # as a prerequisite can now check their expected outcome against the
+    # real outcome.
+    def SetOutcome(self, outcome):
+
+        self.status = outcome
+
+
+    def IsFinished(self):
+
+        return self.status in Result.outcomes
+
+
+    def GetOutcome(self):
+        """Only valid to call this if 'IsFinished()' returns true."""
+
+        return self.status
+
+
+    def AddParent(self, parent_id):
+
+        if self.parents is None:
+            self.parents = []
+        self.parents.append(parent_id)
+
+
+    def ConsumeParents(self):
+
+        parents = self.parents
+        del self.parents
+        if parents is None:
+            return ()
+        else:
+            return parents
+
+
+
 class ExecutionEngine:
     """A 'ExecutionEngine' executes tests.
 
     A 'ExecutionEngine' object handles the execution of a collection
     of tests.
 
-    This class schedules the tests, plus the setup and cleanup of any
-    resources they require, across one or more targets.
+    This class schedules the tests across one or more targets.
 
     The shedule is determined dynamically as the tests are executed
     based on which targets are idle and which are not.  Therefore, the
     testing load should be reasonably well balanced, even across a
     heterogeneous network of testing machines."""
+
     
+    # Implementation:
+    #
+    # This computation is trickier than it might at first appear, when
+    # one takes into account the need to maintain parallelism, properly
+    # handle prerequisites, match tests with targets they can run on,
+    # avoid busy-waiting, and properly catch errors like tests with
+    # unfulfillable target requirements, tests that fail to load from
+    # the database, and (the worst) tests with cyclic dependencies.  And
+    # all this when one is required to scale effectively to large
+    # numbers of tests, and attempt to maintain the user's specified
+    # order when possible.
+    #
+    # We currently don't do a terribly good job of maintaining the
+    # user's order, especially when tests have different target
+    # requirements or when running tests with prerequisites in parallel,
+    # but do handle the others quite well.  This is how it works.
+    #
+    # The basic idea is:
+    #    -- Gather up a list of idle targets that might be able to run a
+    #       test.
+    #    -- For each one, try to find a test that can be run on it.
+    #        -- To find a test, first check the explicit list of
+    #           runnable tests, and try to find one that matches this
+    #           target.
+    #        -- If
+    #
+    # The overall flow of tests through the system looks like this:
+    # 
+    #      Listed prereqs -+ b                    e
+    #                       \          c * Limbo --* Runnable queue
+    #                        *          /             *    |
+    #  User's list ---* Prereq stack --+           f /     |
+    #               a                   \           /      | g
+    #                                  d * Maybe run       |
+    #                                               \ h    |
+    #                                                *     *
+    #                                              Actually run
+    #                                                   |
+    #                                                   * i
+    #                                         Finish run, get outcome
+    # *) pretend *'s are arrowheads.
+    #
+    # Unfortunately, the control flow doesn't look like this at all.
+    # '_RunTests' repeatedly finds some idle targets that might able to
+    # run a test, and uses '_RunATestOn' to attempt to run a test on
+    # each target.  '_RunATestOn' first looks in the runnable queue for
+    # any tests that are runnable on its target.  If it finds any, it
+    # immediately runs them (arc g).  If it doesn't, then it calls
+    # '_FindRunnableTest', which churns through tests until it finds a
+    # test that might be runnable (though not necessarily on the current
+    # target), then returns it to '_RunATestOn' (arc d).  '_RunATestOn'
+    # then checks to see whether the test is runnable on its target (the
+    # "Maybe run" decision).  If it is, then it runs it immediately (arc
+    # h); otherwise, it puts it on the runnable queue for some other
+    # invocation of '_RunATestOn' to pick up (arc f).
+    #
+    # '_FindRunnableTest's declared interface is that it returns a test
+    # that can be run immediately (given an appropriate target); but it
+    # also has side-effects.  Essentially, its job is to pump the left
+    # half of the chart until a test that is immediately runnable is
+    # found.  The main thing this involves is manipulating the "prereq
+    # stack".  The prereq stack is a stack of tests in the process of
+    # having their prerequisites fulfilled.  If this stack is empty,
+    # '_FindRunnableTest' first pulls a test off the user's list and
+    # puts it on the stack.  Thus assured of a non-empty stack, it then
+    # repeatedly examines the top element of the stack to see if it has
+    # any prerequisites that are not yet percolating through the right
+    # half of the diagram.  If it does, it picks one, puts it on top of
+    # the stack (arc b), and repeats.  (Basically, this is a depth-first
+    # search through prerequisites that haven't already been started.)
+    # If it reaches a test all of whose prerequisites have been
+    # satisfied, it returns it (arc d).  If it reaches a test all of
+    # whose prerequisites have been seen and are on their to being run,
+    # then it can't run it immediately, but neither can it keep it on
+    # the stack, because its presence would block further progress.  So
+    # what it does is ask each of the unfinished prerequisites to make a
+    # callback when finished, and throws the test out into limbo (arc
+    # c).  Then it repeats, because it still needs to find a test for
+    # '_RunATestOn' to try.
+    #
+    # Between calling '_RunATestOn', '_RunTests' calls
+    # '_CheckForResponse', which looks for incoming 'Result's, and when
+    # it finds them hands them to '_HandleResult'.  '_HandleResult'
+    # takes care of the minutiae of recording the result (arc i).  It
+    # then goes through all tests registered as callbacks on the test
+    # that it just received a 'Result' for, and for each calls
+    # '_PrerequisiteFinishedCallback'.  The idea here is that all the
+    # tests that got stuck in Limbo because they had the just-finished
+    # test as a prerequisite may be able to run, so we go through each
+    # one and check if it has become runnable.
+    # '_PrerequisiteFinishedCallback' does just this -- rechecks the
+    # relevant test's prerequisites, and if they are no satisfied, adds
+    # the test to the runnable queue (arc e).
+
+    # For understanding the status table, it is useful to know that
+    # 'IsConsidering' becomes true when a test passes arcs a or b;
+    # 'IsRunnable' becomes true when a test passes arcs d or e.
+
+    # Details ignored in the above description include the many places
+    # where errors can occur.  The usual response to an error involving
+    # some test is to immediately add an ERROR or UNTESTED result and
+    # abort further processing; this complicates things, because we have
+    # to check at various places that the test we're working on hasn't
+    # already been run behind our backs.
+
+    # One of the trickier classes of errors to catch are cyclic
+    # dependency chains.  These are checked for by '_FindRunnableTest';
+    # it checks that it never adds a test to the stack if it is already
+    # on the stack.  Since every test on the stack depends on the next
+    # higher test on the stack, such a check will clearly never report a
+    # cyclic dependency when none exists.  Less clear is that it will
+    # catch all cyclic dependencies, but in fact it will.  Define a test
+    # as "eventually runnable" if it is runnable, or if all its
+    # immediate prerequisites are themselves eventually runnable,
+    # recursively.  An eventually runnable test cannot participate in a
+    # cycle, and '_FindRunnableTest' removes a test from the stack iff
+    # it is eventually runnable.
+
+    # It is also useful to note the distinction between an "idle target"
+    # and a "stuck target"; either has the capacity to run tests, but a
+    # stuck target is one for which there are currently no tests to
+    # run.  Distinguishing between the two is important both for
+    # avoiding busy-waiting when all targets are stuck, and for
+    # detecting tests whose target group is invalid.
+
+    # In making this code fast and scalable, it is important to minimize
+    # the number of times 'self.__database.GetTest' is called; loading a
+    # test descriptor is quite slow (and test descriptors take
+    # non-trivial memory).  The current code is careful to ensure that
+    # on the "fast path", when a test takes neither arc c nor arc f
+    # above, the descriptor is loaded only once; if arc f is taken, the
+    # descriptor is loaded only twice.  This seems a good compromise
+    # between speed and memory use; the number of descriptors loaded
+    # into memory at once is bounded by the length of the longest
+    # dependency chain.  The potentially slowest part of the code is the
+    # path that takes arc c; since this can only happen to tests with
+    # prerequisites or odd target descriptions, and only when running in
+    # parallel, and not all the time even then, this seems acceptable.
+
+    # Possibilities for further improvement/weaknesses in current code:
+    #   -- The arc c path could perhaps be made more efficient.
+    #   -- Probably doesn't scale well to truly large numbers of
+    #      processors yet (though it's not clear how large is large,
+    #      here).
+    #   -- Could do a better job of running tests in the requested
+    #      order, for example by doing clever priority queue things with
+    #      the runnable queue.  Or just by actually using queues,
+    #      instead of stacks.  Right now tests put on the runnable queue
+    #      have their order actively scrambled.
+    #   -- Has no knowledge of resources, so it can't try to assign
+    #      tests to targets that already have a given resource set up.
+
     def __init__(self,
                  database,
                  test_ids,
@@ -61,7 +310,7 @@
         
         'test_ids' -- A sequence of IDs of tests to run.  Where
         possible, the tests are started in the order specified.
-
+        
         'context' -- The context object to use when running tests.
 
         'targets' -- A sequence of 'Target' objects, representing
@@ -91,17 +340,13 @@
         
         # All of the targets are idle at first.
         self.__idle_targets = targets[:]
+        # And we haven't yet found any idle targets with nothing to do.
+        self.__stuck_targets = []
         # There are no responses from the targets yet.
         self.__response_queue = qm.queue.Queue(0)
         # There no pending or ready tests yet.
-        self.__pending = []
-        self.__ready = []
         self.__running = 0
 
-        # The descriptor graph has not yet been created.
-        self.__descriptors = {}
-        self.__descriptor_graph = {}
-        
         self.__any_unexpected_outcomes = 0
         
         # Termination has not yet been requested.
@@ -121,7 +366,7 @@
     def IsTerminationRequested(self):
         """Returns true if termination has been requested.
 
-        return -- True if Terminate has been called."""
+        returns -- True if Terminate has been called."""
 
         return self.__terminated
     
@@ -180,143 +425,438 @@
 
         self.__input_handlers[fd] = function
         
-    
+
     def _RunTests(self):
-        """Run all of the tests.
 
-        This function assumes that the targets have already been
-        started.
+        num_tests = len(self.__test_ids)
 
-        The tests are run in the order that they were presented --
-        modulo requirements regarding prerequisites and any
-        nondeterminism introduced by running tests in parallel."""
-
-        # Create a directed graph where each node is a pair
-        # (count, descriptor).  There is an edge from one node
-        # to another if the first node is a prerequisite for the
-        # second.  Begin by creating the nodes of the graph.
+        # No tests have been started yet.
+        self.__num_tests_started = 0
+
+        self.__tests_iterator = iter(self.__test_ids)
+
+        # A big table of all the tests we are to run, to track status
+        # information (and also to allow quick lookup of whether a
+        # listed prerequisite should actually be run).
+        self.__statuses = {}
         for id in self.__test_ids:
-            try:
-                descriptor = self.__database.GetTest(id)
-                self.__descriptors[id] = descriptor
-                self.__descriptor_graph[descriptor] = [0, []]
-                self.__pending.append(descriptor)
-            except:
-                result = Result(Result.TEST, id)
-                result.NoteException(cause = "Could not load test.",
-                                     outcome = Result.UNTESTED)
-                self._AddResult(result)
-                
-        # Create the edges.
-        for descriptor in self.__pending:
-            prereqs = descriptor.GetPrerequisites()
-            if prereqs:
-                for (prereq_id, outcome) in prereqs.items():
-                    if not self.__descriptors.has_key(prereq_id):
-                        # The prerequisite is not amongst the list of
-                        # tests to run.  In that case we do still run
-                        # the dependent test; it was explicitly
-                        # requested by the user.
-                        continue
-                    prereq_desc = self.__descriptors[prereq_id]
-                    self.__descriptor_graph[prereq_desc][1] \
-                        .append((descriptor, outcome))
-                    self.__descriptor_graph[descriptor][0] += 1
-
-            if not self.__descriptor_graph[descriptor][0]:
-                # A node with no prerequisites is ready.
-                self.__ready.append(descriptor)
-
-        # Iterate until there are no more tests to run.
-        while ((self.__pending or self.__ready)
-               and not self.IsTerminationRequested()):
-            # If there are no idle targets, block until we get a
-            # response.  There is nothing constructive we can do.
-            idle_targets = self.__idle_targets
-            if not idle_targets:
+            self.__statuses[id] = _TestStatus()
+
+        # The stack of tests whose prerequisites we are trying to
+        # satisfy.
+        self.__prereq_stack = []
+        # The same thing as a dict, for doing loop detection.
+        self.__ids_on_stack = {}
+
+        # No tests are currently runnable.  This is a dictionary indexed
+        # by target group.  Each element maps to a list of runnable
+        # tests in that target group.
+        self.__runnable = {}
+
+        while self.__num_tests_started < num_tests:
+            # Sweep through and clear any responses that have come
+            # back; this also updates the idle target list.
+            while self._CheckForResponse(wait=0):
+                pass
+
+            # Now look for idle targets.
+            if not self.__idle_targets:
+                if len(self.__stuck_targets) == len(self.__targets):
+                    # All targets are stuck.  This means that last time
+                    # through the loop they were all idle and no work
+                    # could be assigned to them; furthermore, no
+                    # prospect of further work has come in since then,
+                    # or they would have been moved back to the merely
+                    # idle list.  Therefore, any tests currently listed
+                    # as runnable have invalid targets and should be
+                    # marked UNTESTED.
+                    self._Trace("All targets stuck"
+                                " -- clearing runnable queue.")
+                    self._ClearAllRunnableTests("No matching target")
+                    continue
+                # Otherwise, we just need to block until there's work to
+                # do.
                 self._Trace("All targets are busy -- waiting.")
-                # Read a reply from the response_queue.
                 self._CheckForResponse(wait=1)
                 self._Trace("Response received.")
-                # Keep going.
+                # Found one; start over in hopes things will be better
+                # this time.
                 continue
 
-            # If there are no tests ready to run, but no tests are
-            # actually running at this time, we have
-            # a cycle in the dependency graph.  Pull the head off the
-            # pending queue and mark it UNTESTED, see if that helps.
-            if (not self.__ready and not self.__running):
-                descriptor = self.__pending[0]
-                self._Trace(("Dependency cycle, discarding %s."
-                             % descriptor.GetId()))
-                self.__pending.remove(descriptor)
-                self._AddUntestedResult(descriptor.GetId(),
-                                        qm.message("dependency cycle"))
-                self._UpdateDependentTests(descriptor, Result.UNTESTED)
+            # We are careful to only keep idle targets that we could
+            # find tests for, and to loop around again after feeding
+            # each target exactly one test.  If were to instead to give
+            # each target as many tests as it could take until it
+            # became idle, we would have problems in the serial case,
+            # because the target would never become idle, and we would
+            # never loop around to clear out the 'Result's queue.
+            new_idle = []
+            while self.__idle_targets:
+                target = self.__idle_targets.pop()
+                if not self._RunATestOn(target):
+                    # We couldn't find a test; rather than loop
+                    # around in circles, ignore this target until we
+                    # get some more runnable tests.
+                    self.__stuck_targets.append(target)
+                elif target.IsIdle():
+                    # Target is still idle.
+                    new_idle.append(target)
+            self.__idle_targets = new_idle
+
+        # Now all tests have been started; we just have wait for them
+        # all to finish.
+        while self.__running:
+            self._CheckForResponse(wait=1)
+
+
+    def _MakeRunnable(self, descriptor):
+        """Adds a test to the runnable queue."""
+
+        test_id = descriptor.GetId()
+        self._Trace("Test '%s' entered runnable queue." % (test_id,))
+
+        # Previously stuck targets may have something to do now, so
+        # unstick them all.
+        if self.__stuck_targets:
+            self.__idle_targets += self.__stuck_targets
+            self.__stuck_targets = []
+
+        target_group = descriptor.GetTargetGroup()
+        try:
+            stack = self.__runnable[target_group]
+        except KeyError:
+            stack = self.__runnable[target_group] = []
+        stack.append(test_id)
+
+
+    def _RunATestOn(self, target):
+        """Tries to run a test on the given target.
+
+        returns -- true on success, false on failure.  Failure means
+        that no test was found that could be run on this target."""
+
+        self._Trace("Looking for a test for target %s"
+                    % target.GetName())
+
+        for target_group, tests in self.__runnable.iteritems():
+            if target.IsInGroup(target_group) and tests:
+                test_id = tests.pop()
+                try:
+                    descriptor = self.__database.GetTest(test_id)
+                except qm.test.database.NoSuchTestError:
+                    self._AddLoadErrorResult(test_id)
+                else:
+                    self._StartTest(target, descriptor)
+                    return 1
+
+        # No already runnable tests.  Try to get new ones.
+        while 1:
+            descriptor = self._FindRunnableTest()
+            if descriptor is None:
+                # We're out of runnable tests altogether.
+                return 0
+            elif target.IsInGroup(descriptor.GetTargetGroup()):
+                # We found a test we can run.
+                self._StartTest(target, descriptor)
+                return 1
+            else:
+                # We found a test that we can't run, but someone else
+                # can; put it on the queue for them and try again.
+                self._MakeRunnable(descriptor)
+
+
+    def _StartTest(self, target, descriptor):
+
+        target_name = target.GetName()
+        test_id = descriptor.GetId()
+        self._Trace("Running %s on %s" % (test_id, target_name))
+        assert self.__statuses[test_id].IsRunnable()
+        self.__num_tests_started += 1
+        self.__running += 1
+        target.RunTest(descriptor, self.__context)
+
+
+    class _ConsumedTest(Exception):
+        """Thrown when a test is consumed instead of processed normally.
+
+        Generally this happens because an error was detected during said
+        processing, and the test was thrown out marked UNTESTED."""
+        
+        pass
+
+
+    def _FindRunnableTest(self):
+        """Attempt find at least one runnable test.
+
+        This will only return tests that are ready to be run
+        immediately, though as a side effect it will set up tests to
+        become automatically runnable later when the prerequisites they
+        depend on finish running.  All tests become runnable in one of
+        these two ways, and therefore this can be considered the core of
+        the scheduling algorithm.  It is only here that tests are pulled
+        from the user-provided list, and here is also where we detect
+        cycles.
+
+        returns -- the descriptor of the new runnable test, or 'None' if
+        no such test could be found."""
+
+        while 1:
+            if not self.__prereq_stack:
+                # We ran out of prerequisite tests, so pull a new one
+                # off the user's list.
+                try:
+                    test_id = self.__tests_iterator.next()
+                except StopIteration:
+                    # We're entirely out of fresh tests; give up.
+                    return None
+                if self.__statuses[test_id].IsConsidering():
+                    # This test has already been handled (probably
+                    # because it's a prereq of a test already seen).
+                    continue
+                # We have a fresh test.
+                try:
+                    self._AddTestToStack(test_id)
+                except self._ConsumedTest:
+                    # Or maybe not.  Skip it and go on to the next.
+                    continue
+                self._Trace("Added new test %s to root of stack" % (test_id,))
+
+            descriptor, prereqs = self.__prereq_stack[-1]
+            # First look at the listed prereqs.
+            if prereqs:
+                new_test_id = prereqs.pop()
+                # We must filter tests that are already in the process
+                # here; if we were to do it earlier, we would be in
+                # danger of being confused by dependency graphs like
+                # A->B, A->C, B->C, where we can't know ahead of time
+                # that A's dependence on C is unnecessary.
+                if self.__statuses[new_test_id].IsConsidering():
+                    # This one is already in process.  This is also what
+                    # a dependency cycle looks like, so check for that
+                    # now.
+                    if new_test_id in self.__ids_on_stack:
+                        self._Trace("Cycle detected (%s)"
+                                    % (new_test_id,))
+                        self._AddUntestedResult \
+                                 (new_test_id,
+                                  qm.message("dependency cycle"))
+                    continue
+                else:
+                    try:
+                        self._AddTestToStack(new_test_id)
+                        # Got a new test on the top of the stack; start
+                        # over with it.
+                        continue
+                    except self._ConsumedTest:
+                        # If it ran immediately, we still start over,
+                        # but with the old test.
+                        continue
+            else:
+                # This test is ready to come off the stack.
+                # Physically remove it from the stack.
+                test_id = descriptor.GetId()
+                del self.__ids_on_stack[test_id]
+                self.__prereq_stack.pop()
+
+                # Now, either it's already run (probably because it
+                # marked UNTESTED to break a cycle), ready to run, or
+                # needs to wait for some prereqs to finish.
+
+                # First check to see if it has run already.
+                if self.__statuses[test_id].IsRunnable():
+                    # It has already run.  Try again.
+                    continue
+
+                # Now check the prerequisites.
+                try:
+                    waiting_on = self._IsWaitingFor(descriptor)
+                except self._ConsumedTest:
+                    # Never mind, there was a problem (probably a failed
+                    # prerequisite).  Try again.
+                    continue
+                
+                if not waiting_on:
+                    # We've finally found a runnable test.
+                    self.__statuses[test_id].MarkRunnable()
+                    return descriptor
+                else:
+                    # This test will be runnable once we hear back from
+                    # the tests it depends on.  Ask them to notify it.
+                    for child_id in waiting_on:
+                        self.__statuses[child_id].AddParent(test_id)
+                    # But we still need our immediately runnable test,
+                    # so try again.
+                    continue
+
+            # Should never get here.
+            assert 0
+
+
+    def _AddTestToStack(self, test_id):
+        """Adds 'test_id' to the stack of current tests.
+
+        Updates the test status, sets up cycle detection, and suchlike.
+        May consume the passed in test; if so, will throw a
+        '_ConsumedTest' exception."""
+        
+        self._Trace("Trying to add %s to stack" % test_id)
+
+        # Update test status.
+        self.__statuses[test_id].MarkConsidering()
+
+        # Load the descriptor.
+        try:
+            descriptor = self.__database.GetTest(test_id)
+        except:
+            self._AddLoadErrorResult(test_id)
+            raise self._ConsumedTest, test_id, sys.exc_info()[2]
+
+
+        # Finally calculate which prerequisites are actually supposed to
+        # be run (later we will do further filtering to weed out tests
+        # that have already run, but this must be done on the fly).
+        prereqs_iter = iter(descriptor.GetPrerequisites())
+        relevant_prereqs = filter(self.__statuses.has_key, prereqs_iter)
+
+        # And store it all in the stack.
+        self.__ids_on_stack[test_id] = None
+        self.__prereq_stack.append((descriptor, relevant_prereqs))
+
+        
+    def _IsWaitingFor(self, test_descriptor):
+        """Finds the prerequisites 'test_descriptor' is waiting on.
+
+        Returns a list of id's of tests that need to complete before
+        'test_descriptor' can be run.  If any known outcomes are
+        violated, consumes the test and raises a '_ConsumedTest'
+        exception."""
+
+        id = test_descriptor.GetId()
+        needed = []
+
+        prereqs = test_descriptor.GetPrerequisites()
+        for prereq_id, outcome in prereqs.iteritems():
+            try:
+                prereq_status = self.__statuses[prereq_id]
+            except KeyError:
+                # This prerequisite is not being run at all.
                 continue
 
-            # There is at least one idle target.  Try to find something
-            # that it can do.
-            wait = 1
-            for descriptor in self.__ready:
-                for target in idle_targets:
-                    if target.IsInGroup(descriptor.GetTargetGroup()):
-                        # This test can be run on this target.  Remove
-                        # it from the ready list.
-                        self.__ready.remove(descriptor)
-                        # And from the pending list.
-                        try:
-                            self.__pending.remove(descriptor)
-                        except ValueError:
-                            # If the test is not pending, that means it
-                            # got pulled off for some reason
-                            # (e.g. breaking dependency cycles).  Don't
-                            # try to run it, it won't work.
-                            self._Trace(("Ready test %s not pending, skipped"
-                                         % descriptor.GetId()))
-                            wait = 0
-                            break
-
-                        # Output a trace message.
-                        self._Trace(("About to run %s."
-                                     % descriptor.GetId()))
-                        # Run it.
-                        self.__running += 1
-                        target.RunTest(descriptor, self.__context)
-                        # If the target is no longer idle, remove it
-                        # from the idle_targets list.
-                        if not target.IsIdle():
-                            self._Trace("Target is no longer idle.")
-                            self.__idle_targets.remove(target)
-                        else:
-                            self._Trace("Target is still idle.")
-                        # We have done something useful on this
-                        # iteration.
-                        wait = 0
-                        break
+            if prereq_status.IsFinished():
+                prereq_outcome = prereq_status.GetOutcome()
+                if outcome != prereq_outcome:
+                    # Failed prerequisite.
+                    self._AddUntestedResult \
+                        (id,
+                         qm.message("failed prerequisite"),
+                         {'qmtest.prequisite': prereq_id,
+                          'qmtest.outcome': prereq_outcome,
+                          'qmtest.expected_outcome': outcome })
+                    raise self._ConsumedTest
+                else:
+                    # Passed prerequisite, do nothing.
+                    pass
+            else:
+                # Unfinished prerequisite, make a note.
+                needed.append(prereq_id)
+        return needed
 
-                if not wait:
+
+    def _PrerequisiteFinishedCallback(self, test_id):
+        """Check 'test_id's prerequisites, and do the right thing.
+
+        This function is called whenever a test may have become
+        runnable, because a prerequisite's result became available.  It
+        is only called if we are not already runnable.
+
+        The "right thing" means if any prequisites fail, emit an
+        UNTESTED result; otherwise, if any prerequisites have unknown
+        result, do nothing; otherwise, add this test to the runnable
+        queue."""
+
+        self._Trace("%s had a prerequisite finish" % (test_id,))
+
+        try:
+            descriptor = self.__database.GetTest(test_id)
+        except:
+            self._AddLoadErrorResult(test_id)
+            return
+
+        try:
+            waiting_for = self._IsWaitingFor(descriptor)
+        except self._ConsumedTest:
+            return
+
+        if not waiting_for:
+            # All prerequisites ran and were satisfied.  This test can
+            # now run.
+            self.__statuses[test_id].MarkRunnable()
+            self._MakeRunnable(descriptor)
+            
+
+    def _HandleResult(self, result):
+        """Do processing associated with a new result.
+
+        'result' -- A 'Result' object representing the result of running
+        a test or resource."""
+
+        # Output a trace message.
+        id = result.GetId()
+        self._Trace("Recording %s result for %s." % (result.GetKind(), id))
+
+        # Find the target with the name indicated in the result.
+        if result.has_key(Result.TARGET):
+            for target in self.__targets:
+                if target.GetName() == result[Result.TARGET]:
                     break
+            else:
+                assert 0, ("No target %s exists (test id: %s)"
+                           % (result[Result.TARGET], id))
+        else:
+            # Not all results will have associated targets.  If the
+            # test was not run at all, there will be no associated
+            # target.
+            target = None
 
-            # Output a trace message.
-            self._Trace("About to check for a response in %s mode."
-                        % ((wait and "blocking") or "nonblocking"))
-                    
-            # See if any targets have finished their assignments.  If
-            # we did not schedule any additional work during this
-            # iteration of the loop, there's no point in continuing
-            # until some target finishes what it's doing.
-            self._CheckForResponse(wait=wait)
+        # Having no target is a rare occurrence; output a trace message.
+        if not target:
+            self._Trace("No target for %s." % result.GetId())
 
+        # This target might now be idle.
+        if (target
+            and target not in self.__idle_targets
+            and target not in self.__stuck_targets
+            and target.IsIdle()):
             # Output a trace message.
-            self._Trace("Done checking for responses.")
+            self._Trace("Target is now idle.\n")
+            self.__idle_targets.append(target)
+            
+        # Only tests have expectations or scheduling dependencies, so
+        # this next bit only applies to tests:
+        if result.GetKind() == Result.TEST:
+            # We now know this test's outcome, so record it in our global
+            # status dictionary.
+            test_status = self.__statuses[id]
+            test_status.SetOutcome(result.GetOutcome())
+
+            # And then poke all the tests that might have become runnable.
+            parents = test_status.ConsumeParents()
+            for parent_id in parents:
+                if not self.__statuses[parent_id].IsRunnable():
+                    self._PrerequisiteFinishedCallback(parent_id)
+
+            # Check for unexpected outcomes.
+            if result.GetKind() == Result.TEST:
+                if (self.__expectations.get(id, Result.PASS)
+                    != result.GetOutcome()):
+                    self.__any_unexpected_outcomes = 1
+            
+        # Output a trace message.
+        self._Trace("Writing result for %s to streams." % id)
 
-        # Any tests that are still pending are untested, unless there
-        # has been an explicit request that we exit immediately.
-        if not self.IsTerminationRequested():
-            for descriptor in self.__pending:
-                self._AddUntestedResult(descriptor.GetId(),
-                                        qm.message("execution terminated"))
+        # Report the result.
+        for rs in self.__result_streams:
+            rs.WriteResult(result)
 
 
     def _CheckForResponse(self, wait):
@@ -336,19 +876,12 @@
                 self._Trace("Got %s result for %s from queue."
                              % (result.GetKind(), result.GetId()))
                 # Handle it.
-                self._AddResult(result)
+                self._HandleResult(result)
                 if result.GetKind() == Result.TEST:
                     assert self.__running > 0
                     self.__running -= 1
                 # Output a trace message.
                 self._Trace("Recorded result.")
-                # If this was a test result, there may be other tests that
-                # are now eligible to run.
-                if result.GetKind() == Result.TEST:
-                    # Get the descriptor for this test.
-                    descriptor = self.__descriptors[result.GetId()]
-                    # Iterate through each of the dependent tests.
-                    self._UpdateDependentTests(descriptor, result.GetOutcome())
                 return result
             except qm.queue.Empty:
                 # If there is nothing in the queue, then this exception will
@@ -371,102 +904,20 @@
                 continue
 
 
-    def _UpdateDependentTests(self, descriptor, outcome):
-        """Update the status of tests that depend on 'node'.
+    ### Various methods to signal errors with particular tests.
 
-        'descriptor' -- A test descriptor.
 
-        'outcome' -- The outcome associated with the test.
+    def _AddErrorResult(self, result):
+        """All error results should be noted with this method.
 
-        If tests that depend on 'descriptor' required a particular
-        outcome, and 'outcome' is different, mark them as untested.  If
-        tests that depend on 'descriptor' are now eligible to run, add
-        them to the '__ready' queue."""
-
-        node = self.__descriptor_graph[descriptor]
-        for (d, o) in node[1]:
-            # Find the node for the dependent test.
-            n = self.__descriptor_graph[d]
-            # If some other prerequisite has already had an undesired
-            # outcome, there is nothing more to do.
-            if n[0] == 0:
-                continue
+        Error results are those that indicate that a test was not run.
+        This is important to keep an accurate count of how many tests
+        are left to run, and to ensure test status's are updated
+        correctly."""
 
-            # If the actual outcome is not the outcome that was
-            # expected, the dependent test cannot be run.
-            if outcome != o:
-                try:
-                    # This test will never be run.
-                    n[0] = 0
-                    self.__pending.remove(d)
-                    # Mark it untested.
-                    self._AddUntestedResult(d.GetId(),
-                                            qm.message("failed prerequisite"),
-                                            { 'qmtest.prequisite' :
-                                              descriptor.GetId(),
-                                              'qmtest.outcome' : outcome,
-                                              'qmtest.expected_outcome' : o })
-                    # Recursively remove tests that depend on d.
-                    self._UpdateDependentTests(d, Result.UNTESTED)
-                except ValueError:
-                    # This test has already been taken off the pending queue;
-                    # assume a result has already been recorded.  This can
-                    # happen when we're breaking dependency cycles.
-                    pass
-            else:
-                # Decrease the count associated with the node, if
-                # the test has not already been declared a failure.
-                n[0] -= 1
-                # If this was the last prerequisite, this test
-                # is now ready.
-                if n[0] == 0:
-                    self.__ready.append(d)
-                    
-    
-    def _AddResult(self, result):
-        """Report the result of running a test or resource.
-
-        'result' -- A 'Result' object representing the result of running
-        a test or resource."""
-
-        # Output a trace message.
-        self._Trace("Recording %s result for %s."
-                    % (result.GetKind(), result.GetId()))
-
-        # Find the target with the name indicated in the result.
-        if result.has_key(Result.TARGET):
-            for target in self.__targets:
-                if target.GetName() == result[Result.TARGET]:
-                    break
-        else:
-            # Not all results will have associated targets.  If the
-            # test was not run at all, there will be no associated
-            # target.
-            target = None
-
-        # Having no target is a rare occurrence; output a trace message.
-        if not target:
-            self._Trace("No target for %s." % result.GetId())
-                        
-        # Check for unexpected outcomes.
-        if result.GetKind() == Result.TEST  \
-           and (self.__expectations.get(result.GetId(), Result.PASS)
-                != result.GetOutcome()):
-            self.__any_unexpected_outcomes = 1
-            
-        # This target might now be idle.
-        if (target and target not in self.__idle_targets
-            and target.IsIdle()):
-            # Output a trace message.
-            self._Trace("Target is now idle.\n")
-            self.__idle_targets.append(target)
-
-        # Output a trace message.
-        self._Trace("Writing result for %s to streams." % result.GetId())
-
-        # Report the result.
-        for rs in self.__result_streams:
-            rs.WriteResult(result)
+        self.__num_tests_started += 1
+        self.__statuses[result.GetId()].MarkRunnable()
+        self._HandleResult(result)
 
 
     def _AddUntestedResult(self, test_name, cause, annotations={}):
@@ -482,7 +933,34 @@
         # Create the result.
         result = Result(Result.TEST, test_name, Result.UNTESTED, annotations)
         result[Result.CAUSE] = cause
-        self._AddResult(result)
+        self._AddErrorResult(result)
+
+
+    def _AddLoadErrorResult(self, test_id):
+        """Add a 'Result' indicating that loading 'test_id' failed.
+
+        Should be called from the catch block that caught the error, as
+        'Result.NoteException' is called."""
+
+        result = Result(Result.TEST, test_id)
+        result.NoteException(cause = "Could not load test.",
+                             outcome = Result.UNTESTED)
+        self._AddErrorResult(result)
+        
+
+    def _ClearAllRunnableTests(self, cause):
+        """Marks all currently runnable tests as UNTESTED.
+
+        This is called when it is detected that all tests currently in
+        the runnable queue can never by run, generally because their
+        target specification does not match any available target."""
+
+        for runnable_stack in self.__runnable.itervalues():
+            while runnable_stack:
+                self._AddUntestedResult(runnable_stack.pop(), cause)
+
+
+    ### Utility methods.
 
 
     def _Trace(self, message):
diff -urN --exclude='*~' --exclude='.*' --exclude=CVS --exclude='*.pyo' --exclude='*.pyc' --exclude=build --exclude=GNUmakefile --exclude=config.log --exclude=config.status --exclude=setup_path.py --exclude=qm.sh --exclude=qmtest --exclude=qm.spec --exclude='*.dtd' --exclude=CATALOG --exclude=thread_target --exclude=process_target --exclude='*.qmr' qm-clean/tests/regress/bad_target1/QMTest/configuration qm-efficient-scheduling/tests/regress/bad_target1/QMTest/configuration
--- qm-clean/tests/regress/bad_target1/QMTest/configuration	1969-12-31 16:00:00.000000000 -0800
+++ qm-efficient-scheduling/tests/regress/bad_target1/QMTest/configuration	2003-07-29 03:25:02.000000000 -0700
@@ -0,0 +1,5 @@
+<?xml version="1.0" ?>
+<!DOCTYPE extension
+  PUBLIC '-//Software Carpentry//QMTest Extension V0.1//EN'
+  'http://www.software-carpentry.com/qm/xml/extension'>
+<extension class="xml_database.XMLDatabase" kind="database"/>
\ No newline at end of file
diff -urN --exclude='*~' --exclude='.*' --exclude=CVS --exclude='*.pyo' --exclude='*.pyc' --exclude=build --exclude=GNUmakefile --exclude=config.log --exclude=config.status --exclude=setup_path.py --exclude=qm.sh --exclude=qmtest --exclude=qm.spec --exclude='*.dtd' --exclude=CATALOG --exclude=thread_target --exclude=process_target --exclude='*.qmr' qm-clean/tests/regress/bad_target1/a.qmt qm-efficient-scheduling/tests/regress/bad_target1/a.qmt
--- qm-clean/tests/regress/bad_target1/a.qmt	1969-12-31 16:00:00.000000000 -0800
+++ qm-efficient-scheduling/tests/regress/bad_target1/a.qmt	2003-07-29 03:36:10.000000000 -0700
@@ -0,0 +1,5 @@
+<?xml version="1.0" ?>
+<!DOCTYPE extension
+  PUBLIC '-//Software Carpentry//QMTest Extension V0.1//EN'
+  'http://www.software-carpentry.com/qm/xml/extension'>
+<extension class="python.ExecTest" kind="test"><argument name="prerequisites"><set/></argument><argument name="source"><text>pass</text></argument><argument name="target_group"><text>.*</text></argument><argument name="expression"><text>1</text></argument><argument name="resources"><set/></argument></extension>
\ No newline at end of file
diff -urN --exclude='*~' --exclude='.*' --exclude=CVS --exclude='*.pyo' --exclude='*.pyc' --exclude=build --exclude=GNUmakefile --exclude=config.log --exclude=config.status --exclude=setup_path.py --exclude=qm.sh --exclude=qmtest --exclude=qm.spec --exclude='*.dtd' --exclude=CATALOG --exclude=thread_target --exclude=process_target --exclude='*.qmr' qm-clean/tests/regress/bad_target1/bad_target.qmt qm-efficient-scheduling/tests/regress/bad_target1/bad_target.qmt
--- qm-clean/tests/regress/bad_target1/bad_target.qmt	1969-12-31 16:00:00.000000000 -0800
+++ qm-efficient-scheduling/tests/regress/bad_target1/bad_target.qmt	2003-07-29 03:35:53.000000000 -0700
@@ -0,0 +1,5 @@
+<?xml version="1.0" ?>
+<!DOCTYPE extension
+  PUBLIC '-//Software Carpentry//QMTest Extension V0.1//EN'
+  'http://www.software-carpentry.com/qm/xml/extension'>
+<extension class="python.ExecTest" kind="test"><argument name="prerequisites"><set/></argument><argument name="source"><text>pass</text></argument><argument name="target_group"><text>$^</text></argument><argument name="expression"><text>1</text></argument><argument name="resources"><set/></argument></extension>
\ No newline at end of file
diff -urN --exclude='*~' --exclude='.*' --exclude=CVS --exclude='*.pyo' --exclude='*.pyc' --exclude=build --exclude=GNUmakefile --exclude=config.log --exclude=config.status --exclude=setup_path.py --exclude=qm.sh --exclude=qmtest --exclude=qm.spec --exclude='*.dtd' --exclude=CATALOG --exclude=thread_target --exclude=process_target --exclude='*.qmr' qm-clean/tests/regress/bad_target2/QMTest/configuration qm-efficient-scheduling/tests/regress/bad_target2/QMTest/configuration
--- qm-clean/tests/regress/bad_target2/QMTest/configuration	1969-12-31 16:00:00.000000000 -0800
+++ qm-efficient-scheduling/tests/regress/bad_target2/QMTest/configuration	2003-07-29 03:25:07.000000000 -0700
@@ -0,0 +1,5 @@
+<?xml version="1.0" ?>
+<!DOCTYPE extension
+  PUBLIC '-//Software Carpentry//QMTest Extension V0.1//EN'
+  'http://www.software-carpentry.com/qm/xml/extension'>
+<extension class="xml_database.XMLDatabase" kind="database"/>
\ No newline at end of file
diff -urN --exclude='*~' --exclude='.*' --exclude=CVS --exclude='*.pyo' --exclude='*.pyc' --exclude=build --exclude=GNUmakefile --exclude=config.log --exclude=config.status --exclude=setup_path.py --exclude=qm.sh --exclude=qmtest --exclude=qm.spec --exclude='*.dtd' --exclude=CATALOG --exclude=thread_target --exclude=process_target --exclude='*.qmr' qm-clean/tests/regress/bad_target2/a.qmt qm-efficient-scheduling/tests/regress/bad_target2/a.qmt
--- qm-clean/tests/regress/bad_target2/a.qmt	1969-12-31 16:00:00.000000000 -0800
+++ qm-efficient-scheduling/tests/regress/bad_target2/a.qmt	2003-07-29 03:39:42.000000000 -0700
@@ -0,0 +1,5 @@
+<?xml version="1.0" ?>
+<!DOCTYPE extension
+  PUBLIC '-//Software Carpentry//QMTest Extension V0.1//EN'
+  'http://www.software-carpentry.com/qm/xml/extension'>
+<extension class="python.ExecTest" kind="test"><argument name="prerequisites"><set><tuple><text>bad_target</text><enumeral>PASS</enumeral></tuple></set></argument><argument name="source"><text>pass</text></argument><argument name="target_group"><text>.*</text></argument><argument name="expression"><text>1</text></argument><argument name="resources"><set/></argument></extension>
diff -urN --exclude='*~' --exclude='.*' --exclude=CVS --exclude='*.pyo' --exclude='*.pyc' --exclude=build --exclude=GNUmakefile --exclude=config.log --exclude=config.status --exclude=setup_path.py --exclude=qm.sh --exclude=qmtest --exclude=qm.spec --exclude='*.dtd' --exclude=CATALOG --exclude=thread_target --exclude=process_target --exclude='*.qmr' qm-clean/tests/regress/bad_target2/bad_target.qmt qm-efficient-scheduling/tests/regress/bad_target2/bad_target.qmt
--- qm-clean/tests/regress/bad_target2/bad_target.qmt	1969-12-31 16:00:00.000000000 -0800
+++ qm-efficient-scheduling/tests/regress/bad_target2/bad_target.qmt	2003-07-29 03:39:08.000000000 -0700
@@ -0,0 +1,5 @@
+<?xml version="1.0" ?>
+<!DOCTYPE extension
+  PUBLIC '-//Software Carpentry//QMTest Extension V0.1//EN'
+  'http://www.software-carpentry.com/qm/xml/extension'>
+<extension class="python.ExecTest" kind="test"><argument name="prerequisites"><set/></argument><argument name="source"><text>pass</text></argument><argument name="target_group"><text>$^</text></argument><argument name="expression"><text>1</text></argument><argument name="resources"><set/></argument></extension>
\ No newline at end of file
diff -urN --exclude='*~' --exclude='.*' --exclude=CVS --exclude='*.pyo' --exclude='*.pyc' --exclude=build --exclude=GNUmakefile --exclude=config.log --exclude=config.status --exclude=setup_path.py --exclude=qm.sh --exclude=qmtest --exclude=qm.spec --exclude='*.dtd' --exclude=CATALOG --exclude=thread_target --exclude=process_target --exclude='*.qmr' qm-clean/tests/regress/nocycle1/QMTest/configuration qm-efficient-scheduling/tests/regress/nocycle1/QMTest/configuration
--- qm-clean/tests/regress/nocycle1/QMTest/configuration	1969-12-31 16:00:00.000000000 -0800
+++ qm-efficient-scheduling/tests/regress/nocycle1/QMTest/configuration	2003-07-29 03:24:47.000000000 -0700
@@ -0,0 +1,5 @@
+<?xml version="1.0" ?>
+<!DOCTYPE extension
+  PUBLIC '-//Software Carpentry//QMTest Extension V0.1//EN'
+  'http://www.software-carpentry.com/qm/xml/extension'>
+<extension class="xml_database.XMLDatabase" kind="database"/>
\ No newline at end of file
diff -urN --exclude='*~' --exclude='.*' --exclude=CVS --exclude='*.pyo' --exclude='*.pyc' --exclude=build --exclude=GNUmakefile --exclude=config.log --exclude=config.status --exclude=setup_path.py --exclude=qm.sh --exclude=qmtest --exclude=qm.spec --exclude='*.dtd' --exclude=CATALOG --exclude=thread_target --exclude=process_target --exclude='*.qmr' qm-clean/tests/regress/nocycle1/a.qmt qm-efficient-scheduling/tests/regress/nocycle1/a.qmt
--- qm-clean/tests/regress/nocycle1/a.qmt	1969-12-31 16:00:00.000000000 -0800
+++ qm-efficient-scheduling/tests/regress/nocycle1/a.qmt	2003-07-29 03:29:36.000000000 -0700
@@ -0,0 +1,5 @@
+<?xml version="1.0" ?>
+<!DOCTYPE extension
+  PUBLIC '-//Software Carpentry//QMTest Extension V0.1//EN'
+  'http://www.software-carpentry.com/qm/xml/extension'>
+<extension class="python.ExecTest" kind="test"><argument name="prerequisites"><set><tuple><text>b</text><enumeral>PASS</enumeral></tuple><tuple><text>c</text><enumeral>PASS</enumeral></tuple></set></argument><argument name="source"><text>pass</text></argument><argument name="target_group"><text>.*</text></argument><argument name="expression"><text>1</text></argument><argument name="resources"><set/></argument></extension>
diff -urN --exclude='*~' --exclude='.*' --exclude=CVS --exclude='*.pyo' --exclude='*.pyc' --exclude=build --exclude=GNUmakefile --exclude=config.log --exclude=config.status --exclude=setup_path.py --exclude=qm.sh --exclude=qmtest --exclude=qm.spec --exclude='*.dtd' --exclude=CATALOG --exclude=thread_target --exclude=process_target --exclude='*.qmr' qm-clean/tests/regress/nocycle1/b.qmt qm-efficient-scheduling/tests/regress/nocycle1/b.qmt
--- qm-clean/tests/regress/nocycle1/b.qmt	1969-12-31 16:00:00.000000000 -0800
+++ qm-efficient-scheduling/tests/regress/nocycle1/b.qmt	2003-07-29 03:31:31.000000000 -0700
@@ -0,0 +1,5 @@
+<?xml version="1.0" ?>
+<!DOCTYPE extension
+  PUBLIC '-//Software Carpentry//QMTest Extension V0.1//EN'
+  'http://www.software-carpentry.com/qm/xml/extension'>
+<extension class="python.ExecTest" kind="test"><argument name="prerequisites"><set><tuple><text>d</text><enumeral>PASS</enumeral></tuple></set></argument><argument name="source"><text>pass</text></argument><argument name="target_group"><text>.*</text></argument><argument name="expression"><text>1</text></argument><argument name="resources"><set/></argument></extension>
diff -urN --exclude='*~' --exclude='.*' --exclude=CVS --exclude='*.pyo' --exclude='*.pyc' --exclude=build --exclude=GNUmakefile --exclude=config.log --exclude=config.status --exclude=setup_path.py --exclude=qm.sh --exclude=qmtest --exclude=qm.spec --exclude='*.dtd' --exclude=CATALOG --exclude=thread_target --exclude=process_target --exclude='*.qmr' qm-clean/tests/regress/nocycle1/c.qmt qm-efficient-scheduling/tests/regress/nocycle1/c.qmt
--- qm-clean/tests/regress/nocycle1/c.qmt	1969-12-31 16:00:00.000000000 -0800
+++ qm-efficient-scheduling/tests/regress/nocycle1/c.qmt	2003-07-29 03:31:09.000000000 -0700
@@ -0,0 +1,5 @@
+<?xml version="1.0" ?>
+<!DOCTYPE extension
+  PUBLIC '-//Software Carpentry//QMTest Extension V0.1//EN'
+  'http://www.software-carpentry.com/qm/xml/extension'>
+<extension class="python.ExecTest" kind="test"><argument name="prerequisites"><set><tuple><text>d</text><enumeral>PASS</enumeral></tuple></set></argument><argument name="source"><text>pass</text></argument><argument name="target_group"><text>.*</text></argument><argument name="expression"><text>1</text></argument><argument name="resources"><set/></argument></extension>
diff -urN --exclude='*~' --exclude='.*' --exclude=CVS --exclude='*.pyo' --exclude='*.pyc' --exclude=build --exclude=GNUmakefile --exclude=config.log --exclude=config.status --exclude=setup_path.py --exclude=qm.sh --exclude=qmtest --exclude=qm.spec --exclude='*.dtd' --exclude=CATALOG --exclude=thread_target --exclude=process_target --exclude='*.qmr' qm-clean/tests/regress/nocycle1/d.qmt qm-efficient-scheduling/tests/regress/nocycle1/d.qmt
--- qm-clean/tests/regress/nocycle1/d.qmt	1969-12-31 16:00:00.000000000 -0800
+++ qm-efficient-scheduling/tests/regress/nocycle1/d.qmt	2003-07-29 03:31:24.000000000 -0700
@@ -0,0 +1,5 @@
+<?xml version="1.0" ?>
+<!DOCTYPE extension
+  PUBLIC '-//Software Carpentry//QMTest Extension V0.1//EN'
+  'http://www.software-carpentry.com/qm/xml/extension'>
+<extension class="python.ExecTest" kind="test"><argument name="prerequisites"><set><tuple><text>e</text><enumeral>PASS</enumeral></tuple></set></argument><argument name="source"><text>pass</text></argument><argument name="target_group"><text>.*</text></argument><argument name="expression"><text>1</text></argument><argument name="resources"><set/></argument></extension>
diff -urN --exclude='*~' --exclude='.*' --exclude=CVS --exclude='*.pyo' --exclude='*.pyc' --exclude=build --exclude=GNUmakefile --exclude=config.log --exclude=config.status --exclude=setup_path.py --exclude=qm.sh --exclude=qmtest --exclude=qm.spec --exclude='*.dtd' --exclude=CATALOG --exclude=thread_target --exclude=process_target --exclude='*.qmr' qm-clean/tests/regress/nocycle1/e.qmt qm-efficient-scheduling/tests/regress/nocycle1/e.qmt
--- qm-clean/tests/regress/nocycle1/e.qmt	1969-12-31 16:00:00.000000000 -0800
+++ qm-efficient-scheduling/tests/regress/nocycle1/e.qmt	2003-07-29 03:33:04.000000000 -0700
@@ -0,0 +1,6 @@
+<?xml version="1.0" ?>
+<!DOCTYPE extension
+  PUBLIC '-//Software Carpentry//QMTest Extension V0.1//EN'
+  'http://www.software-carpentry.com/qm/xml/extension'>
+<extension class="python.ExecTest" kind="test"><argument name="prerequisites"><set/></argument><argument name="source"><text>import time
+time.sleep(1)</text></argument><argument name="target_group"><text>.*</text></argument><argument name="expression"><text>1</text></argument><argument name="resources"><set/></argument></extension>
diff -urN --exclude='*~' --exclude='.*' --exclude=CVS --exclude='*.pyo' --exclude='*.pyc' --exclude=build --exclude=GNUmakefile --exclude=config.log --exclude=config.status --exclude=setup_path.py --exclude=qm.sh --exclude=qmtest --exclude=qm.spec --exclude='*.dtd' --exclude=CATALOG --exclude=thread_target --exclude=process_target --exclude='*.qmr' qm-clean/tests/regress/nocycle2/QMTest/configuration qm-efficient-scheduling/tests/regress/nocycle2/QMTest/configuration
--- qm-clean/tests/regress/nocycle2/QMTest/configuration	1969-12-31 16:00:00.000000000 -0800
+++ qm-efficient-scheduling/tests/regress/nocycle2/QMTest/configuration	2003-07-29 03:24:47.000000000 -0700
@@ -0,0 +1,5 @@
+<?xml version="1.0" ?>
+<!DOCTYPE extension
+  PUBLIC '-//Software Carpentry//QMTest Extension V0.1//EN'
+  'http://www.software-carpentry.com/qm/xml/extension'>
+<extension class="xml_database.XMLDatabase" kind="database"/>
\ No newline at end of file
diff -urN --exclude='*~' --exclude='.*' --exclude=CVS --exclude='*.pyo' --exclude='*.pyc' --exclude=build --exclude=GNUmakefile --exclude=config.log --exclude=config.status --exclude=setup_path.py --exclude=qm.sh --exclude=qmtest --exclude=qm.spec --exclude='*.dtd' --exclude=CATALOG --exclude=thread_target --exclude=process_target --exclude='*.qmr' qm-clean/tests/regress/nocycle2/a.qmt qm-efficient-scheduling/tests/regress/nocycle2/a.qmt
--- qm-clean/tests/regress/nocycle2/a.qmt	1969-12-31 16:00:00.000000000 -0800
+++ qm-efficient-scheduling/tests/regress/nocycle2/a.qmt	2003-07-29 03:29:36.000000000 -0700
@@ -0,0 +1,5 @@
+<?xml version="1.0" ?>
+<!DOCTYPE extension
+  PUBLIC '-//Software Carpentry//QMTest Extension V0.1//EN'
+  'http://www.software-carpentry.com/qm/xml/extension'>
+<extension class="python.ExecTest" kind="test"><argument name="prerequisites"><set><tuple><text>b</text><enumeral>PASS</enumeral></tuple><tuple><text>c</text><enumeral>PASS</enumeral></tuple></set></argument><argument name="source"><text>pass</text></argument><argument name="target_group"><text>.*</text></argument><argument name="expression"><text>1</text></argument><argument name="resources"><set/></argument></extension>
diff -urN --exclude='*~' --exclude='.*' --exclude=CVS --exclude='*.pyo' --exclude='*.pyc' --exclude=build --exclude=GNUmakefile --exclude=config.log --exclude=config.status --exclude=setup_path.py --exclude=qm.sh --exclude=qmtest --exclude=qm.spec --exclude='*.dtd' --exclude=CATALOG --exclude=thread_target --exclude=process_target --exclude='*.qmr' qm-clean/tests/regress/nocycle2/b.qmt qm-efficient-scheduling/tests/regress/nocycle2/b.qmt
--- qm-clean/tests/regress/nocycle2/b.qmt	1969-12-31 16:00:00.000000000 -0800
+++ qm-efficient-scheduling/tests/regress/nocycle2/b.qmt	2003-07-29 03:34:23.000000000 -0700
@@ -0,0 +1,5 @@
+<?xml version="1.0" ?>
+<!DOCTYPE extension
+  PUBLIC '-//Software Carpentry//QMTest Extension V0.1//EN'
+  'http://www.software-carpentry.com/qm/xml/extension'>
+<extension class="python.ExecTest" kind="test"><argument name="prerequisites"><set><tuple><text>c</text><enumeral>PASS</enumeral></tuple></set></argument><argument name="source"><text>pass</text></argument><argument name="target_group"><text>.*</text></argument><argument name="expression"><text>1</text></argument><argument name="resources"><set/></argument></extension>
diff -urN --exclude='*~' --exclude='.*' --exclude=CVS --exclude='*.pyo' --exclude='*.pyc' --exclude=build --exclude=GNUmakefile --exclude=config.log --exclude=config.status --exclude=setup_path.py --exclude=qm.sh --exclude=qmtest --exclude=qm.spec --exclude='*.dtd' --exclude=CATALOG --exclude=thread_target --exclude=process_target --exclude='*.qmr' qm-clean/tests/regress/nocycle2/c.qmt qm-efficient-scheduling/tests/regress/nocycle2/c.qmt
--- qm-clean/tests/regress/nocycle2/c.qmt	1969-12-31 16:00:00.000000000 -0800
+++ qm-efficient-scheduling/tests/regress/nocycle2/c.qmt	2003-07-29 03:34:32.000000000 -0700
@@ -0,0 +1,5 @@
+<?xml version="1.0" ?>
+<!DOCTYPE extension
+  PUBLIC '-//Software Carpentry//QMTest Extension V0.1//EN'
+  'http://www.software-carpentry.com/qm/xml/extension'>
+<extension class="python.ExecTest" kind="test"><argument name="prerequisites"><set/></argument><argument name="source"><text>pass</text></argument><argument name="target_group"><text>.*</text></argument><argument name="expression"><text>1</text></argument><argument name="resources"><set/></argument></extension>

From mark at codesourcery.com  Tue Jul 29 20:25:45 2003
From: mark at codesourcery.com (Mark Mitchell)
Date: Tue, 29 Jul 2003 13:25:45 -0700
Subject: Fix Windows builds
Message-ID: <00a801c3560f$97a92a10$6900a8c0@minax>

Some recent changes were preventing QMTest from building/installing on
Windows.

This patch seems to bring things back to a better state.

-- Mark
-------------- next part --------------
A non-text attachment was scrubbed...
Name: diffs
Type: application/octet-stream
Size: 31063 bytes
Desc: not available
URL: <http://sourcerytools.com/pipermail/qmtest/attachments/20030729/13ba8d72/attachment.obj>

From mark at codesourcery.com  Wed Jul 30 20:13:53 2003
From: mark at codesourcery.com (Mark Mitchell)
Date: Wed, 30 Jul 2003 13:13:53 -0700
Subject: [qmtest] test in tutorial fails on Windows
References: <200307241551.16967.ghost@cs.msu.su> <3F1FFDBE.2070903@tejasconsulting.com>
Message-ID: <00c801c356d7$1aa05b60$6900a8c0@minax>

Danny --

Thanks for your email, and your feedback.  We'll fix these documentation
issues in the next version of QMTest.

The problem you're running into with the expected output from "echo hi" not
matching the actual output has to do with the difference in line-ending
conventions between DOS and UNIX.  We're working on a fix for that problem
as well.

Thanks!

----- Original Message ----- 
From: "Danny Faught" <faught at tejasconsulting.com>
To: <qmtest at codesourcery.com>
Sent: Thursday, July 24, 2003 8:39 AM
Subject: [qmtest] test in tutorial fails on Windows


> I just started looking at QMTest 2.0.3 in earnest in order to use it as
> an example in a tutorial I'm giving at a conference.  So far I'm very
> impressed.
>
> I'm going through the tutorial on Windows 2000 now, and I'm getting
> stuck.  On the page at
>
http://www.codesourcery.com/qm/qmtest_downloads/qm-2.0.3/manual.html/sec-testtut-modifying.html,
>   it says "Click on the Home link to return to the main QMTest page."  I
> haven't found a Home link on the web interface.
>
> In any case, it's not too hard to get to a page where I can do File->New
> Test.  I create a test that tries to do "echo test", but the result when
> I run it is an error: "pywintypes.api_error: (2, 'CreateProcess', 'The
> system cannot find the file specified.')"  It appears that echo is a DOS
> builtin rather than a separate utility as on Unix.
>
> Here's the source for command.test1:
>
> <?xml version="1.0" ?>
> <extension class="command.ExecTest" kind="test"><argument
> name="environment"><set/></argument><argument
> name="program"><text>echo</text></argument><argument
> name="resources"><set/></argument><argument
> name="arguments"><set><text>test</text>
> </set></argument><argument name="stderr"><text/></argument><argument
> name="stdout"><text>test
> </text></argument><argument
> name="prerequisites"><set/></argument><argument name
> ="stdin"><text/></argument><argument
> name="exit_code"><integer>0</integer></argument><argument
> name="target_group"><text>.*</text></argument></extension>
>
> It works better using the command.ShellCommandTest class instead.  In
> that case, the test runs, but I can't get it to match the output -
>
> -------------
> command.test2
> Outcome Cause
> FAIL Unexpected standard output.
> Annotation Value
> ExecTest.expected_stdout
>
> test
>
> ExecTest.stdout
>
> test
>
> qmtest.target local
> --------------
>
> There is a carriage return in the expected output, like the tutorial says:
>
> <?xml version="1.0" ?>
> <extension class="command.ShellCommandTest" kind="test"><argument
> name="environment"><set/></argument><argument
> name="target_group"><text>.*</text></argument><argument
> name="command"><text>echo test</text></argument><argument name="stderr">
> <text/></argument><argument name="stdout"><text>test
> </text></argument><argument
> name="prerequisites"><set/></argument><argument
> name="stdin"><text/></argument><argument
> name="exit_code"><integer>0</integer></argument><argument
> name="resources"><set/></argument></extension>
>
> Any pointers would be appreciated.  I'm using ActiveState Python 2.2.2.
> -- 
> Danny R. Faught
> Tejas Software Consulting
> publisher of Open Testware Reviews -
>    http://tejasconsulting.com/open-testware/
>
>



From mark at codesourcery.com  Wed Jul 30 21:45:15 2003
From: mark at codesourcery.com (Mark Mitchell)
Date: Wed, 30 Jul 2003 14:45:15 -0700
Subject: PATCH: Fix Windows text-comparison problems
Message-ID: <010101c356e3$dcf78600$6900a8c0@minax>

This patch makes comparisons of "stdout" and "stderr" work correctly on
Windows when using the test classes in the "command" module.

-- Mark

-------------- next part --------------
A non-text attachment was scrubbed...
Name: diffs
Type: application/octet-stream
Size: 40639 bytes
Desc: not available
URL: <http://sourcerytools.com/pipermail/qmtest/attachments/20030730/7bc81b59/attachment.obj>

From mark at codesourcery.com  Thu Jul 31 23:21:49 2003
From: mark at codesourcery.com (Mark Mitchell)
Date: Thu, 31 Jul 2003 16:21:49 -0700
Subject: PATCH: Execution engine improvements
Message-ID: <200307312321.h6VNLniI022747@doubledemon.codesourcery.com>


This patch (which is Nathaniel's patch, with some "tidying" by yours
truly) improve the scalability of QMTest's execution engine by
reducing the total amount of memory required and by avoiding a lengthy
up-front computation to determine dependencies between tests.

--
Mark Mitchell
CodeSourcery, LLC
mark at codesourcery.com

2003-07-31  Nathaniel Smith  <njs at codesourcery.com>
	    Mark Mitchell  <mark at codesourcery.com>

	* qm/test/execution_engine.py: Rewrite to improve scalability.
	* test/regress/bad_target1: New test.
	* test/regress/bad_target2: Likewise.
	* test/regress/nocycle1: Likewise.
	* test/regress/nocycle2: Likewise.
	* benchmarks/throughput: New benchmark.

	* qm/test/classes/text_result_stream.py (TextResultStream): Fix
	typo in documentation.
	* qm/test/doc/tour.xml: Update instructions to match GUI changes.

Index: benchmarks/throughput/QMTest/classes.qmc
===================================================================
RCS file: benchmarks/throughput/QMTest/classes.qmc
diff -N benchmarks/throughput/QMTest/classes.qmc
*** /dev/null	1 Jan 1970 00:00:00 -0000
--- benchmarks/throughput/QMTest/classes.qmc	31 Jul 2003 23:12:46 -0000
***************
*** 0 ****
--- 1,2 ----
+ <?xml version="1.0" ?>
+ <class-directory><class kind="test">throughput.ThroughputTest</class><class kind="database">throughput.ThroughputDatabase</class></class-directory>
\ No newline at end of file
Index: benchmarks/throughput/QMTest/configuration
===================================================================
RCS file: benchmarks/throughput/QMTest/configuration
diff -N benchmarks/throughput/QMTest/configuration
*** /dev/null	1 Jan 1970 00:00:00 -0000
--- benchmarks/throughput/QMTest/configuration	31 Jul 2003 23:12:46 -0000
***************
*** 0 ****
--- 1,2 ----
+ <?xml version="1.0" ?>
+ <extension class="throughput.ThroughputDatabase" kind="database"><argument name="num_tests"><integer>1000</integer></argument></extension>
Index: benchmarks/throughput/QMTest/throughput.py
===================================================================
RCS file: benchmarks/throughput/QMTest/throughput.py
diff -N benchmarks/throughput/QMTest/throughput.py
*** /dev/null	1 Jan 1970 00:00:00 -0000
--- benchmarks/throughput/QMTest/throughput.py	31 Jul 2003 23:12:46 -0000
***************
*** 0 ****
--- 1,70 ----
+ ########################################################################
+ #
+ # File:   throughput.py
+ # Author: Mark Mitchell
+ # Date:   07/31/2003
+ #
+ # Contents:
+ #   Test datbase for testing execution engine throughput.
+ #
+ # Copyright (c) 2003 by CodeSourcery, LLC.  All rights reserved. 
+ #
+ # For license terms see the file COPYING.
+ #
+ ########################################################################
+ 
+ ########################################################################
+ # Imports
+ ########################################################################
+ 
+ from qm.fields import *
+ from qm.test.database import *
+ from qm.test.result import *
+ from qm.test.test import *
+ import random
+ 
+ ########################################################################
+ # Classes
+ ########################################################################
+ 
+ class ThroughputTest(Test):
+ 
+     def Run(self, context, result):
+ 
+         return
+         
+ 
+     
+ class ThroughputDatabase(Database):
+ 
+     arguments = [
+         IntegerField("num_tests",
+                      default_value = 100)
+         ]
+ 
+         
+     def GetIds(self, kind, directory = "", scan_subdirs = 1):
+ 
+         if kind != Database.TEST:
+             return super(ThroughputDatabase, self).GetIds(kind,
+                                                           directory,
+                                                           scan_subdirs)
+ 
+         tests = []
+         for x in xrange(self.num_tests):
+             tests.append("test%d" % x)
+ 
+         return tests
+ 
+         
+     def GetTest(self, test_id):
+ 
+         prereqs = []
+         for x in xrange(random.randrange(5)):
+             test = "test%d" % random.randrange(self.num_tests)
+             outcome = random.choice(Result.outcomes)
+             prereqs.append((test, outcome))
+             
+         return TestDescriptor(self, test_id,
+                               "throughput.ThroughputTest",
+                               { Test.PREREQUISITES_FIELD_ID : prereqs })
Index: qm/test/execution_engine.py
===================================================================
RCS file: /home/sc/Repository/qm/qm/test/execution_engine.py,v
retrieving revision 1.21
diff -c -5 -p -r1.21 execution_engine.py
*** qm/test/execution_engine.py	3 Jul 2003 19:32:04 -0000	1.21
--- qm/test/execution_engine.py	31 Jul 2003 23:12:47 -0000
***************
*** 20,29 ****
--- 20,30 ----
  import os
  import qm.common
  import qm.queue
  from   qm.test.base import *
  import qm.test.cmdline
+ import qm.test.database
  from   qm.test.context import *
  import qm.xmlutil
  from   result import *
  import select
  import sys
*************** class ExecutionEngine:
*** 37,54 ****
      """A 'ExecutionEngine' executes tests.
  
      A 'ExecutionEngine' object handles the execution of a collection
      of tests.
  
!     This class schedules the tests, plus the setup and cleanup of any
!     resources they require, across one or more targets.
  
      The shedule is determined dynamically as the tests are executed
      based on which targets are idle and which are not.  Therefore, the
      testing load should be reasonably well balanced, even across a
      heterogeneous network of testing machines."""
!     
      def __init__(self,
                   database,
                   test_ids,
                   context,
                   targets,
--- 38,171 ----
      """A 'ExecutionEngine' executes tests.
  
      A 'ExecutionEngine' object handles the execution of a collection
      of tests.
  
!     This class schedules the tests across one or more targets.
  
      The shedule is determined dynamically as the tests are executed
      based on which targets are idle and which are not.  Therefore, the
      testing load should be reasonably well balanced, even across a
      heterogeneous network of testing machines."""
! 
! 
!     class __TestStatus(object):
!         """A '__TestStatus' indicates whether or not a test has been run.
! 
!         The 'outcome' slot indicates whether the test has not be queued so
!         that it can be run, has completed, or has not been processed at all.
! 
!         If there are tests that have this test as a prerequisite, they are
!         recorded in the 'dependants' slot.
! 
!         Ever test passes through the following states, in the following
!         order:
! 
!         1. Initial
! 
!            A test in this state has not yet been processed.  In this state,
!            the 'outcome' slot is 'None'.
! 
!         2. Queued
! 
!            A test in this state has been placed on the stack of tests
!            waiting to run.  In this state, the 'outcome' slot is
!            'QUEUED'.  Such a test may be waiting for prerequisites to
!            complete before it can run.
! 
!         3. Ready
! 
!            A test in this state is ready to run.  All prerequisites have
!            completed, and their outcomes were as expected.  In this
!            state, the 'outcome' slot is 'READY'.
! 
!         4. Finished
! 
!            A test in this state has finished running.  In this state, the
!            'outcome' slot is one of the 'Result.outcomes'.
! 
!         The only exception to this order is that when an error is noted
!         (like a failure to load a test from the database, or a
!         prerequisite has an unexpected outcome) a test may jump to the
!         "finished" state without passing through intermediate states."""
! 
!         __slots__ = "outcome", "dependants"
! 
!         QUEUED = "QUEUED"
!         READY = "READY"
! 
!         def __init__(self):
! 
!             self.outcome = None
!             self.dependants = None
! 
! 
!         def GetState(self):
!             """Return the state of this test.
! 
!             returns -- The state of this test, using the representation
!             documented above."""
!             
!             return self.outcome
!         
!         
!         def NoteQueued(self):
!             """Place the test into the "queued" state."""
! 
!             assert self.outcome is None
!             self.outcome = self.QUEUED
! 
! 
!         def HasBeenQueued(self):
!             """Returns true if the test was ever queued.
! 
!             returns -- True if the test has ever been on the queue.
!             Such a test may be ready to run, or may in fact have already
!             run to completion."""
! 
!             return self.outcome == self.QUEUED or self.HasBeenReady()
! 
! 
!         def NoteReady(self):
!             """Place the test into the "ready" state."""
! 
!             assert self.outcome is self.QUEUED
!             self.outcome = self.READY
!             
! 
!         def HasBeenReady(self):
!             """Returns true if the test was ever ready.
! 
!             returns -- True if the test was every ready to run.  Such a
!             test may have already run to completion."""
! 
!             return self.outcome == self.READY or self.IsFinished()
! 
! 
!         def IsFinished(self):
!             """Returns true if the test is in the "finished" state.
! 
!             returns -- True if this test is in the "finished" state."""
! 
!             return not (self.outcome is None
!                         or self.outcome is self.READY
!                         or self.outcome is self.QUEUED)
! 
! 
!         def NoteDependant(self, test_id):
!             """Note that 'test_id' depends on 'self'.
! 
!             'test_id' -- The name of a test.  That test has this test as a
!             prerequisite."""
! 
!             if self.dependants is None:
!                 self.dependants = [test_id]
!             else:
!                 self.dependants.append(test_id)
! 
! 
! 
      def __init__(self,
                   database,
                   test_ids,
                   context,
                   targets,
*************** class ExecutionEngine:
*** 59,69 ****
          'database' -- The 'Database' containing the tests that will be
          run.
          
          'test_ids' -- A sequence of IDs of tests to run.  Where
          possible, the tests are started in the order specified.
! 
          'context' -- The context object to use when running tests.
  
          'targets' -- A sequence of 'Target' objects, representing
          targets on which tests may be run.
  
--- 176,186 ----
          'database' -- The 'Database' containing the tests that will be
          run.
          
          'test_ids' -- A sequence of IDs of tests to run.  Where
          possible, the tests are started in the order specified.
!         
          'context' -- The context object to use when running tests.
  
          'targets' -- A sequence of 'Target' objects, representing
          targets on which tests may be run.
  
*************** class ExecutionEngine:
*** 87,109 ****
              self.__expectations = {}
              
          # There are no input handlers.
          self.__input_handlers = {}
          
-         # All of the targets are idle at first.
-         self.__idle_targets = targets[:]
          # There are no responses from the targets yet.
          self.__response_queue = qm.queue.Queue(0)
          # There no pending or ready tests yet.
-         self.__pending = []
-         self.__ready = []
          self.__running = 0
  
-         # The descriptor graph has not yet been created.
-         self.__descriptors = {}
-         self.__descriptor_graph = {}
-         
          self.__any_unexpected_outcomes = 0
          
          # Termination has not yet been requested.
          self.__terminated = 0
          
--- 204,218 ----
*************** class ExecutionEngine:
*** 119,129 ****
          
          
      def IsTerminationRequested(self):
          """Returns true if termination has been requested.
  
!         return -- True if Terminate has been called."""
  
          return self.__terminated
      
  
      def Run(self):
--- 228,238 ----
          
          
      def IsTerminationRequested(self):
          """Returns true if termination has been requested.
  
!         returns -- True if Terminate has been called."""
  
          return self.__terminated
      
  
      def Run(self):
*************** class ExecutionEngine:
*** 153,163 ****
              for target in self.__targets:
                  target.Stop()
  
              # Read responses until there are no more.
              self._Trace("Checking for final responses.")
!             while self._CheckForResponse(wait=0):
                  pass
              
              # Let all of the result streams know that the test run is
              # complete.
              end_time_str = qm.common.format_time_iso(time.time())
--- 262,272 ----
              for target in self.__targets:
                  target.Stop()
  
              # Read responses until there are no more.
              self._Trace("Checking for final responses.")
!             while self.__CheckForResponse(wait=0):
                  pass
              
              # Let all of the result streams know that the test run is
              # complete.
              end_time_str = qm.common.format_time_iso(time.time())
*************** class ExecutionEngine:
*** 178,327 ****
          The execution engine will periodically monitor 'fd'.  When input
          is available, it will call 'function' passing it 'fd'."""
  
          self.__input_handlers[fd] = function
          
!     
      def _RunTests(self):
-         """Run all of the tests.
  
!         This function assumes that the targets have already been
!         started.
  
!         The tests are run in the order that they were presented --
!         modulo requirements regarding prerequisites and any
!         nondeterminism introduced by running tests in parallel."""
! 
!         # Create a directed graph where each node is a pair
!         # (count, descriptor).  There is an edge from one node
!         # to another if the first node is a prerequisite for the
!         # second.  Begin by creating the nodes of the graph.
          for id in self.__test_ids:
!             try:
!                 descriptor = self.__database.GetTest(id)
!                 self.__descriptors[id] = descriptor
!                 self.__descriptor_graph[descriptor] = [0, []]
!                 self.__pending.append(descriptor)
!             except:
!                 result = Result(Result.TEST, id)
!                 result.NoteException(cause = "Could not load test.",
!                                      outcome = Result.UNTESTED)
!                 self._AddResult(result)
!                 
!         # Create the edges.
!         for descriptor in self.__pending:
!             prereqs = descriptor.GetPrerequisites()
!             if prereqs:
!                 for (prereq_id, outcome) in prereqs.items():
!                     if not self.__descriptors.has_key(prereq_id):
!                         # The prerequisite is not amongst the list of
!                         # tests to run.  In that case we do still run
!                         # the dependent test; it was explicitly
!                         # requested by the user.
!                         continue
!                     prereq_desc = self.__descriptors[prereq_id]
!                     self.__descriptor_graph[prereq_desc][1] \
!                         .append((descriptor, outcome))
!                     self.__descriptor_graph[descriptor][0] += 1
! 
!             if not self.__descriptor_graph[descriptor][0]:
!                 # A node with no prerequisites is ready.
!                 self.__ready.append(descriptor)
! 
!         # Iterate until there are no more tests to run.
!         while ((self.__pending or self.__ready)
!                and not self.IsTerminationRequested()):
!             # If there are no idle targets, block until we get a
!             # response.  There is nothing constructive we can do.
!             idle_targets = self.__idle_targets
!             if not idle_targets:
                  self._Trace("All targets are busy -- waiting.")
!                 # Read a reply from the response_queue.
!                 self._CheckForResponse(wait=1)
                  self._Trace("Response received.")
-                 # Keep going.
                  continue
  
!             # If there are no tests ready to run, but no tests are
!             # actually running at this time, we have
!             # a cycle in the dependency graph.  Pull the head off the
!             # pending queue and mark it UNTESTED, see if that helps.
!             if (not self.__ready and not self.__running):
!                 descriptor = self.__pending[0]
!                 self._Trace(("Dependency cycle, discarding %s."
!                              % descriptor.GetId()))
!                 self.__pending.remove(descriptor)
!                 self._AddUntestedResult(descriptor.GetId(),
!                                         qm.message("dependency cycle"))
!                 self._UpdateDependentTests(descriptor, Result.UNTESTED)
                  continue
  
!             # There is at least one idle target.  Try to find something
!             # that it can do.
!             wait = 1
!             for descriptor in self.__ready:
!                 for target in idle_targets:
!                     if target.IsInGroup(descriptor.GetTargetGroup()):
!                         # This test can be run on this target.  Remove
!                         # it from the ready list.
!                         self.__ready.remove(descriptor)
!                         # And from the pending list.
!                         try:
!                             self.__pending.remove(descriptor)
!                         except ValueError:
!                             # If the test is not pending, that means it
!                             # got pulled off for some reason
!                             # (e.g. breaking dependency cycles).  Don't
!                             # try to run it, it won't work.
!                             self._Trace(("Ready test %s not pending, skipped"
!                                          % descriptor.GetId()))
!                             wait = 0
!                             break
! 
!                         # Output a trace message.
!                         self._Trace(("About to run %s."
!                                      % descriptor.GetId()))
!                         # Run it.
!                         self.__running += 1
!                         target.RunTest(descriptor, self.__context)
!                         # If the target is no longer idle, remove it
!                         # from the idle_targets list.
!                         if not target.IsIdle():
!                             self._Trace("Target is no longer idle.")
!                             self.__idle_targets.remove(target)
!                         else:
!                             self._Trace("Target is still idle.")
!                         # We have done something useful on this
!                         # iteration.
!                         wait = 0
!                         break
  
!                 if not wait:
                      break
  
!             # Output a trace message.
!             self._Trace("About to check for a response in %s mode."
!                         % ((wait and "blocking") or "nonblocking"))
!                     
!             # See if any targets have finished their assignments.  If
!             # we did not schedule any additional work during this
!             # iteration of the loop, there's no point in continuing
!             # until some target finishes what it's doing.
!             self._CheckForResponse(wait=wait)
  
              # Output a trace message.
!             self._Trace("Done checking for responses.")
  
!         # Any tests that are still pending are untested, unless there
!         # has been an explicit request that we exit immediately.
!         if not self.IsTerminationRequested():
!             for descriptor in self.__pending:
!                 self._AddUntestedResult(descriptor.GetId(),
!                                         qm.message("execution terminated"))
  
  
!     def _CheckForResponse(self, wait):
          """See if any of the targets have completed a task.
  
          'wait' -- If false, this function returns immediately if there
          is no available response.  If 'wait' is true, this function
          continues to wait until a response is available.
--- 287,689 ----
          The execution engine will periodically monitor 'fd'.  When input
          is available, it will call 'function' passing it 'fd'."""
  
          self.__input_handlers[fd] = function
          
! 
      def _RunTests(self):
  
!         num_tests = len(self.__test_ids)
  
!         # No tests have been started yet.
!         self.__num_tests_started = 0
! 
!         self.__tests_iterator = iter(self.__test_ids)
! 
!         # A map from the tests we are supposed to run to their current
!         # status.
!         self.__statuses = {}
          for id in self.__test_ids:
!             self.__statuses[id] = self.__TestStatus()
! 
!         # A stack of tests.  If a test has prerequisites, the
!         # prerequisites will appear nearer to the top of the stack.
!         self.__test_stack = []
!         # A hash-table giving the names of the tests presently on the
!         # stack.  The names are the keys; the values are unused.
!         self.__ids_on_stack = {}
! 
!         # Every target is in one of three states: busy, idle, or
!         # starving.  A busy target is running tests, an idle target is
!         # ready to run tests, and a starving target is ready to run
!         # tests, but no tests are available for it to run.  The value
!         # recorded in the table is 'None' for a starving target, true
!         # for an idle target, and false for a busy target.
!         self.__target_state = {}
!         for target in self.__targets:
!             self.__target_state[target] = 1
!         # The total number of idle targets.
!         self.__num_idle_targets = len(self.__targets)
!         
!         # Figure out what target groups are available.
!         self.__target_groups = {}
!         for target in self.__targets:
!             self.__target_groups[target.GetGroup()] = None
!         self.__target_groups = self.__target_groups.keys()
!         
!         # A hash-table indicating whether or not a particular target
!         # pattern is matched by any of our targets.
!         self.__pattern_ok = {}
!         # A map from target groups to patterns satisfied by the group.
!         self.__patterns = {}
!         # A map from target patterns to lists of test descriptors ready
!         # to run.
!         self.__target_pattern_queues = {}
!         
!         while self.__num_tests_started < num_tests:
!             # Process any responses and update the count of idle targets.
!             while self.__CheckForResponse(wait=0):
!                 pass
! 
!             # Now look for idle targets.
!             if self.__num_idle_targets == 0:
!                 # Block until one of the running tests completes.
                  self._Trace("All targets are busy -- waiting.")
!                 self.__CheckForResponse(wait=1)
                  self._Trace("Response received.")
                  continue
  
!             # Go through each of the idle targets, finding work for it
!             # to do.
!             self.__num_idle_targets = 0
!             for target in self.__targets:
!                 if self.__target_state[target] != 1:
!                     continue
!                 # Try to find work for the target.  If there is no
!                 # available work, the target is starving.
!                 if not self.__FeedTarget(target):
!                     self.__target_state[target] = None
!                 else:
!                     is_idle = target.IsIdle()
!                     self.__target_state[target] = is_idle
!                     if is_idle:
!                         self.__num_idle_targets += 1
! 
!         # Now all tests have been started; we just have wait for them
!         # all to finish.
!         while self.__running:
!             self.__CheckForResponse(wait=1)
! 
! 
!     def __FeedTarget(self, target):
!         """Run a test on 'target'
! 
!         'target' -- The 'Target' on which the test should be run.
! 
!         returns -- True, iff a test could be found to run on 'target'.
!         False otherwise."""
! 
!         self._Trace("Looking for a test for target %s" % target.GetName())
! 
!         descriptor = None
! 
!         # See if there is already a ready-to-run test for this target.
!         for pattern in self.__patterns.get(target.GetGroup(), []):
!             tests = self.__target_pattern_queues.get(pattern, [])
!             if tests:
!                 descriptor = tests.pop()
!                 break
! 
!         # If there is no ready test, find one.
!         descriptor = self.__FindRunnableTest(target)
!         if descriptor is None:
!             # There are no more tests ready to run.
!             return 0
!                 
!         target_name = target.GetName()
!         test_id = descriptor.GetId()
!         self._Trace("Running %s on %s" % (test_id, target_name))
!         assert self.__statuses[test_id].GetState() == self.__TestStatus.READY
!         self.__num_tests_started += 1
!         self.__running += 1
!         target.RunTest(descriptor, self.__context)
!         return 1
! 
! 
!     def __FindRunnableTest(self, target):
!         """Return a test that is ready to run.
! 
!         'target' -- The 'Target' on which the test will run.
!         
!         returns -- the 'TestDescriptor' for the next available ready
!         test, or 'None' if no test could be found that will run on
!         'target'.
! 
!         If a test with unsatisfied prerequisites is encountered, the
!         test will be pushed on the stack and the prerequisites processed
!         recursively."""
! 
!         while 1:
!             if not self.__test_stack:
!                 # We ran out of prerequisite tests, so pull a new one
!                 # off the user's list.
!                 try:
!                     test_id = self.__tests_iterator.next()
!                 except StopIteration:
!                     # We're entirely out of fresh tests; give up.
!                     return None
!                 if self.__statuses[test_id].HasBeenQueued():
!                     # This test has already been handled (probably
!                     # because it's a prereq of a test already seen).
!                     continue
!                 # Try to add the new test to the stack.
!                 if not self.__AddTestToStack(test_id):
!                     # If that failed, look for another test.
!                     continue
!                 self._Trace("Added new test %s to stack" % test_id)
! 
!             descriptor, prereqs = self.__test_stack[-1]
!             # First look at the listed prereqs.
!             if prereqs:
!                 new_test_id = prereqs.pop()
!                 # We must filter tests that are already in the process
!                 # here; if we were to do it earlier, we would be in
!                 # danger of being confused by dependency graphs like
!                 # A->B, A->C, B->C, where we can't know ahead of time
!                 # that A's dependence on C is unnecessary.
!                 if self.__statuses[new_test_id].HasBeenQueued():
!                     # This one is already in process.  This is also what
!                     # a dependency cycle looks like, so check for that
!                     # now.
!                     if new_test_id in self.__ids_on_stack:
!                         self._Trace("Cycle detected (%s)"
!                                     % (new_test_id,))
!                         self.__AddUntestedResult \
!                                  (new_test_id,
!                                   qm.message("dependency cycle"))
!                     continue
!                 else:
!                     self.__AddTestToStack(new_test_id)
!                     continue
!             else:
!                 # Remove the test from the stack.
!                 test_id = descriptor.GetId()
!                 del self.__ids_on_stack[test_id]
!                 self.__test_stack.pop()
! 
!                 # Check to see if the test is already ready to run, or
!                 # has completed.  The first case occurs when the test
!                 # has prerequisites that have completed after it was
!                 # placed on the stack; the second occurs when a test
!                 # is marked UNTESTED after a cycle is detected.
!                 if self.__statuses[test_id].HasBeenReady():
!                     continue
! 
!                 # Now check the prerequisites.
!                 prereqs = self.__GetPendingPrerequisites(descriptor)
!                 # If one of the prerequisites failed, the test will have
!                 # been marked UNTESTED.  Keep looking for a runnable
!                 # test.
!                 if prereqs is None:
!                     continue
!                 # If there are prerequisites, request notification when
!                 # they have completed.
!                 if prereqs:
!                     for p in prereqs:
!                         self.__statuses[p].NoteDependant(test_id)
!                     # Keep looking for a runnable test.                        
!                     continue
! 
!                 # This test is ready to run.  See if it can run on
!                 # target.
!                 if not target.IsInGroup(descriptor.GetTargetGroup()):
!                     # This test can't be run on this target, but it can be
!                     # run on another target.
!                     self.__AddToTargetPatternQueue(descriptor)
!                     continue
!                     
!                 self.__statuses[descriptor.GetId()].NoteReady()
!                 return descriptor
! 
! 
!     def __AddTestToStack(self, test_id):
!         """Adds 'test_id' to the stack of current tests.
! 
!         returns -- True if the test was added to the stack; false if the
!         test could not be loaded.  In the latter case, an 'UNTESTED'
!         result is recorded for the test."""
!         
!         self._Trace("Trying to add %s to stack" % test_id)
! 
!         # Update test status.
!         self.__statuses[test_id].NoteQueued()
! 
!         # Load the descriptor.
!         descriptor = self.__GetTestDescriptor(test_id)
!         if not descriptor:
!             return 0
! 
!         # Ignore prerequisites that are not going to be run at all.
!         prereqs_iter = iter(descriptor.GetPrerequisites())
!         relevant_prereqs = filter(self.__statuses.has_key, prereqs_iter)
! 
!         # Store the test on the stack.
!         self.__ids_on_stack[test_id] = None
!         self.__test_stack.append((descriptor, relevant_prereqs))
! 
!         return 1
! 
!         
!     def __AddToTargetPatternQueue(self, descriptor):
!         """A a test to the appropriate target pattern queue.
! 
!         'descriptor' -- A 'TestDescriptor'.
! 
!         Adds the test to the target pattern queue indicated in the
!         descriptor."""
! 
!         test_id = descriptor.GetId()
!         self.__statuses[test_id].NoteReady()
! 
!         pattern = descriptor.GetTargetGroup()
! 
!         # If we have not already determined whether or not this pattern
!         # matches any of the targets, do so now.
!         if not self.__pattern_ok.has_key(pattern):
!             self.__pattern_ok[pattern] = 0
!             for group in self.__target_groups:
!                 if re.match(pattern, group):
!                     self.__pattern_ok[group] = 1
!                     patterns = self.__patterns.setdefault(group, [])
!                     patterns.append(pattern)
!         # If none of the targets can run this test, mark it untested.
!         if not self.__pattern_ok[pattern]:
!             self.__AddUntestedResult(test_id,
!                                      "No target matching %s." % pattern)
!             return
! 
!         queue = self.__target_pattern_queues.setdefault(pattern, [])
!         queue.append(descriptor)
! 
! 
!     def __GetPendingPrerequisites(self, descriptor):
!         """Return pending prerequisite tests for 'descriptor'.
! 
!         'descriptor' -- A 'TestDescriptor'.
!         
!         returns -- A list of prerequisite test ids that have to
!         complete, or 'None' if one of the prerequisites had an
!         unexpected outcome."""
! 
!         needed = []
! 
!         prereqs = descriptor.GetPrerequisites()
!         for prereq_id, outcome in prereqs.iteritems():
!             try:
!                 prereq_status = self.__statuses[prereq_id]
!             except KeyError:
!                 # This prerequisite is not being run at all.
                  continue
  
!             if prereq_status.IsFinished():
!                 prereq_outcome = prereq_status.outcome
!                 if outcome != prereq_outcome:
!                     # Failed prerequisite.
!                     self.__AddUntestedResult \
!                         (descriptor.GetId(),
!                          qm.message("failed prerequisite"),
!                          {'qmtest.prequisite': prereq_id,
!                           'qmtest.outcome': prereq_outcome,
!                           'qmtest.expected_outcome': outcome })
!                     return None
!             else:
!                 # This prerequisite has not yet completed.
!                 needed.append(prereq_id)
  
!         return needed
! 
! 
!     def __AddResult(self, result):
!         """Report the result of running a test or resource.
!         
!         'result' -- A 'Result' object representing the result of running
!         a test or resource."""
! 
!         # Output a trace message.
!         id = result.GetId()
!         self._Trace("Recording %s result for %s." % (result.GetKind(), id))
! 
!         # Find the target with the name indicated in the result.
!         if result.has_key(Result.TARGET):
!             for target in self.__targets:
!                 if target.GetName() == result[Result.TARGET]:
                      break
+             else:
+                 assert 0, ("No target %s exists (test id: %s)"
+                            % (result[Result.TARGET], id))
+         else:
+             # Not all results will have associated targets.  If the
+             # test was not run at all, there will be no associated
+             # target.
+             target = None
  
!         # Having no target is a rare occurrence; output a trace message.
!         if not target:
!             self._Trace("No target for %s." % id)
  
+         # This target might now be idle.
+         if (target and target.IsIdle()):
              # Output a trace message.
!             self._Trace("Target is now idle.\n")
!             self.__target_state[target] = 1
!             self.__num_idle_targets += 1
!             
!         # Only tests have expectations or scheduling dependencies.
!         if result.GetKind() == Result.TEST:
!             # Record the outcome for this test.
!             test_status = self.__statuses[id]
!             test_status.outcome = result.GetOutcome()
! 
!             # If there were tests waiting for this one to complete, they
!             # may now be ready to execute.
!             if test_status.dependants:
!                 for dependant in test_status.dependants:
!                     if not self.__statuses[dependant].HasBeenReady():
!                         descriptor = self.__GetTestDescriptor(dependant)
!                         if not descriptor:
!                             continue
!                         prereqs = self.__GetPendingPrerequisites(descriptor)
!                         if prereqs is None:
!                             continue
!                         if not prereqs:
!                             # All prerequisites ran and were satisfied.
!                             # This test can now run.
!                             self.__AddToTargetPatternQueue(descriptor)
!                 # Free the memory consumed by the list.
!                 del test_status.dependants
! 
!             # Check for unexpected outcomes.
!             if result.GetKind() == Result.TEST:
!                 if (self.__expectations.get(id, Result.PASS)
!                     != result.GetOutcome()):
!                     self.__any_unexpected_outcomes = 1
! 
!             # Any targets that were starving may now be able to find
!             # work.
!             for t in self.__targets:
!                 if self.__target_state[t] is None:
!                     self.__target_state[t] = 1
!             
!         # Output a trace message.
!         self._Trace("Writing result for %s to streams." % id)
  
!         # Report the result.
!         for rs in self.__result_streams:
!             rs.WriteResult(result)
  
  
!     def __CheckForResponse(self, wait):
          """See if any of the targets have completed a task.
  
          'wait' -- If false, this function returns immediately if there
          is no available response.  If 'wait' is true, this function
          continues to wait until a response is available.
*************** class ExecutionEngine:
*** 333,356 ****
                  # Read a reply from the response_queue.
                  result = self.__response_queue.get(0)
                  # Output a trace message.
                  self._Trace("Got %s result for %s from queue."
                               % (result.GetKind(), result.GetId()))
!                 # Handle it.
!                 self._AddResult(result)
                  if result.GetKind() == Result.TEST:
                      assert self.__running > 0
                      self.__running -= 1
                  # Output a trace message.
                  self._Trace("Recorded result.")
-                 # If this was a test result, there may be other tests that
-                 # are now eligible to run.
-                 if result.GetKind() == Result.TEST:
-                     # Get the descriptor for this test.
-                     descriptor = self.__descriptors[result.GetId()]
-                     # Iterate through each of the dependent tests.
-                     self._UpdateDependentTests(descriptor, result.GetOutcome())
                  return result
              except qm.queue.Empty:
                  # If there is nothing in the queue, then this exception will
                  # be thrown.
                  if not wait:
--- 695,711 ----
                  # Read a reply from the response_queue.
                  result = self.__response_queue.get(0)
                  # Output a trace message.
                  self._Trace("Got %s result for %s from queue."
                               % (result.GetKind(), result.GetId()))
!                 # Record the result.
!                 self.__AddResult(result)
                  if result.GetKind() == Result.TEST:
                      assert self.__running > 0
                      self.__running -= 1
                  # Output a trace message.
                  self._Trace("Recorded result.")
                  return result
              except qm.queue.Empty:
                  # If there is nothing in the queue, then this exception will
                  # be thrown.
                  if not wait:
*************** class ExecutionEngine:
*** 369,492 ****
                  
                  # There may be a response now.
                  continue
  
  
!     def _UpdateDependentTests(self, descriptor, outcome):
!         """Update the status of tests that depend on 'node'.
! 
!         'descriptor' -- A test descriptor.
! 
!         'outcome' -- The outcome associated with the test.
! 
!         If tests that depend on 'descriptor' required a particular
!         outcome, and 'outcome' is different, mark them as untested.  If
!         tests that depend on 'descriptor' are now eligible to run, add
!         them to the '__ready' queue."""
! 
!         node = self.__descriptor_graph[descriptor]
!         for (d, o) in node[1]:
!             # Find the node for the dependent test.
!             n = self.__descriptor_graph[d]
!             # If some other prerequisite has already had an undesired
!             # outcome, there is nothing more to do.
!             if n[0] == 0:
!                 continue
  
!             # If the actual outcome is not the outcome that was
!             # expected, the dependent test cannot be run.
!             if outcome != o:
!                 try:
!                     # This test will never be run.
!                     n[0] = 0
!                     self.__pending.remove(d)
!                     # Mark it untested.
!                     self._AddUntestedResult(d.GetId(),
!                                             qm.message("failed prerequisite"),
!                                             { 'qmtest.prequisite' :
!                                               descriptor.GetId(),
!                                               'qmtest.outcome' : outcome,
!                                               'qmtest.expected_outcome' : o })
!                     # Recursively remove tests that depend on d.
!                     self._UpdateDependentTests(d, Result.UNTESTED)
!                 except ValueError:
!                     # This test has already been taken off the pending queue;
!                     # assume a result has already been recorded.  This can
!                     # happen when we're breaking dependency cycles.
!                     pass
!             else:
!                 # Decrease the count associated with the node, if
!                 # the test has not already been declared a failure.
!                 n[0] -= 1
!                 # If this was the last prerequisite, this test
!                 # is now ready.
!                 if n[0] == 0:
!                     self.__ready.append(d)
!                     
!     
!     def _AddResult(self, result):
!         """Report the result of running a test or resource.
  
!         'result' -- A 'Result' object representing the result of running
!         a test or resource."""
  
!         # Output a trace message.
!         self._Trace("Recording %s result for %s."
!                     % (result.GetKind(), result.GetId()))
  
!         # Find the target with the name indicated in the result.
!         if result.has_key(Result.TARGET):
!             for target in self.__targets:
!                 if target.GetName() == result[Result.TARGET]:
!                     break
          else:
!             # Not all results will have associated targets.  If the
!             # test was not run at all, there will be no associated
!             # target.
!             target = None
! 
!         # Having no target is a rare occurrence; output a trace message.
!         if not target:
!             self._Trace("No target for %s." % result.GetId())
!                         
!         # Check for unexpected outcomes.
!         if result.GetKind() == Result.TEST  \
!            and (self.__expectations.get(result.GetId(), Result.PASS)
!                 != result.GetOutcome()):
!             self.__any_unexpected_outcomes = 1
!             
!         # This target might now be idle.
!         if (target and target not in self.__idle_targets
!             and target.IsIdle()):
!             # Output a trace message.
!             self._Trace("Target is now idle.\n")
!             self.__idle_targets.append(target)
! 
!         # Output a trace message.
!         self._Trace("Writing result for %s to streams." % result.GetId())
! 
!         # Report the result.
!         for rs in self.__result_streams:
!             rs.WriteResult(result)
! 
! 
!     def _AddUntestedResult(self, test_name, cause, annotations={}):
!         """Add a 'Result' indicating that 'test_name' was not run.
  
-         'test_name' -- The label for the test that could not be run.
  
!         'cause' -- A string explaining why the test could not be run.
  
!         'annotations' -- A map from strings to strings giving
!         additional annotations for the result."""
  
!         # Create the result.
!         result = Result(Result.TEST, test_name, Result.UNTESTED, annotations)
!         result[Result.CAUSE] = cause
!         self._AddResult(result)
  
  
      def _Trace(self, message):
          """Write a trace 'message'.
  
          'message' -- A string to be output as a trace message."""
  
--- 724,780 ----
                  
                  # There may be a response now.
                  continue
  
  
!     def __AddUntestedResult(self, test_name, cause, annotations={},
!                             exc_info = None):
!         """Add a 'Result' indicating that 'test_name' was not run.
  
!         'test_name' -- The label for the test that could not be run.
  
!         'cause' -- A string explaining why the test could not be run.
  
!         'annotations' -- A map from strings to strings giving
!         additional annotations for the result.
  
!         'exc_info' -- If this test could not be tested due to a thrown
!         exception, 'exc_info' is the result of 'sys.exc_info()' when the
!         exception was caught.  'None' otherwise."""
! 
!         # Remember that this test was started.
!         self.__num_tests_started += 1
! 
!         # Create and record the result.
!         result = Result(Result.TEST, test_name, annotations = annotations)
!         if exc_info:
!             result.NoteException(exc_info, cause, Result.UNTESTED)
          else:
!             result.SetOutcome(Result.UNTESTED, cause)
!         self.__AddResult(result)
  
  
!     ### Utility methods.
  
!     def __GetTestDescriptor(self, test_id):
!         """Return the 'TestDescriptor' for 'test_id'.
  
!         returns -- The 'TestDescriptor' for 'test_id', or 'None' if the
!         descriptor could not be loaded.
  
+         If the database cannot load the descriptor, an 'UNTESTED' result
+         is recorded for 'test_id'."""
  
+         try:
+             return self.__database.GetTest(test_id)
+         except:
+             self.__AddUntestedResult(test_id,
+                                      "Could not load test.",
+                                      exc_info = sys.exc_info())
+             return None
+         
+         
      def _Trace(self, message):
          """Write a trace 'message'.
  
          'message' -- A string to be output as a trace message."""
  
Index: qm/test/classes/text_result_stream.py
===================================================================
RCS file: /home/sc/Repository/qm/qm/test/classes/text_result_stream.py,v
retrieving revision 1.1
diff -c -5 -p -r1.1 text_result_stream.py
*** qm/test/classes/text_result_stream.py	16 Jun 2003 23:45:51 -0000	1.1
--- qm/test/classes/text_result_stream.py	31 Jul 2003 23:12:48 -0000
*************** class TextResultStream(FileResultStream)
*** 79,90 ****
              gives details about any tests with unexpected outcomes.
  
              The "full" format is like "brief" except that all
              annotations are shown for tests as they are run.
  
!             The "stats" format is omits the failing tests section is
!             omitted."""),
          ]
      
      def __init__(self, arguments):
          """Construct a 'TextResultStream'.
  
--- 79,89 ----
              gives details about any tests with unexpected outcomes.
  
              The "full" format is like "brief" except that all
              annotations are shown for tests as they are run.
  
!             The "stats" format omits the failing tests section."""),
          ]
      
      def __init__(self, arguments):
          """Construct a 'TextResultStream'.
  
Index: qm/test/doc/tour.xml
===================================================================
RCS file: /home/sc/Repository/qm/qm/test/doc/tour.xml,v
retrieving revision 1.6
diff -c -5 -p -r1.6 tour.xml
*** qm/test/doc/tour.xml	13 May 2003 07:10:47 -0000	1.6
--- qm/test/doc/tour.xml	31 Jul 2003 23:12:48 -0000
*************** QMTest running at http://127.0.0.1:1158/
*** 253,269 ****
     <guibutton>OK</guibutton> button at the bottom of the page to save
     your changes.  Choose <guibutton>This Test</guibutton> from the
     <guibutton>Run</guibutton> menu and observe that the test now
     passes.</para>
  
!    <para>Creating a new test works in a similar way.  Click on the
!    <guilabel>Home</guilabel> link to return to the main &qmtest; page.
!    Then, select <guibutton>New Test</guibutton> from the
!    <guilabel>File</guilabel> menu to create a new test.  &qmtest;
!    displays a form that contains two fields: the test name, and the
!    test class.  The test name identifies the test; the test class
!    indicates what kind of test will be created.</para>
  
     <para>Test names must be composed entirely of lowercase letters,
     numbers, the <quote>_</quote> character, and the <quote>.</quote>
     character.  You can think of test names like file names.  The
     <quote>.</quote> character takes the place of <quote>/</quote> on
--- 253,270 ----
     <guibutton>OK</guibutton> button at the bottom of the page to save
     your changes.  Choose <guibutton>This Test</guibutton> from the
     <guibutton>Run</guibutton> menu and observe that the test now
     passes.</para>
  
!    <para>Creating a new test works in a similar way.  Choose
!    <guilabel>Directory</guilabel> under the <guilabel>View</guilabel>
!    menu to return to the main &qmtest; page.  Then, select
!    <guibutton>New Test</guibutton> from the <guilabel>File</guilabel>
!    menu to create a new test.  &qmtest; displays a form that contains
!    two fields: the test name, and the test class.  The test name
!    identifies the test; the test class indicates what kind of test
!    will be created.</para>
  
     <para>Test names must be composed entirely of lowercase letters,
     numbers, the <quote>_</quote> character, and the <quote>.</quote>
     character.  You can think of test names like file names.  The
     <quote>.</quote> character takes the place of <quote>/</quote> on
*************** QMTest running at http://127.0.0.1:1158/
*** 279,312 ****
     to run a group of related tests at once.</para>
  
     <para>Enter <filename>command.test1</filename> for the test name.
     This will create a new test named <filename>test1</filename> in the
     <filename>command</filename> directory.  Choose
!    <classname>command.ExecTest</classname> as the test class.  This
!    kind of test runs a command and compares its actual output against
!    the expected output.  If they match, the test passes.  This test
!    class is useful for testing many programs.  Click on the
     <guibutton>Next</guibutton> button to continue.</para>
     
     <para>Now, &qmtest; will present you with a form that looks just
     like the form you used to edit <filename>exec1</filename>, except
     that the arguments are different.  The arguments are different
     because you're creating a different kind of test.  Enter
!    <literal>echo</literal> in the <guilabel>Program</guilabel> field.
!    Click on the <guibutton>Add Another</guibutton> button to add a program
!    argument and enter <literal>test</literal> in the box that appears.
!    At this point, you've told qmtest that you want to run the command
!    <command>echo test</command>.  This command will produce an output
!    (the word <literal>test</literal>) as output, so find the
!    <guilabel>Standard Output</guilabel> box and enter
!    <literal>test</literal> in this box.  Make sure to hit the
!    <keycap>Return</keycap> key after you type <literal>test</literal>;
!    the <command>echo</command> command will output a carriage return
!    after it prints the word <literal>test</literal>, so you must
!    indicate that you expect a carriage return.  When you are done,
!    click the <guibutton>OK</guibutton> button at the bottom of the
!    form.</para>
  
     <para>Now you can select <guibutton>This Test</guibutton> from the
     <guilabel>Run</guilabel> menu to run the test.</para>
  
     <para>When you're done experimenting with &qmtest, choose
--- 280,309 ----
     to run a group of related tests at once.</para>
  
     <para>Enter <filename>command.test1</filename> for the test name.
     This will create a new test named <filename>test1</filename> in the
     <filename>command</filename> directory.  Choose
!    <classname>command.ShellCommandTest</classname> as the test class.
!    This kind of test runs a command and compares its actual output
!    against the expected output.  If they match, the test passes.  This
!    test class is useful for testing many programs.  Click on the
     <guibutton>Next</guibutton> button to continue.</para>
     
     <para>Now, &qmtest; will present you with a form that looks just
     like the form you used to edit <filename>exec1</filename>, except
     that the arguments are different.  The arguments are different
     because you're creating a different kind of test.  Enter
!    <literal>echo test</literal> in the <guilabel>Command</guilabel>
!    field.  This command will produce an output (the word
!    <literal>test</literal>), so find the <guilabel>Standard
!    Output</guilabel> box and enter <literal>test</literal> in this
!    box.  Make sure to hit the <keycap>Return</keycap> key after you
!    type <literal>test</literal>; the <command>echo</command> command
!    will output a carriage return after it prints the word
!    <literal>test</literal>, so you must indicate that you expect a
!    carriage return.  When you are done, click the
!    <guibutton>OK</guibutton> button at the bottom of the form.</para>
  
     <para>Now you can select <guibutton>This Test</guibutton> from the
     <guilabel>Run</guilabel> menu to run the test.</para>
  
     <para>When you're done experimenting with &qmtest, choose
Index: tests/regress/bad_target1/a.qmt
===================================================================
RCS file: tests/regress/bad_target1/a.qmt
diff -N tests/regress/bad_target1/a.qmt
*** /dev/null	1 Jan 1970 00:00:00 -0000
--- tests/regress/bad_target1/a.qmt	31 Jul 2003 23:12:48 -0000
***************
*** 0 ****
--- 1,5 ----
+ <?xml version="1.0" ?>
+ <!DOCTYPE extension
+   PUBLIC '-//Software Carpentry//QMTest Extension V0.1//EN'
+   'http://www.software-carpentry.com/qm/xml/extension'>
+ <extension class="python.ExecTest" kind="test"><argument name="prerequisites"><set/></argument><argument name="source"><text>pass</text></argument><argument name="target_group"><text>.*</text></argument><argument name="expression"><text>1</text></argument><argument name="resources"><set/></argument></extension>
\ No newline at end of file
Index: tests/regress/bad_target1/bad_target.qmt
===================================================================
RCS file: tests/regress/bad_target1/bad_target.qmt
diff -N tests/regress/bad_target1/bad_target.qmt
*** /dev/null	1 Jan 1970 00:00:00 -0000
--- tests/regress/bad_target1/bad_target.qmt	31 Jul 2003 23:12:48 -0000
***************
*** 0 ****
--- 1,5 ----
+ <?xml version="1.0" ?>
+ <!DOCTYPE extension
+   PUBLIC '-//Software Carpentry//QMTest Extension V0.1//EN'
+   'http://www.software-carpentry.com/qm/xml/extension'>
+ <extension class="python.ExecTest" kind="test"><argument name="prerequisites"><set/></argument><argument name="source"><text>pass</text></argument><argument name="target_group"><text>$^</text></argument><argument name="expression"><text>1</text></argument><argument name="resources"><set/></argument></extension>
\ No newline at end of file
Index: tests/regress/bad_target1/results.qmr
===================================================================
RCS file: tests/regress/bad_target1/results.qmr
diff -N tests/regress/bad_target1/results.qmr
Binary files /dev/null and results.qmr differ
Index: tests/regress/bad_target1/QMTest/configuration
===================================================================
RCS file: tests/regress/bad_target1/QMTest/configuration
diff -N tests/regress/bad_target1/QMTest/configuration
*** /dev/null	1 Jan 1970 00:00:00 -0000
--- tests/regress/bad_target1/QMTest/configuration	31 Jul 2003 23:12:48 -0000
***************
*** 0 ****
--- 1,5 ----
+ <?xml version="1.0" ?>
+ <!DOCTYPE extension
+   PUBLIC '-//Software Carpentry//QMTest Extension V0.1//EN'
+   'http://www.software-carpentry.com/qm/xml/extension'>
+ <extension class="xml_database.XMLDatabase" kind="database"/>
\ No newline at end of file
Index: tests/regress/bad_target2/a.qmt
===================================================================
RCS file: tests/regress/bad_target2/a.qmt
diff -N tests/regress/bad_target2/a.qmt
*** /dev/null	1 Jan 1970 00:00:00 -0000
--- tests/regress/bad_target2/a.qmt	31 Jul 2003 23:12:48 -0000
***************
*** 0 ****
--- 1,5 ----
+ <?xml version="1.0" ?>
+ <!DOCTYPE extension
+   PUBLIC '-//Software Carpentry//QMTest Extension V0.1//EN'
+   'http://www.software-carpentry.com/qm/xml/extension'>
+ <extension class="python.ExecTest" kind="test"><argument name="prerequisites"><set><tuple><text>bad_target</text><enumeral>PASS</enumeral></tuple></set></argument><argument name="source"><text>pass</text></argument><argument name="target_group"><text>.*</text></argument><argument name="expression"><text>1</text></argument><argument name="resources"><set/></argument></extension>
Index: tests/regress/bad_target2/bad_target.qmt
===================================================================
RCS file: tests/regress/bad_target2/bad_target.qmt
diff -N tests/regress/bad_target2/bad_target.qmt
*** /dev/null	1 Jan 1970 00:00:00 -0000
--- tests/regress/bad_target2/bad_target.qmt	31 Jul 2003 23:12:48 -0000
***************
*** 0 ****
--- 1,5 ----
+ <?xml version="1.0" ?>
+ <!DOCTYPE extension
+   PUBLIC '-//Software Carpentry//QMTest Extension V0.1//EN'
+   'http://www.software-carpentry.com/qm/xml/extension'>
+ <extension class="python.ExecTest" kind="test"><argument name="prerequisites"><set/></argument><argument name="source"><text>pass</text></argument><argument name="target_group"><text>$^</text></argument><argument name="expression"><text>1</text></argument><argument name="resources"><set/></argument></extension>
\ No newline at end of file
Index: tests/regress/bad_target2/results.qmr
===================================================================
RCS file: tests/regress/bad_target2/results.qmr
diff -N tests/regress/bad_target2/results.qmr
Binary files /dev/null and results.qmr differ
Index: tests/regress/bad_target2/QMTest/configuration
===================================================================
RCS file: tests/regress/bad_target2/QMTest/configuration
diff -N tests/regress/bad_target2/QMTest/configuration
*** /dev/null	1 Jan 1970 00:00:00 -0000
--- tests/regress/bad_target2/QMTest/configuration	31 Jul 2003 23:12:48 -0000
***************
*** 0 ****
--- 1,5 ----
+ <?xml version="1.0" ?>
+ <!DOCTYPE extension
+   PUBLIC '-//Software Carpentry//QMTest Extension V0.1//EN'
+   'http://www.software-carpentry.com/qm/xml/extension'>
+ <extension class="xml_database.XMLDatabase" kind="database"/>
\ No newline at end of file
Index: tests/regress/nocycle1/a.qmt
===================================================================
RCS file: tests/regress/nocycle1/a.qmt
diff -N tests/regress/nocycle1/a.qmt
*** /dev/null	1 Jan 1970 00:00:00 -0000
--- tests/regress/nocycle1/a.qmt	31 Jul 2003 23:12:48 -0000
***************
*** 0 ****
--- 1,5 ----
+ <?xml version="1.0" ?>
+ <!DOCTYPE extension
+   PUBLIC '-//Software Carpentry//QMTest Extension V0.1//EN'
+   'http://www.software-carpentry.com/qm/xml/extension'>
+ <extension class="python.ExecTest" kind="test"><argument name="prerequisites"><set><tuple><text>b</text><enumeral>PASS</enumeral></tuple><tuple><text>c</text><enumeral>PASS</enumeral></tuple></set></argument><argument name="source"><text>pass</text></argument><argument name="target_group"><text>.*</text></argument><argument name="expression"><text>1</text></argument><argument name="resources"><set/></argument></extension>
Index: tests/regress/nocycle1/b.qmt
===================================================================
RCS file: tests/regress/nocycle1/b.qmt
diff -N tests/regress/nocycle1/b.qmt
*** /dev/null	1 Jan 1970 00:00:00 -0000
--- tests/regress/nocycle1/b.qmt	31 Jul 2003 23:12:48 -0000
***************
*** 0 ****
--- 1,5 ----
+ <?xml version="1.0" ?>
+ <!DOCTYPE extension
+   PUBLIC '-//Software Carpentry//QMTest Extension V0.1//EN'
+   'http://www.software-carpentry.com/qm/xml/extension'>
+ <extension class="python.ExecTest" kind="test"><argument name="prerequisites"><set><tuple><text>d</text><enumeral>PASS</enumeral></tuple></set></argument><argument name="source"><text>pass</text></argument><argument name="target_group"><text>.*</text></argument><argument name="expression"><text>1</text></argument><argument name="resources"><set/></argument></extension>
Index: tests/regress/nocycle1/c.qmt
===================================================================
RCS file: tests/regress/nocycle1/c.qmt
diff -N tests/regress/nocycle1/c.qmt
*** /dev/null	1 Jan 1970 00:00:00 -0000
--- tests/regress/nocycle1/c.qmt	31 Jul 2003 23:12:48 -0000
***************
*** 0 ****
--- 1,5 ----
+ <?xml version="1.0" ?>
+ <!DOCTYPE extension
+   PUBLIC '-//Software Carpentry//QMTest Extension V0.1//EN'
+   'http://www.software-carpentry.com/qm/xml/extension'>
+ <extension class="python.ExecTest" kind="test"><argument name="prerequisites"><set><tuple><text>d</text><enumeral>PASS</enumeral></tuple></set></argument><argument name="source"><text>pass</text></argument><argument name="target_group"><text>.*</text></argument><argument name="expression"><text>1</text></argument><argument name="resources"><set/></argument></extension>
Index: tests/regress/nocycle1/d.qmt
===================================================================
RCS file: tests/regress/nocycle1/d.qmt
diff -N tests/regress/nocycle1/d.qmt
*** /dev/null	1 Jan 1970 00:00:00 -0000
--- tests/regress/nocycle1/d.qmt	31 Jul 2003 23:12:48 -0000
***************
*** 0 ****
--- 1,5 ----
+ <?xml version="1.0" ?>
+ <!DOCTYPE extension
+   PUBLIC '-//Software Carpentry//QMTest Extension V0.1//EN'
+   'http://www.software-carpentry.com/qm/xml/extension'>
+ <extension class="python.ExecTest" kind="test"><argument name="prerequisites"><set><tuple><text>e</text><enumeral>PASS</enumeral></tuple></set></argument><argument name="source"><text>pass</text></argument><argument name="target_group"><text>.*</text></argument><argument name="expression"><text>1</text></argument><argument name="resources"><set/></argument></extension>
Index: tests/regress/nocycle1/e.qmt
===================================================================
RCS file: tests/regress/nocycle1/e.qmt
diff -N tests/regress/nocycle1/e.qmt
*** /dev/null	1 Jan 1970 00:00:00 -0000
--- tests/regress/nocycle1/e.qmt	31 Jul 2003 23:12:48 -0000
***************
*** 0 ****
--- 1,6 ----
+ <?xml version="1.0" ?>
+ <!DOCTYPE extension
+   PUBLIC '-//Software Carpentry//QMTest Extension V0.1//EN'
+   'http://www.software-carpentry.com/qm/xml/extension'>
+ <extension class="python.ExecTest" kind="test"><argument name="prerequisites"><set/></argument><argument name="source"><text>import time
+ time.sleep(1)</text></argument><argument name="target_group"><text>.*</text></argument><argument name="expression"><text>1</text></argument><argument name="resources"><set/></argument></extension>
Index: tests/regress/nocycle1/results.qmr
===================================================================
RCS file: tests/regress/nocycle1/results.qmr
diff -N tests/regress/nocycle1/results.qmr
Binary files /dev/null and results.qmr differ
Index: tests/regress/nocycle1/QMTest/configuration
===================================================================
RCS file: tests/regress/nocycle1/QMTest/configuration
diff -N tests/regress/nocycle1/QMTest/configuration
*** /dev/null	1 Jan 1970 00:00:00 -0000
--- tests/regress/nocycle1/QMTest/configuration	31 Jul 2003 23:12:48 -0000
***************
*** 0 ****
--- 1,5 ----
+ <?xml version="1.0" ?>
+ <!DOCTYPE extension
+   PUBLIC '-//Software Carpentry//QMTest Extension V0.1//EN'
+   'http://www.software-carpentry.com/qm/xml/extension'>
+ <extension class="xml_database.XMLDatabase" kind="database"/>
\ No newline at end of file
Index: tests/regress/nocycle2/a.qmt
===================================================================
RCS file: tests/regress/nocycle2/a.qmt
diff -N tests/regress/nocycle2/a.qmt
*** /dev/null	1 Jan 1970 00:00:00 -0000
--- tests/regress/nocycle2/a.qmt	31 Jul 2003 23:12:48 -0000
***************
*** 0 ****
--- 1,5 ----
+ <?xml version="1.0" ?>
+ <!DOCTYPE extension
+   PUBLIC '-//Software Carpentry//QMTest Extension V0.1//EN'
+   'http://www.software-carpentry.com/qm/xml/extension'>
+ <extension class="python.ExecTest" kind="test"><argument name="prerequisites"><set><tuple><text>b</text><enumeral>PASS</enumeral></tuple><tuple><text>c</text><enumeral>PASS</enumeral></tuple></set></argument><argument name="source"><text>pass</text></argument><argument name="target_group"><text>.*</text></argument><argument name="expression"><text>1</text></argument><argument name="resources"><set/></argument></extension>
Index: tests/regress/nocycle2/b.qmt
===================================================================
RCS file: tests/regress/nocycle2/b.qmt
diff -N tests/regress/nocycle2/b.qmt
*** /dev/null	1 Jan 1970 00:00:00 -0000
--- tests/regress/nocycle2/b.qmt	31 Jul 2003 23:12:48 -0000
***************
*** 0 ****
--- 1,5 ----
+ <?xml version="1.0" ?>
+ <!DOCTYPE extension
+   PUBLIC '-//Software Carpentry//QMTest Extension V0.1//EN'
+   'http://www.software-carpentry.com/qm/xml/extension'>
+ <extension class="python.ExecTest" kind="test"><argument name="prerequisites"><set><tuple><text>c</text><enumeral>PASS</enumeral></tuple></set></argument><argument name="source"><text>pass</text></argument><argument name="target_group"><text>.*</text></argument><argument name="expression"><text>1</text></argument><argument name="resources"><set/></argument></extension>
Index: tests/regress/nocycle2/c.qmt
===================================================================
RCS file: tests/regress/nocycle2/c.qmt
diff -N tests/regress/nocycle2/c.qmt
*** /dev/null	1 Jan 1970 00:00:00 -0000
--- tests/regress/nocycle2/c.qmt	31 Jul 2003 23:12:48 -0000
***************
*** 0 ****
--- 1,5 ----
+ <?xml version="1.0" ?>
+ <!DOCTYPE extension
+   PUBLIC '-//Software Carpentry//QMTest Extension V0.1//EN'
+   'http://www.software-carpentry.com/qm/xml/extension'>
+ <extension class="python.ExecTest" kind="test"><argument name="prerequisites"><set/></argument><argument name="source"><text>pass</text></argument><argument name="target_group"><text>.*</text></argument><argument name="expression"><text>1</text></argument><argument name="resources"><set/></argument></extension>
Index: tests/regress/nocycle2/results.qmr
===================================================================
RCS file: tests/regress/nocycle2/results.qmr
diff -N tests/regress/nocycle2/results.qmr
Binary files /dev/null and results.qmr differ
Index: tests/regress/nocycle2/QMTest/configuration
===================================================================
RCS file: tests/regress/nocycle2/QMTest/configuration
diff -N tests/regress/nocycle2/QMTest/configuration
*** /dev/null	1 Jan 1970 00:00:00 -0000
--- tests/regress/nocycle2/QMTest/configuration	31 Jul 2003 23:12:48 -0000
***************
*** 0 ****
--- 1,5 ----
+ <?xml version="1.0" ?>
+ <!DOCTYPE extension
+   PUBLIC '-//Software Carpentry//QMTest Extension V0.1//EN'
+   'http://www.software-carpentry.com/qm/xml/extension'>
+ <extension class="xml_database.XMLDatabase" kind="database"/>
\ No newline at end of file


