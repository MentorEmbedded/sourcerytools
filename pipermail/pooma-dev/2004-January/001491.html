<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [pooma-dev] Re: [PATCH] Fix deadlocks in MPI reduction evaluators
   </TITLE>
   <LINK REL="Index" HREF="index.html" >
   <LINK REL="made" HREF="mailto:pooma-dev%40codesourcery.com?Subject=Re%3A%20%5Bpooma-dev%5D%20Re%3A%20%5BPATCH%5D%20Fix%20deadlocks%20in%20MPI%20reduction%20evaluators&In-Reply-To=%3CPine.LNX.4.58.0401132032500.724%40goofy%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="001489.html">
   <LINK REL="Next"  HREF="001499.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[pooma-dev] Re: [PATCH] Fix deadlocks in MPI reduction evaluators</H1>
    <B>Richard Guenther</B> 
    <A HREF="mailto:pooma-dev%40codesourcery.com?Subject=Re%3A%20%5Bpooma-dev%5D%20Re%3A%20%5BPATCH%5D%20Fix%20deadlocks%20in%20MPI%20reduction%20evaluators&In-Reply-To=%3CPine.LNX.4.58.0401132032500.724%40goofy%3E"
       TITLE="[pooma-dev] Re: [PATCH] Fix deadlocks in MPI reduction evaluators">rguenth at tat.physik.uni-tuebingen.de
       </A><BR>
    <I>Tue Jan 13 19:43:46 UTC 2004</I>
    <P><UL>
        <LI>Previous message: <A HREF="001489.html">[PATCH] Fix deadlocks in MPI reduction evaluators
</A></li>
        <LI>Next message: <A HREF="001499.html">[pooma-dev] Re: [PATCH] Fix deadlocks in MPI reduction evaluators
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#1491">[ date ]</a>
              <a href="thread.html#1491">[ thread ]</a>
              <a href="subject.html#1491">[ subject ]</a>
              <a href="author.html#1491">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>On Tue, 13 Jan 2004, Jeffrey D. Oldham wrote:

&gt;<i> Richard Guenther wrote:
</I>&gt;<i> &gt; Hi!
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; The following patch is necessary to avoid deadlocks with the MPI
</I>&gt;<i> &gt; implementation and multi-patch setups where one context does not
</I>&gt;<i> &gt; participate in the reduction.
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; Fixes failure of array_test_.. - I don't remember - with MPI.
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; Basically the scenario is that the collective synchronous MPI_Gather is
</I>&gt;<i> &gt; called from ReduceOverContexts&lt;&gt; on the non-participating (and thus
</I>&gt;<i> &gt; not receiving) contexts while the SendIterates are still in the
</I>&gt;<i> &gt; schedulers queue.  The calculation participating contexts will wait for
</I>&gt;<i> &gt; the ReceiveIterates and patch reductions to complete using the CSem
</I>&gt;<i> &gt; forever then.
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; So the fix is to make the not participating contexts wait on the CSem,
</I>&gt;<i> &gt; too, by using a fake write iterate queued after the send iterates which
</I>&gt;<i> &gt; will trigger as soon as the send iterates complete.
</I>&gt;<i>
</I>&gt;<i> Instead of adding fake write iterate can we adjust the MPI_Gather so
</I>&gt;<i> non-participating contexts do not participate?
</I>
The problem is not easy to tackle in MPI_Gather, as collective
communication primitives involve all contexts and this can be overcome
only by creating a new MPI communicator, which is costly.  Also I'm not
sure that this will solve the problem at all.

The problem is that contexts participating only via sending their data to
a remote context (i.e. are participating, but not computing) don't have
the counting semaphore to block on (its height is zero for them).  So
after queuing the send iterates they go straight to the final reduction
which is not done via an extra iterate and block there, not firing off the
send iterate in the first place. Ugh. Same of course for completely non
participating contexts, and even this may be a problem because of old
unrun iterates.

So in first I thought of creating a DataObject to hold the reduction
result, so we can do usual data-flow evaluation on it, and not ignore
dependencies on it, as we do now.  But this turned out to be more invasive
and I didn't have time to complete this.

So the fake writing iterate solves the problem (only partly, because, I
could imagine for completely non-participating contexts the problem is
still there) for me.

But anyway, I'm not pushing this very hard now, but it's guaranteed to
deadlock at reductions otherwise for MPI for me (so there's a race even
in the case of all-participating contexts, or the intersector is doing
something strange).

Richard.

</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="001489.html">[PATCH] Fix deadlocks in MPI reduction evaluators
</A></li>
	<LI>Next message: <A HREF="001499.html">[pooma-dev] Re: [PATCH] Fix deadlocks in MPI reduction evaluators
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#1491">[ date ]</a>
              <a href="thread.html#1491">[ thread ]</a>
              <a href="subject.html#1491">[ subject ]</a>
              <a href="author.html#1491">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="http://sourcerytools.com/cgi-bin/mailman/listinfo/pooma-dev">More information about the pooma-dev
mailing list</a><br>
</body></html>
