<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [PATCH] Fix deadlocks in MPI reduction evaluators
   </TITLE>
   <LINK REL="Index" HREF="index.html" >
   <LINK REL="made" HREF="mailto:pooma-dev%40codesourcery.com?Subject=Re%3A%20%5BPATCH%5D%20Fix%20deadlocks%20in%20MPI%20reduction%20evaluators&In-Reply-To=%3C400441CF.1030007%40codesourcery.com%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="001484.html">
   <LINK REL="Next"  HREF="001491.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[PATCH] Fix deadlocks in MPI reduction evaluators</H1>
    <B>Jeffrey D. Oldham</B> 
    <A HREF="mailto:pooma-dev%40codesourcery.com?Subject=Re%3A%20%5BPATCH%5D%20Fix%20deadlocks%20in%20MPI%20reduction%20evaluators&In-Reply-To=%3C400441CF.1030007%40codesourcery.com%3E"
       TITLE="[PATCH] Fix deadlocks in MPI reduction evaluators">oldham at codesourcery.com
       </A><BR>
    <I>Tue Jan 13 19:06:55 UTC 2004</I>
    <P><UL>
        <LI>Previous message: <A HREF="001484.html">[PATCH] Fix deadlocks in MPI reduction evaluators
</A></li>
        <LI>Next message: <A HREF="001491.html">[pooma-dev] Re: [PATCH] Fix deadlocks in MPI reduction evaluators
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#1489">[ date ]</a>
              <a href="thread.html#1489">[ thread ]</a>
              <a href="subject.html#1489">[ subject ]</a>
              <a href="author.html#1489">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Richard Guenther wrote:
&gt;<i> Hi!
</I>&gt;<i> 
</I>&gt;<i> The following patch is necessary to avoid deadlocks with the MPI
</I>&gt;<i> implementation and multi-patch setups where one context does not
</I>&gt;<i> participate in the reduction.
</I>&gt;<i> 
</I>&gt;<i> Fixes failure of array_test_.. - I don't remember - with MPI.
</I>&gt;<i> 
</I>&gt;<i> Basically the scenario is that the collective synchronous MPI_Gather is
</I>&gt;<i> called from ReduceOverContexts&lt;&gt; on the non-participating (and thus
</I>&gt;<i> not receiving) contexts while the SendIterates are still in the
</I>&gt;<i> schedulers queue.  The calculation participating contexts will wait for
</I>&gt;<i> the ReceiveIterates and patch reductions to complete using the CSem
</I>&gt;<i> forever then.
</I>&gt;<i> 
</I>&gt;<i> So the fix is to make the not participating contexts wait on the CSem,
</I>&gt;<i> too, by using a fake write iterate queued after the send iterates which
</I>&gt;<i> will trigger as soon as the send iterates complete.
</I>
Instead of adding fake write iterate can we adjust the MPI_Gather so 
non-participating contexts do not participate?

&gt;<i> Tested using MPI, Cheetah and serial some time ago.
</I>&gt;<i> 
</I>&gt;<i> Ok?
</I>&gt;<i> 
</I>&gt;<i> Richard.
</I>&gt;<i> 
</I>&gt;<i> 
</I>&gt;<i> 2004Jan08  Richard Guenther &lt;<A HREF="http://sourcerytools.com/cgi-bin/mailman/listinfo/pooma-dev">richard.guenther at uni-tuebingen.de</A>&gt;
</I>&gt;<i> 
</I>&gt;<i> 	* src/Engine/RemoteEngine.h: use a waiting iterate to wait for
</I>&gt;<i> 	reduction completion in remote single and multi-patch reduction
</I>&gt;<i> 	evaluator.
</I>&gt;<i> 	Do begin/endGeneration at the toplevel evaluate.
</I>&gt;<i> 	src/Evaluator/Reduction.h: do begin/endGeneration at the toplevel
</I>&gt;<i> 	evaluate.
</I>&gt;<i> 
</I>&gt;<i> --- src/Engine/RemoteEngine.h	2004-01-02 12:57:48.000000000 +0100
</I>&gt;<i> +++ /home/richard/src/pooma/pooma-mpi3/r2/src/Engine/RemoteEngine.h	2004-01-08 23:00:40.000000000 +0100
</I>&gt;<i> @@ -1954,6 +1962,29 @@
</I>&gt;<i>    }
</I>&gt;<i>  };
</I>&gt;<i> 
</I>&gt;<i> +
</I>&gt;<i> +template &lt;class Expr&gt;
</I>&gt;<i> +struct WaitingIterate : public Pooma::Iterate_t {
</I>&gt;<i> +  WaitingIterate(const Expr&amp; e, Pooma::CountingSemaphore&amp; csem)
</I>&gt;<i> +    : Pooma::Iterate_t(Pooma::scheduler()),
</I>&gt;<i> +      e_m(e), csem_m(csem)
</I>&gt;<i> +  {
</I>&gt;<i> +    DataObjectRequest&lt;WriteRequest&gt; writeReq(*this);
</I>&gt;<i> +    engineFunctor(e_m, writeReq);
</I>&gt;<i> +  }
</I>&gt;<i> +  virtual void run()
</I>&gt;<i> +  {
</I>&gt;<i> +    csem_m.incr();
</I>&gt;<i> +  }
</I>&gt;<i> +  virtual ~WaitingIterate()
</I>&gt;<i> +  {
</I>&gt;<i> +    DataObjectRequest&lt;WriteRelease&gt; writeRel;
</I>&gt;<i> +    engineFunctor(e_m, writeRel);
</I>&gt;<i> +  }
</I>&gt;<i> +  Expr e_m;
</I>&gt;<i> +  Pooma::CountingSemaphore&amp; csem_m;
</I>&gt;<i> +};
</I>&gt;<i> +
</I>&gt;<i>  //-----------------------------------------------------------------------------
</I>&gt;<i>  // Single-patch Reductions involving remote engines:
</I>&gt;<i>  //
</I>&gt;<i> @@ -1998,12 +2029,11 @@
</I>&gt;<i>      Pooma::CountingSemaphore csem;
</I>&gt;<i>      csem.height(1);
</I>&gt;<i> 
</I>&gt;<i> -    Pooma::scheduler().beginGeneration();
</I>&gt;<i> -
</I>&gt;<i>      if (Pooma::context() != computationContext)
</I>&gt;<i>      {
</I>&gt;<i>        expressionApply(e, RemoteSend(computationContext));
</I>&gt;<i> -      csem.incr();
</I>&gt;<i> +      Pooma::Iterate_t *it = new WaitingIterate&lt;Expr&gt;(e, csem);
</I>&gt;<i> +      Pooma::scheduler().handOff(it);
</I>&gt;<i>      }
</I>&gt;<i>      else
</I>&gt;<i>      {
</I>&gt;<i> @@ -2013,8 +2043,7 @@
</I>&gt;<i>  		 forEach(e, view, TreeCombine()), csem);
</I>&gt;<i>      }
</I>&gt;<i> 
</I>&gt;<i> -    Pooma::scheduler().endGeneration();
</I>&gt;<i> -
</I>&gt;<i> +    // Wait for RemoteSend or Reduction to complete.
</I>&gt;<i>      csem.wait();
</I>&gt;<i> 
</I>&gt;<i>      RemoteProxy&lt;T&gt; globalRet(ret, computationContext);
</I>&gt;<i> @@ -2102,8 +2131,6 @@
</I>&gt;<i>      csem.height(n);
</I>&gt;<i>      T *vals = new T[n];
</I>&gt;<i> 
</I>&gt;<i> -    Pooma::scheduler().beginGeneration();
</I>&gt;<i> -
</I>&gt;<i>      i = inter.begin();
</I>&gt;<i>      k = 0;
</I>&gt;<i>      for (j = 0; j &lt; inter.size(); j++)
</I>&gt;<i> @@ -2129,13 +2156,19 @@
</I>&gt;<i>  	    else
</I>&gt;<i>  	    {
</I>&gt;<i>  	      expressionApply(e(*i), RemoteSend(computationalContext[j]));
</I>&gt;<i> +              // One extra RemoteSend to wait for. Maybe we can combine these
</I>&gt;<i> +              // iterates, but maybe not. Play safe for now.
</I>&gt;<i> +	      csem.raise_height(1);
</I>&gt;<i> +	      Pooma::Iterate_t *it = new WaitingIterate
</I>&gt;<i> +		&lt;typename View1&lt;Expr, INode&lt;Expr::dimensions&gt; &gt;::Type_t&gt;(e(*i), csem);
</I>&gt;<i> +	      Pooma::scheduler().handOff(it);
</I>&gt;<i>  	    }
</I>&gt;<i>  	  }
</I>&gt;<i> 
</I>&gt;<i>  	++i;
</I>&gt;<i>        }
</I>&gt;<i> 
</I>&gt;<i> -    Pooma::scheduler().endGeneration();
</I>&gt;<i> +    // Wait for RemoteSends and Reductions to complete.
</I>&gt;<i>      csem.wait();
</I>&gt;<i> 
</I>&gt;<i>      if (n &gt; 0)
</I>&gt;<i> --- src/Evaluator/Reduction.h	2003-11-21 22:30:38.000000000 +0100
</I>&gt;<i> +++ /home/richard/src/pooma/pooma-mpi3/r2/src/Evaluator/Reduction.h	2004-01-02 00:40:14.000000000 +0100
</I>&gt;<i> @@ -128,10 +128,15 @@
</I>&gt;<i>    void evaluate(T &amp;ret, const Op &amp;op, const Expr &amp;e) const
</I>&gt;<i>    {
</I>&gt;<i>      typedef typename EvaluatorTag1&lt;Expr&gt;::Evaluator_t Evaluator_t;
</I>&gt;<i> +
</I>&gt;<i> +    Pooma::scheduler().beginGeneration();
</I>&gt;<i> +
</I>&gt;<i>      PAssert(checkValidity(e, WrappedInt&lt;Expr::hasRelations&gt;()));
</I>&gt;<i>      forEach(e, PerformUpdateTag(), NullCombine());
</I>&gt;<i>      Reduction&lt;Evaluator_t&gt;().evaluate(ret, op, e());
</I>&gt;<i> 
</I>&gt;<i> +    Pooma::scheduler().endGeneration();
</I>&gt;<i> +
</I>&gt;<i>      POOMA_INCREMENT_STATISTIC(NumReductions)
</I>&gt;<i>    }
</I>&gt;<i>  };
</I>&gt;<i> @@ -184,12 +189,8 @@
</I>&gt;<i>      Pooma::CountingSemaphore csem;
</I>&gt;<i>      csem.height(1);
</I>&gt;<i> 
</I>&gt;<i> -    Pooma::scheduler().beginGeneration();
</I>&gt;<i> -
</I>&gt;<i>      evaluate(ret, op, e, csem);
</I>&gt;<i> 
</I>&gt;<i> -    Pooma::scheduler().endGeneration();
</I>&gt;<i> -
</I>&gt;<i>      csem.wait();
</I>&gt;<i>    }
</I>&gt;<i>  };
</I>&gt;<i> @@ -237,12 +238,10 @@
</I>&gt;<i> 
</I>&gt;<i>      expressionApply(e, IntersectorTag&lt;Inter_t&gt;(inter));
</I>&gt;<i> 
</I>&gt;<i> -    const int n = std::distance(inter.begin(), inter.end());
</I>&gt;<i> +    const int n = inter.size();
</I>&gt;<i>      Pooma::CountingSemaphore csem;
</I>&gt;<i>      csem.height(n);
</I>&gt;<i>      T *vals = new T[n];
</I>&gt;<i> -
</I>&gt;<i> -    Pooma::scheduler().beginGeneration();
</I>&gt;<i> 
</I>&gt;<i>      typename Inter_t::const_iterator i = inter.begin();
</I>&gt;<i>      int j = 0;
</I>&gt;<i> @@ -253,8 +252,6 @@
</I>&gt;<i>          ++i; ++j;
</I>&gt;<i>        }
</I>&gt;<i> 
</I>&gt;<i> -    Pooma::scheduler().endGeneration();
</I>&gt;<i> -
</I>&gt;<i>      csem.wait();
</I>&gt;<i> 
</I>&gt;<i>      ret = vals[0];
</I>

-- 
Jeffrey D. Oldham
<A HREF="http://sourcerytools.com/cgi-bin/mailman/listinfo/pooma-dev">oldham at codesourcery.com</A>


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="001484.html">[PATCH] Fix deadlocks in MPI reduction evaluators
</A></li>
	<LI>Next message: <A HREF="001491.html">[pooma-dev] Re: [PATCH] Fix deadlocks in MPI reduction evaluators
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#1489">[ date ]</a>
              <a href="thread.html#1489">[ thread ]</a>
              <a href="subject.html#1489">[ subject ]</a>
              <a href="author.html#1489">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="http://sourcerytools.com/cgi-bin/mailman/listinfo/pooma-dev">More information about the pooma-dev
mailing list</a><br>
</body></html>
